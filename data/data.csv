channel_name,video_title,video_id,transcript,summary,processed_date
The AI Daily Brief: Artificial Intelligence News,The Biggest Battle in AI is for Your Personal Context,w9xkk58hT3E,"Claude coowork chatbt health Google personal intelligence all of this ultimately is about the biggest battle in AI the battle for your personal context welcome back to the AI daily brief today we were talking about Google Gemini's big upgrade that they are calling personal intelligence yesterday they announced the very obvious and yet very useful ability to connect Gemini with all the information from other Google apps that you interact with like Gmail photos search YouTube all of course in an effort to help make Gemini I more personalized for the individual user. What's interesting is that I believe that at core you can view almost every single move being made in and around consumer AI as in some way a battle for personal context. So let's look at what I mean. Big news from earlier this week was the announcement of Claude Co-work. It's basically Claude code but simplified in a way that it's designed for non-technical users. You don't have to deal with the terminal anymore. It lives right inside your Claude desktop app and it allows you to do the types of things that people have been using Claude Code for outside of coding. Now, the big thing that has made Claude code and now Claude co-work powerful is that it has access to a unique set of context which is the stuff on your desktop. What makes it different than just the Claude chat window or the chat GPT window or the Gemini window is that instead of having to upload the context that's relevant for any particular thing you're trying to do, you just point it at the relevant part of the computer. Now, of course, in addition to having that better context, co-working cloud code can also do things and interact with your desktop, making it more agentic. But that power comes from its ability to access everything on your computer. And yet, even with that, a lot of the issues that people have discussed when it comes to Claude Co-work over the last few days, which admittedly are more likely having to do with the fact that it was built in the 10 days previous isn't around connecting other types of context. While Claude Co-work and Claude Code have access to what's on your machine, if you live in the modern world, there's going to be lots of other data sources and places where your data lives that are not just on your desktop. And for that, Claude gives you access to things via what they call connectors. Connectors are ways to link things like Google Drive, obviously powered by the model context protocol. And in the first couple of days after Cloud Co-work went live, a lot of people's challenges have been in and around making those connectors work. The point being in some ways that we are so hungry for personal context that just having access to our full computers isn't enough. We still need access to everything that exists on the web as well. So okay, we've repositioned cloud co-work and cloud code as powerful because of the way they give you unique access to your desktop context. How are the other things that AI companies are launching right now also in some way about this battle for personal context? I would argue that when it comes to chat GBT, a huge anchor to their strategy has always been to try to leverage the fact that because chat GBT was many people's default, it has a huge amount of personal context in the form of past chats. And when you view everything in the battle for personal context, all of a sudden, OpenAI strategy to add more and more applications all of the time with an incredible shipping velocity starts to make a little more sense. They are trying with each new app release to get more personal context which makes the switching cost of leaving and going to another AI service more and more costly in terms of that lost context. For months now folks have been talking about how memory is the next big moat and I think that that's dead on. Now bringing it back to things that have been released recently so far from OpenAI the biggest product that we've got in January is the introduction of chatbt health. It's a dedicated health experience inside the app whose entire purpose is to collect a huge amount of personal health context and organize it in a single place that makes it accessible to the AI. In their announcement post, they wrote, ""Today, health information is often scattered across portals, apps, wearables, PDFs, and medical notes. So, it's hard to see the full picture, and people are left to navigate a complex healthcare system on their own."" Now, as they point out, people are already using chat GPD to help navigate all this, but now they're allowing you to port all of that context in, and they are really trying to pull that health context from everywhere it lives. Just a few days later, we got Anthropic's answer to that in their Cloud for Healthcare. A big part of that Claude for Healthcare announcement was about connecting personal health data. The announcement came with a bunch of new connectors rolling out specifically for that type of personal context. I would even argue that Grock's big play outside of having Elon for fundraising and for building the biggest supercomputers in the world is once again around personal context. The unique personal context that Grock has access to is everything that happens in and around X/ Twitter, which for those of you who aren't on X/ Twitter might not seem like it matters, but for those of us who are and who have been for a very long time is a very significant part of personal context. Okay, so now you're starting to see all of these different moves through the lens of personal context, but Google's latest announcement is in some ways the clearest yet. Yesterday, CEO Sundarbachai tweeted, ""Answering a top request from our users, we're introducing personal intelligence in the Gemini app. You can now securely connect to Google apps for an even more helpful experience. Personal intelligence combines two core strengths. Reasoning across complex sources and retrieving specific details, eg from an email or photo to provide uniquely tailored answers. It's built with privacy at the center and you choose exactly which apps to connect with the connected app settings off by default. Some of the examples that Google gives about how this might be useful are really concentrated on day-to-day life. This is not about work. In their announcement thread, they wrote, ""Ever need to buy parts for your car but don't have the info handy? Ask Gemini to recommend tires for my car. By referencing connected apps like Gmail and Photos, it can understand your car's make and model and even the types of trips you take to give recommendations of tires and info like your license plate number to make your visit to the auto shop go more smoothly. When a user asks for recommendations around travel, instead of it being generic lists, the specific travel dates that can be found in Gmail, plus other evidence, like in their example, your love for nature photography found in Google Photos, lead to more personalized recommendations. People's first instinct was that this was a big deal and that in many ways it was inevitable, but kind of a killing blow play from Google. AI YouTuber Matthew Berman writes, ""Gemini will now be my daily driver AI for the next few weeks, all because of personal intelligence. Google would have never allowed this kind of feature to release just 18 months ago. They would have been too nervous, too much red tape. But now they got out of their own way and allowed users to choose. Google is so well positioned to win AI. Apple, where you at? Akos Gupta writes, ""Google just revealed the AI mode nobody can replicate. Every AI company is racing to build memory and personalization. Google connects to a decade of your Gmail threads, every photo you've ever taken, your complete YouTube watch history, and every search query you've made since 2005. The question for every other AI company, how do you compete on personalization when your competitor has the user's entire digital life and you're starting from a blank conversation? I think there are a couple answers to this. First of all, I do think it's important to note that while it does seem obvious that this would make AI better for a variety of use cases, I don't think we yet have enough evidence to know exactly the full complexity of the way that AI gets used over time. To be clear, I am far from the average consumer and user of AI. And yet I do represent a type of user of AI and I couldn't care less about this if I tried. For my work rellated use cases, I care about the quality of AI strategic thinking, its ability to process and articulate multiple angles around the same decisions, how good it is at accessing other types of data, how good it is at analyzing types of data I give it access to, how good it is at building the things that I need. There's not a universe in which I'm switching models because I can get better travel recommendations or need a shortcut way to figure out what my license plate is. And to be clear, this is not at all a knock on these new features from Google, nor an argument that I'm anywhere near the normal consumer. My point is solely that when it comes to these big bold claims that Gemini is killing everyone because of this, I think there's going to be a lot of types of different AI users, all of whom have different types of priorities. Still, let's assume that this type of personalization is really valuable for many, if not most, consumers. Well, another path is to ask who else has access to that data, which brings us back to Matthew Burman's question, Apple, where you at? When Apple announced Apple Intelligence way back when, it was all with this same argument. The pitch was simple, helpful day-to-day use cases that took advantage of the context that Apple had about you because it powers all of your devices. Now, obviously, it has not delivered on that promise. One of the big takeaways after Google IO last year, in fact, was that Google had basically shipped everything that Apple's AI wanted to do. And now, of course, Gemini is going to actually power Apple intelligence. And yet, Apple still does have an enormous amount of personal context that others don't have. Google, for example, does not have your iMes. And for iPhone users, iMes tend to represent dozens of gigabytes of personal context that is extraordinarily valuable. And frankly, when it comes to a personal level, more valuable for many use cases than the stuff that's in your Gmail. Apple also has something else. Ownership of devices that operate in the physical world. And I think when you start to view everything through the lens of this battle for personal context, OpenAI's hardware decisions start to make a little more sense. Hardware allows them to go after a very specific type of personal context, which is the personal context of how you interact in the physical world. One thing that Apple did last year that did capture people's attention was the new live translation feature they announced for Apple AirPods. Unlike many other form factors for AI devices, AirPods are something we already interact with. It's not at all weird or abnormal to talk to someone who has AirPods in. And so to the extent that AirPods can become a starting point for AI to interact with your physical experience, it could unlock a whole additional set of that personal context. This is why it wasn't all that surprising when we found out that it seems like at least one of the form factors that OpenAI and Johnny Ivor are exploring is something at least tangentially related to an AirPod. Make no mistake about it, Google giving Gemini access to all of this information is a major inflection point and a major upgrade in their positioning when it comes to the consumer AI race. But it's still early innings and a lot of battles yet to be fought and a lot of personal context that still needs to be organized. For now though, that is going to do it for today's AI daily brief. Appreciate you listening or watching as always and until next time, peace.","**The Battle for Personal Context: The Ultimate Showdown in AI**

The world of Artificial Intelligence (AI) is witnessing a fierce battle, and it's not about **machine learning** or **natural language processing**. The biggest battle in AI is for **personal context**, and every major player is vying for control. From **Google** to **OpenAI**, **Anthropic**, and **Grock**, each company is trying to outdo the others in collecting, organizing, and leveraging **personal data** to provide more **personalized experiences**.

Recently, **Google** announced a significant upgrade to its **Gemini** app, introducing **personal intelligence** that allows users to connect their Google apps, such as **Gmail**, **Photos**, and **YouTube**, to create a more **helpful experience**. This move is seen as a major inflection point in the consumer AI race, as it gives **Google** access to a vast amount of **personal context**.

However, **Google** is not the only player in this game. **OpenAI** has been making significant strides with its **ChatGPT** and **Cloud Co-work** platforms, which aim to collect and organize **personal context** from various sources. **Anthropic** has also announced its **Cloud for Healthcare**, which focuses on connecting **personal health data**.

**Grock**, with its access to **X/Twitter** data, is also a significant player in this battle. The company's unique **personal context** is a valuable asset in the AI landscape. Meanwhile, **Apple** has been relatively quiet, but its **Apple Intelligence** and **iMes** data provide a significant amount of **personal context** that others don't have.

The battle for **personal context** is far from over, and it's still early innings. As AI companies continue to innovate and collect more **personal data**, the landscape will evolve. **Hardware** will play a crucial role in this battle, as companies like **OpenAI** and **Apple** explore new form factors, such as **AirPods**, to interact with users in the physical world.

The key takeaways from this battle are:

1. **Personal context** is the ultimate prize in the AI world.
2. **Google**'s **personal intelligence** upgrade is a significant move in the consumer AI race.
3. **OpenAI**, **Anthropic**, and **Grock** are major players in the battle for **personal context**.
4. **Apple**'s **iMes** data and **hardware** capabilities provide a unique advantage.
5. **Hardware** will play a crucial role in the battle for **personal context**.

As the battle for **personal context** continues, one thing is clear: the company that can collect, organize, and leverage **personal data** most effectively will emerge victorious. The question is, who will it be?

**Social Media Post Ideas:**

* ""The battle for **personal context** is on! Who will win the ultimate prize in AI? #AI #PersonalContext""
* ""Google's **personal intelligence** upgrade is a game-changer! But can others catch up? #Google #AI""
* ""The future of AI is all about **personal context**. Who's leading the charge? #AI #Innovation""
* ""The battle for **personal context** is far from over. Stay tuned for the latest developments! #AI #Tech""",2026-01-16T01:45:43.463306
NextWork,AI x FinOps Series (DAY #3) | Privacy-First Checkout Analytics: PostHog Conversion Funnels,_uxTZDbkJ8U,"Hello, back for another project of the 21 and 21 project in 21 days. This one is going to be the final uh project in the fin ops series. So, we're going to be looking at using Post Hog, which is a um analytics analytics tool um for understanding traffic and all sorts um that come through your web app and running tests. um yeah really understanding why conversions and and what if you want to understand um aren't going through or you know what is working. Um so yeah just just a great way of of using um using the data you kind of already have access to um just sort of turning it into visualizations then creating experiments with those. So yeah, super cool project. Um yeah, really looking forward to walking through this one. Uh, this project does build on the previous two in the series. Uh, there is a catchup um section, so if you haven't done them, that's cool, too. But I'd recommend doing them in their entirety. Um, they're setting up the e-commerce site we're using in VZ. Uh, and so using Versel, Vzero, and Cursor as well. Uh, the second part is Stripe payments. So, setting that up. And then this last one is like I said using post hog u the tool post hog to um understand data uh and then do something about it. Cool. So privacy first checkout analytics build conversion funnel tracking with post hog that reveals drop offs without storing sensitive payment data. This is a relatively easy project. Should take around 45 minutes. probably take a bit longer than that because I'm doing it um and explaining what I'm doing. Gone over the related projects. They're just projects from the series. Key concepts are Post Hog, Versel, Analytics, and privacy compliance. Cool. So, here's a 30 secondond summary. Imagine your checkout is live. Users visit, some buy, most don't. Where do they drop off? Is this the product page, the checkout form, the payment sh uh step? Uh without analytics, you're just guessing. Companies like Brainboard, Y Combinator and Superbase use Post Hog as as well as Nexwork, we use it as well um for privacy first analytics. They track checkout funnels to see exactly where customers abandon their purchase, then fix those gaps to boost conversions without capturing sensitive user data um and user and payment data. Um yeah, so you'll build the same analytics setup through cursor's chat, installing post hog, tracking events, and building um conversion funnels without writing manual code. So an over overview of what you'll build. So you've got the user on the browser so that um they visit the checkout app, triggers page views, um they click um into payment actions and then on our next js, so hosted on Versell um our app, Next.js JS app posted on the cell the post do uh post hog SDK is initialized uh and then an analytics tracking functions capture um so page view on landing clicked the specific product checkout initiated payment submitted and payment exceeded/failed um so we're looking at the entire flow from when someone lands on our page we want them to get to the end and to getting to the end and obviously there will be some fall off but you want that to be as a minimum um the minimal as possible. You want the most amount of people that go onto your site to convert to paying users, whatever whatever that is. In this case, it's a e-commerce store. U but that could be a um yeah, really anything. So then then you've got the Post Hog cloud which receives event stream from this SDK software development kit. Uh the Post Hog one that's initialized here uh stores behavioral data only and provides conversion funnels, session replays, and browser device breakdowns and and more. Uh there's more you can do with Post Hog. Cool. So in this project, you'll use Post Hog to set up privacy first checkout and analytics, track conversion funnels, and learn the methodology for finding and fixing drop offs. So why privacy first analytics? Traditional analytics often store sensitive data like card numbers, email uh emails, addresses. That's a liability you don't need or want. Um what you need is visibility without risk. Post Hog lets you track user behavior without storing PII. Um, you'll see exactly where users abandon their purchase, without ever touching their payment uh, their payment data. This keeps you compliant with PCIe. DSS is just um, law like a GDPR is the European um, one set up for data privacy. Um, and PII is personal personal identifiable uh, information. Um so any yeah if if I have someone's address or a phone number I can use theoretically use that to to find who um who this person is. Um and we don't want to store that. So you don't need any prior analytic experience. This project teaches you the workflow professionals use to optimize conversions. And you're very welcome to use the ask feature here to check if this project's right for you. You can ask the chat um and then it's going to give you some feedback um and suggest if it's the correct option for you. Cool. So, first you'll install Post Hog and set up event tracking uh for your checkout flow using cursor. Then you'll build a conversion funnel and post hog to visualize what uses drop off. Finally, you'll learn the datadriven methodology that uses um companies use to turn funnel insights into conversion improvements. By the end of this project, you'll have privacy safe event tracking setup with cursor prompts and post hog. a conversion funnel from landing page to successful payment. And then if you do the secret mission, we're going to do some AB testing for different um checkout button styles. You can do really anything with AB testing, but that's what we're going to do. So, what do I need before starting this project builds on the landing page um part one of this series and the secure payments with Stripe part two. So, you'll reuse the entire application. So, it's really crucial you've done that. Um and it's Post Hog free. Yes, it is. Uh, Post Hog offers a generous free tier with 1 million events per month, more than enough for this project, and you won't need a credit card to get started. Now, there's a quiz here. If you want to do that before you start, you're very welcome to. Um, we'll leave it to the end. Um, and we'll be doing walking through the step by step uh guidance uh in this video, but there is a low touch um sort of some guidance and then um on your own. So just going through and doing uh all the bullet points um there if you feel really confident in this area. Cool. And if you get stuck feel free to reach out to the community. Um plenty of people are able to help there with this project and other projects in 21 and 21 and all the projects but that's our focus right now. Um so before we start step one what are we doing in this project? In this project, I'm going to set up Post Hog to monitor to monitor my web app. Post Hog is a tool that allows me to um better understand traffic or my applications traffic and turn this into actionable items that result in uh better results. Eg better conversions. Privacy first analytics means uh I don't need to worry about storing sensitive user information like passwords or maybe not like addresses addresses uh phone numbers uh etc. Cool. So, what are we doing in this project? In this project, I'm going to set up Post Hog to monitor my web app. Post Hog is a tool that allows me to better understand my application's traffic and turn this into actionable um into actionable items that result in better results, result better results that um results in better outcomes. AGBA conversions. Uh, privacy first anal analytics means I don't need to worry about storing sensitive user information like addresses, phone numbers, etc. Twice, etc. Cool. All right. Step one. So, we're going to track checkout events without storing sensitive data. Your checkout is live if you've done the project. Um, but you're flying blind. users visit, some buy, most don't. Where do they drop off? Is the Is it the product page, the checkout form, the payment step, or that analytics? You're just guessing. Here's the challenge. Traditional analytics often store sensitive data, car numbers, emails, addresses. That's a liability you don't need. Um, what you need is a a visibility without risk. Exactly what we're about to build. We'll set up post hog for privacy first analytics tracking user behavior through every step of the checkout without storing any PII. That's the personal identifiable information. You'll see exactly where users abandon their purchase without ever touching their payment data. Very good. So in this step, get ready to sign up for Post Hog. The free tier won't cost you anything. Install the post SDK into your versell app and then add event tracking for checkout flow events. So what are we doing in this step? In this step, I'm setting up uh post hog um install plus sign up um to track um checkout or event tracking. Oh, I should just go straight to to track um checkout flow events without storing sensitive data. This is important because storing this kind of data is a liability. What are we doing in this step? In this step, I'm setting up post hog. It's installing and signing up um to track checkout flow events without storing sensitive data. This is important because storing this kind of data is a liability. Cool. All right. So, before we begin, um if you've completed parts one and two, you're you're all good to go. Um if you're starting fresh, then there's this path here as well um that you can definitely follow. there's steps how to catch up to what we did. Um, but you're very welcome to also just do the projects uh themselves. There's a a few steps here because there's um two projects. So, yeah, more than welcome to follow this or go do the project in in full. Um, but I'm already done this. So, we're just going to go down the I completed parts one and two. So, you have already you already have a working checkout flow with Stripe integration. Your Versell app is ready for analytics. Open your existing folder in your code editor. Make sure your app runs locally and continue to the next section. Cool. Open up cursor in the background. Post Hog offers a generous free tier with 1 million events per month. More than enough for learning and for small projects. Um, now we're going to go and create a account. Um, so I do have a Posthole account, but I will go and go through the process of creating one um on a new Chrome profile. So, we'll go here and I will just open up a separate Chrome profile so we can walk through this. Cool. So, we'll go to Post Hog. Um, we'll click get started free. Take us to a signup page. Um, we're going to continue with GitHub. Um, that's what we're doing with this project, with this series. Um, where we're using GitHub to make sure everything's synced together. Um, so I'll just sign up on that over here. Cool. So, I'll just click dash dashboard here. create an account and then go to GitHub. Cool. So, I'll just call my organization um Cahoo's Next Work store. What's your role? Um, we can just say data. Click create organization. Cool. Awesome. So we can select product analytics session replay and experiments then click get started. Doesn't really matter. Um we just click um like myself but oh there you go. So, product analytic session replay that's in here somewhere experience. Cool. All right. So, why are these three products? These are our steps we're doing effectively. So, product analytics is steps one and two. Session replay is step three and experiments are step oh sorry, secret missions final final thing. Cool. So, um, open cursor in the background and we're just going to go get our API keys now. Um, so I've got cursor just here. Um, I'll put it here briefly. I'll move it. Um, once we once you don't need it anymore. So, yeah, we'll select Nex.js. That's the um the framework we're using. Um and then we can see here the the installation steps. So we'll copy this go into our um and yeah so we uh we'll go over this first. Get your API keys. Now let's add Post Hog to your project. Post hog will give you framework specific um instruments or instructions sorry based on the type of your app. Select Nex.js from the framework list. Why Nex.js JS because your check out app parts one and two is built with Nex.js. Post hog will give you a framework specific setup instructions to match your app. So we'll click Nex.js and then we'll just copy these um copy these environment um variables into our local file inside of our web app. Awesome. Um, yeah. Or you can just paste the key in there and then go from there. Cool. So, we're going to open terminal inside of cursor and run the command to install the Post Hog SDK. Shouldn't take too long. Cool. Looks like it installed. So successfully installed. That's great. There's also I got an error. So you can go if you had had an error in that case. Maybe there was a conflict in the package install. Then you can go down there. You're not the right um directory could have an impact. So yeah, but we successfully installed. So we'll go down that path. So now we're going to initialize Post Hog in our app using cursor. Nice. With the SDK installed, let's initialize Post Hog so it starts tracking when your app loads. Let's open cursors chat and then send this prompt. So I'm going to replace this side with cursor here. There we go. Go to chat. So that's control L or command L if you're on Mac. Uh we'll send this prompt off. This is from the previous um previous project. Just close that. Um cool. So this prompt is saying implement post hog initialization with instrument client instrumentation client.ts s and ensure it's imported/executed so that post hog initialize it when the app loads and then it lists where the API keys are. Cool. So, we'll just let that whirl away. Um, what is instrumentation-clienttype? This is a special Nex.js file that runs code when your app first loads in the browser. cursor will create the file and set up post hog and initialization. Here it reads your API keys for environment variables so your secrets stay secure and never get committed to code. Awesome. Cool. So just waiting for cursor to go through and do that. Cool. So, it's finished um doing that. It says post hog will now initialize automatically when the app loads using those keys. Great. Awesome. So, now we're going to track the key events in our checkout flow. So, what we want to track effectively, what we're asking is create create analytic tracking functions for my checkout flow that track the following events. So there's page view landing. Um there's clicked product, there's checkout initiated, payment submitted, and payment succeeded/ payment failed. Cool. So while cursor goes away and does that um we can read about why are we avoiding PII analytics in our analytics. PII or personal identifiable information is any data that can identify a specific person like card numbers, emails, billing addresses, etc. By um yeah, and by excluding this from the data we're tracking, we avoid legal liability and stay compliant with privacy laws um listed there, there's GDPR, most popular or common one um that we're interested about online. Um and you can still understand where you just drop off without storing the sort of sensitive data and information. Should this finish now? Cool. So it's given us a summary centralized tracking functions for all checkout flow events. No PII emails, card numbers, billing addresses or transaction amounts. Uh implemented tracking events. So page re landing click product checkout initiated and payment submitted. Oh, and payment succeeded and failed too. Very good. Cool. So now we're going to integrate events into your checkout flow. So cursor just created your tracking functions. Now let's wire them up to your actual checkout components in cursor's chat. Send this prompt. So the prompt is integrate the analytics tracking function into my checkout flow. Um track page view landing when the landing page loads. Uh track clicked product when the user clicks. Oh, maybe did I already. Okay, cool. Um, cool. So, track page view landing when the landing page loads. Track clicked product when the user clicks on a product. Track checkout uh initiated when a user starts um the checkout. Track payment submitted when the payment form is submitted. Track payment succeeded or payment failed based on payment result. Awesome. So what is cursor doing here? Cursor will find the parts of your app that handle checkout product buttons, payment forms, success pages, and add code that records when users interact with them. This creates the data trail you you'll use to build funnels in Post Hog. Um and yeah, accept curses changes there. All right. So now we're going to deploy our changes to Versel. Um so as we spoke about um Versel relies on um a git repo which we've set up in the previous two projects and that's um so whenever we push anything to it it will um automatically sync and then deploy that stuff. So what we'll do is we'll just clear this. It's a little bigger. Close that. And then we can run these commands. So get add all um we can do get commit um dash m add post hog analytics tracking just push that Nice. Cool. So, what we'll do is we'll just go on to our GitHub. And we will have a little look to see if this has loaded. Yeah, it can take 30 odd seconds for the um for the push to come through. Awesome. So we can see here that um that little small that green went away when I size um this green tick means that the push to versel was successful. So go look at this. Um yeah so we can see that's that's come through. So the changes we just made not visual um but in the back end have gone through and have been applied successfully. So, we're um we're good to go. Yeah. So, we can see that on our vers dashboard, we see ready. Uh we see that the things come through and we'll we'll take a screenshot of that. How did your you deploy your postal tracking code and verify it was successful? I deployed it by running get add commit then push. The deployment succeeded which means my new code is live on the web app. This confirms that the um new post hog function tracking code um was well written and is now deployed. How did you deploy postog tracking code and verify it was successful? I deployed it by running get add commit then push the deploy deployment succeeded which means my code is now live on the web app. This confirms that the new poster function tracking code was well written and is now deployed or is now live. Awesome. Um and full screen for a second. You can use the view my work here to see what you're building. Um, so far with your documentation, I definitely recommend it. Uh, I think it's super cool and you can see see what's going on. Um, I'll keep it closed right now because it's um bit tricky with the size of our um window. Trying to show two things, but I'll go over it at the end also. All right. So, let's verify events in Post Hog. Time to see your tracking in action. Open your live versel app URL. Um, click around the app, view a product, click checkout, go to the payment flow. Um, and you should see something. So, what we'll do is we'll just head over to first cell. Uh, again, head over to our project. So, I'm just getting to the URL domain of our project. Um and in so this this tab here is making sure uh you're in the right spot for actually understanding whether you've um correctly set up the post hog thing. So you you see an output. Um it's different if you're just creating a new account or if you've got an existing account. Um so verify events post hog time to see the tracking see your tracking in action. Open your live versell app URL. Click around the app. View a product. Check out go through the payment flow. um just to get get views on the page. Technically, viewing it should be enough. Um but we'll go on here and we'll click around. Uh buy now um place order. Just get some interaction with the app. Cool. So, we're on a new account. Um but if you are on an existing account, there's steps here for how to see um see what's going on. um you just go look at the activity sidebar opposed to um just when we open this will probably see it immediately um the go to post hog you can see installation complete there um and then oh I click continue I go back um installation complete so that means uh you can see here the like it it did say like a page view required um to to complete the installation to make sure it's set up um and we Did that view view the page, refreshed it, so so it's gone through. So we're good to go. We know everything's linked up. Cool. So installation complete. Great. Uh no events detected. It's another possibility. Most likely thing is environment variables aren't um aren't linked up. It's make it's really important to make sure they are um included. Awesome. So yeah, check your API key, check deployment, check console. um try and go, you know, if everything else fails and then um yeah, you can go to the community or use the ask feature as well if you're struggling with this. So, in our case, it's working. So, this means your tracking code is properly installed and sending data. Every click, page view, and check out action is now being recorded uh without any sensitive payment data. Cool. So, click continue. Uh, and we'll click enable session replay. We can skip this for now. Um, for now and then select the free plan. Um, Cool. All right. We don't have any teammates for this project, so we'll just click finish. But if you have anyone you want to add, you're very welcome to add them. Awesome. So, we can see our see our dashboard. We can see our um view there. Um one unique view. So, that's great. We know everything's being synced up. So, welcome to your analytics command center. From here, you can see everything happening in your app. Every click, every page view, every check out attempt. Um, great. So, we're going to view our events. Um, we go to here the activity. We can see um the clicking around that was done on our web app after we've set it up. Um so we should see um all sorts of um all sorts of payments submitted there. Um all sorts of tags that we included at the start we prompted cursor to uh include in our um yeah list of events we're looking for. So look at that real user data flowing in. Notice what you're seeing is paid for landing clicked product checkout initiated payment submitted payment succeeded. This is not your entire This is your entire checkout funnel captured step by step. And look what's not there. No card numbers, no emails, no addresses, um just behavior um data. Awesome. This is the foundation of privacy first analytics. You now have the complete visibility into your checkout flow without any of the liability that comes with storing sensitive data. Cool. So we can explore our data now that events are following. Try exploring what post hog captured. Click individual events to see their properties. Notice the time stamp, browser and location. All nonpiI personal um personally identifiable information which is what we don't want. Um look for patterns events coming through in the expected order. Um so we can see um CTA or click product CTA product viewed rage click that's when a lot of clicks happen at once. Um and then clicked into buy now that makes sense. It's a page view. Um we can see this information here. I can expand it. Um look at some other stuff here. So um yeah really really over what we're doing at the moment. it's probably not quite required to look at the exact um latitude and longitude of someone. Um we can look at the browser and other things and we actually be doing some browser stuff um there later on um using using browser stuff inside of um inside of Post Hog um but not uh a requirement for this. But it's um still really good to good to see all this information how much stuff is actually tracked. The more you explore, the better you'll understand what data you're collecting. And the next step, you'll turn this raw event stream into actionable funnels that show you exactly where users drop off. Cool. So, we'll upload a screenshot of our Post Hog uh of your events in Post Hog. So, I'll just scroll to the top here and then I will paste this image in. What events did you see in post hog and why is this data privacy compliant? I verified events by checking the activity section of the dashboard. The events I saw include um page view landing. Uh what else do we have? There's buy now clicked. clicked product etc. Plenty more in there data uh this data is privacy safe because we do not have access to PII from the user. What events did you see in post hog and is this data privacy compliant? I verified events by checking the activity section of the dashboard. The events I saw include page view landing buy now clicked clicked product etc. This data is privacy safe because uh we do not have access to PII from the user. Very cool. Awesome. So finding your biggest drop off point. um your tracking is live and data is coming in. Nice work getting this far. Now let's turn this um that raw data into a visual funnel that shows you exactly where users abandon checkout. So in the step get ready to create a funnel in Post Hog tracking the full checkout journey. Identify your biggest drop off point and filter by browser to find patterns. Um awesome. What are we doing in the step? In this step, I'm building a funnel inside of Host Hog to track the full checkout journey of users. Oh, users to identify where they might be stopping along the way. This helps me understand um why conversions rates conversion rates are what they are. So conversion rate is going from effectively point A to point B where that might be like landing on the page checking out. Um in this step I'm sorry uh what are we doing in this step? In this step, I'm building a funnel inside of Post Hog to track the full checkout journey of users to identify where they might be stopping along the way. This helps me understand why conversion rates are what they are. Check your conversion funnel. A funnel shows how users progress through a series of steps and more importantly where they leave. In Post Hog, go to dashboards and click add insight. Um, cool. Go to home. Add inside here. Select new inside. Cool. Go to funnels. And now we're going to add some stitch here. Um, so our first one we're going to do is page view um landing to load. And now we're going to go through all of the um steps that we had before. Cool. So we'll select page view landing. Then we're going to click add a step. And we're going to select clicked product. Another step and then buy now clicked and payment submitted. Oops. And then last but not least, payment submitted. Uh yeah, we'll call this landing to purchase funnel. We'll select save. Awesome. So, we'll upload a screenshot of our funnel and then I'll explain sort of what we're looking at here. What does your funnel show off and where is the biggest drop off? Um, I created a funnel with steps. What do we actually use? I can't remember. We can look page view landing clicked. Product buy now clicked and payment. Oops. Payment submitted. The biggest drop off is between um I think this is like kind of a interesting one because there is no drop off in the way I because this is very dependent on how you test it. We would go go to go back and test this by um not going all the way through in a in a payment um sort of flow then you would see a drop off but we don't have a drop off because of the way I u went through the checkout. So that's we'll just say that uh it's between um uh there is no drop off currently because I did one test run through this indicates that get rid of that. um say I expect this to change when I run the or when I get more customers. Um so yeah, I kind of explained what I said I was going to, but um and we go on to this shortly, but this is not what it'll look like for a typical checkout. It'll look quite different in fact. Um so we can have a look here about um so this is the funnel there. Um so what do these numbers mean? Total conversion rate shows the percentage um I wonder if we can so that's total conversion rate there shows the percentage of users who completed all three steps. The average time to convert shows how long um the journey takes. Each bar shows how many users reach that step. So if you see your funnel that's great at this point. If you don't um then you probably don't have enough events. Um maybe there's environment variables configured incorrectly. Um also maybe a time range here is incorrectly selected. Um if you still can't see data then I definitely recommend using the ask feature by clicking on this button here to ask the AI chat. Um and then if that doesn't work then you can definitely reach out to network community and ask them uh any questions you have there. Cool. So, understanding drop off patterns. Um, this is probably one of the most important pieces of information from this project um that it's it would be impossible to to show off in the short term because it really requires a large number of um users on an app um or at least a number of users on an app, not just you clicking around. So, that's why we we've sort of given um examples here of what this could look like. So, a example of a healthy funnel and I'll just make this a little bit bigger. Oh, that's we're on now. Um, this a healthy funnel could look like this. So, you've got um a thousand people at the start and that transitions down to a smaller number that actually click and are interested on the product and then a even smaller number actually go to the buy now page and, you know, hopefully pay. Um, you can be super granular with this. You could go um like how close they are, how close their cursor is in terms of proximity to different buttons and whatever, but this is like the the overall outlining, a common outline you would see for the sort of um yeah, any commerce store with a a product page, a landing page, a product page, and a buy now space. Um, so you've got a total conversion rate of 12%. That's reasonably good. That's over one in 10 people that visit your site at all will, you know, purchase your product. So that that's, you know, pretty good. Um, obviously ours is perfect because we just did one run through when we were testing it. Um, and we got an average time to convert there, which is important. Um, but yeah, this is what you'd expect to see a gradual drop off, probably quite a large drop off um towards the buy now, actually paying for the item. Um, but this this this is healthy. This is what it should, you know, in a perfect world, this is what it would look like. Um but over here if you see a cliff a massive drop off in one specific step that's not normal um user behavior that's a problem with your product. Um so you can see here plenty of people thousand people same same group viewed this landing page and then um the next landing page only gets um what 20% of the viewership opposed to you know you'd expect more people to just go from landing page to actually initially looking at your product. So, this should tell you, hey, look, there's pro maybe this is that's a really hard button to find. Maybe maybe the landing page takes a long time to load. Um, it really allows you to ask questions. It won't give you an answer immediately. It'll tell you where to look. Um, so yeah, that's that's good to know. Um, yeah, so as it notes here, in a healthy funnel, you'll see a gradual drop off at each stage. Some users will always leave. That's normal. Maybe they were just browsing or got distracted um or decide to buy later. That's normal. Um but there's still a you know consistent drop off opposed to a huge one and then flat. That's um unusual. Where should you focus? So landing page product um click low. Your products aren't grabbing attention. Try better images, clearer titles or featured products, product to buy now. Um, so the the landing page of products is what we saw here. Um, this is a very large cliff drop. So you'd be wanting to look into products um like better images, clearer titles or featured products potentially or other stuff too. It really depends on the store or other environments, you know. Um, product to buy now is low. Users are interested but not convinced. Check your pricing, display, add a reviews or improve your call to action button. Buy now to payment low. Um, users might uh users want to buy now but something stopping them. Look out for checkout bugs, confusing forms, or missing trust signals. Awesome. Okay. So, now we're going to filter by browser that I spoke about before. Um, we were, yeah, we were talking about how we're going to set up uh, filtering. So, now we're going to do that. Cool. So, we're just going to click edit here on the same landing to purchase funnel. And then we're going to scroll down and see the um the plus add a breakdown button. Um, and if you're wondering what the difference between a breakdown and a filter is, a filter will simply like if you you could filter also by browser. If you wanted to filter by Chrome, you would, in this case, I would see 100% of users on Chrome because I open this on Chrome. But if you if all of my users were on Firefox for some reason, I would see nothing in here. So that's, you know, good for some things, um, but not so good for others. If you're interested in, you know, what percentage of people are on each, then a breakdown is is should be what you go to use because you can, you know, set up Chrome, Microsoft Edge, um, Firefox, Safari, whatever, and you can understand people view from different browsers and that's, um, yeah, that makes that really clear. So, that's what we're going to do. Cool. So, we're going to click plus breakdown. Um, we'll select browser here on the menu. And now we can see that, um, Chrome is 100%. So, um, if we were to, and I might give this a go. See if this can take a little bit of time for this to load. I'll just or to update, sorry. See if um, Microsoft Edge wants to play ball. just refresh and click around for a little bit. Um, but yeah, so if we try go to buy now and then make a purchase. This we might get lucky and this might update um live but also might struggle. So I'll click save and see if we can uh just wait a minute. But it it will definitely come through. It just can take um a minute or so sometimes. Yeah, we might just give that just leave it at that. But it it will come through. It just needs some time post. So it can take a can take a minute sometimes. Um cool. All right. Very good. Yeah. And this this shows the um each of the rows shows the different browser which is really really clear. Um and yeah, so here just like that it's quite hard to immediately see um immediately see changes because it can take a little bit of time and sometimes like that might not even have registered as a as a thing as an event in post hog. Um, so yeah, it's it's very hard to get a bunch of data and actually have a good look at it here on a real web app in a project. So we've given you some example tables and sort of the idea is you you use those to understand what you know a good um layout of uh browsers in this case before it was um fall off for customers going from looking at your website to go and uh paying at the end. this time it's um you know you get to spot something wrong with a specific browser because there's actually a lot of stuff a lot of nuances between browsers um namely Firefox and and Chrome um that will um something might not render as you expect on one um shaders is is famous on Chrome for causing issues um and other stuff on Firefox can cause similar problems. U all these browsers are actually built on Chromium. Um but there's nuances between them. Um cool. So this is what a good, you know, a good example would look like. Very similar conversion rates between. Nothing really you can take home from um this apart from the fact that it looks like your site's functioning well across all um all browsers. So healthy breakdown shows similar conversion rates across browsers. All browsers should perform roughly the same. users on Chrome, Firefox, Safari, and Edge should all be able to complete a checkout. Cool. Very good. Um, so this is what a browser bug could look like. So if you look here, um, all of the things have pretty standard outcomes, but in this case, Firefox has dropped all the way uh 95% to the very end. So clearly um the I I would almost say I can almost guarantee that in this situation there would be a bug which um stops people maybe on desktop or mobile or whatever your larger audience is from checking out due to something special on Firefox. Um so yeah definitely good to look into that. So in one browser or if one browser has significantly lower conversion you found a bug. In this example Firefox users are dropping off way more than other users. This indicates a potential issue with our app running on Firefox browsers and is definitely worth investigating. Awesome. So, time to open Firefox. Go through your checkout flow and find what's breaking. Um, why do browsers behave differently? Not all browsers render web pages the same way. Even Even when we use identical HTML, CSS, and JavaScript, each browser has its own rendering engine. Chrome uses Blink. Safari uses WebKit. Firefox uses Gecko which can interpret code slightly differently. Common causes are CSS flick box or grid rendering differently on Safari. Newer JS versions not supported on older browsers. Form elements um styling differently in Firefox and scroll behavior on mobile browsers. Cool. So, we'll just send a screenshot of this there. What did the browser breakdown reveal about your funnel? I add a browser breakdown by um editing my funnel and uh enabling breakdown for browsers. the conversion rates show um a perfect in this case perfect um maybe not perfect a 100% um breakdown for 100% chance breakdown for Chrome users. This reveals that um with 100%. Check out success rate. This reveals that our site's Chrome users um are able to uh successfully use our site. So I added so what did the browser breakdown reveal about the funnel? I added a browser breakdown by editing the funnel and enabling breakdown for browsers. The conversion rates show at 100% breakdown for Chrome users with 100% success rate in checkout. This reveals that our sites uh Chrome users are able to successfully um use our site um or Chrome users are successfully are able to use our site. Very good. All right, turn drop offs into conversion improvements. You've built um the analytics in infrastructure. Now, let's walk through how companies like Wower use the same data-driven approach to increase conversions by 10% and reduce time to purchase from 2 days to under four hours. Since you're setting this up for demo purposes, you won't have any real customer data. Instead, we'll walk you through the methodology so that you don't um so when you do have traffic, you'll know exactly what to do. So in this step, get ready to learn how to form hypothesis policies from funnel data, understand common fixes for each drop off point, and verify your session replay is privacy compliant. What are we doing in this step? In this step, I'm learning um how to turn my funnel data into um or learning how to increase uh my conversions. Um converting convert hypothesis into actions to turn a low conversion rate to a high conversion rate. This increases profits. This methodology helps me uh oh we can say that to there increase the number of customers and the checkout. What are we doing in this step? In this step I'm learning how to increase my conversions converting hypothesis um into actions to turn a low conversion rate into a high conversion rate. This methodology helps me increase the number of customers in the checkout. So data-driven improvement process when you have real uh real life traffic, here's the process you'll follow. Identify, find your biggest drop off point, hypothesize, form a theory about why users are leaving and make a change um address to that theory and then measure compare the before and after conversion rates. So let's walk through each step with real world examples. So let's spot the difference. Um let's take a look at the example data from the previous step. Um this doesn't look like a normal checkout flow. Instead the gradual drop off instead of a gradual drop off at each step. Um there is a sharp cliff at one specific point just here. Um so how do you spot an abnormal funnel? A healthy e-commerce funnel has a gradual decline. maybe 60% of your products, 30% add to cart, 15 checkout, 10% complete payment. If you see one step with a massive drop, others stay flat. That's your red flag. Something's broken instead. So, let's form a hypothesis. Um, start with the data. Look where the data drops off. Drop off happens. So, that's yeah, ask why, not what. Don't just say users aren't clicking. Ask why. Um, so in this case, it may not be attractive enough. Um, is the button hidden? Is the price scary? Make it testable. Your hypothesis should um suggest a specific change you want to make and measure. This page is bad. Isn't testable. Users can't find the products. Suggest um so users can't find products is a better one. Um suggests a fix you can verify. Um now let's apply this for our example. See that clip between landing page got clicked. users um are landing on the site but have very few clicking through to view products. However, once users do get to the product page, they have a very high chance of checking out and actually completing the payment. Um this tells you exactly when you need to focus the landing page and doing its job for a landing page to product click drop off. Common causes are products aren't visible, no clear call to action CTA. Landing page is confusing or cluttered. Users don't understand what you're selling. Um why do you think um this pattern um can be seen? So what's yeah what's your hypothesis? I think that um say products aren't visible or compelling enough. I think this because the conversion rate from landing page to um checkout or to uh product page is very low. Awesome. So, let's fix the issue. Now, let cursor work its magic. Give it your goals and hypothesis. It will figure out the right changes to make in Curs's chat. Send this prompt. So, we'll hop back over here. open up this chat and then I will send that prompt off based on this hypothesis. Improve the landing page and fix this drop off. Um that's that's the prompt. So why don't we need to list specific fixes? The power here is that cursor infers what change to um to make based on your hypothesis. Different hypotheses lead to different solutions and cursor figures that out or hopefully it does. Um cursor will propose changes based on your hypothesis. Um you can see here it's improving the landing page to increase product visibility and click through. Um why do these changes make sense? Look at what curs suggested. Um each change targets the clickthrough problem. So in the case of this screenshot here, not necessarily what we've asked it. uh it's suggested feature product section um puts products in front of front and center so users see them immediately larger product images a shop now call to action uh this gives users a clear next step um not leaving them guessing and prices on landing page so reduces the friction um rather than just showing them last minute um cool so we keep all um that over here Um, and then these aren't random changes. Each one directly addresses the users don't know where to go uh by making proxy impossible to miss. In real scenario, you deploy this and watch the funnel to see if it um if that step improves. Um, cool. So, I'll just take a screenshot of scroll up a little bit some of the changes here. What hypothesis did you form and what changes did cursive propose? My hypothesis was that we can actually copy the hypothesis cursor proposed changes like said uh increasing product visibility. These changes will uh address the problem by uh making users um making it more clear for users what the product actually was. So my hypothesis was the products aren't visible or compelling enough. Um I think it's because it's low conversion rate. Cursive proposed changes like increasing the product visibility. These changes address the problem by making it more clear for users what product actually was. Cool. So hypothetically after we've gone and deployed this um we would see something more like the healthy one we touched on earlier. Um so this looks like this. So a much higher like this isn't necessarily higher. That's another issue. Um but this is much higher. So much much more users after we've made our changes uh we expect to be going into the product page and actually being interested in the product rather than just um jumping off and thinking you know well I don't actually understand this. Um cool. So some users as we talked about before will always leave um distracted just browsing. Um that's normal. The goal is eliminating the cliffs. Um and you could argue there's a cliff here a cliff here. Um, but what is your hypothesis? Um, what if your hypothesis is wrong? What if you want to see exactly what users do before they left? That's where session replay comes in. Um, so session replay is like literally looking at what that person did. Um, awesome. Cool. So, we will go over to our post hog space over here. We'll go into search and then we'll go into session replay the app and we should be able to see a flow here. Um, obviously this is what was us just before um just going through and doing it. This might be on Edge, not sure. There you go. Microsoft Edge. Um and we can see what what this looks like. Um and you know in this case this person didn't go through or me didn't go through and um they just stopped at this product page. So maybe you know if we can guess we have a look um you know it's possible that maybe there was a visual bug that stopped them from doing something here and and then they thought okay well I can't check out so I'm not going to. Um but yeah that's that's really the beauty of this. That's the bit you can't see with um just raw numbers. You know, there there are bits and um yeah, that aren't clear through that. Um and this gives you the most granular approach um to that. So, yeah, super useful to be able to come come back and check and you can see, you know, where people were, what OS they were on, the number of clicks, keys, etc. Um so, super useful. Look at this. The user made it all the way to the payment page, but never completed the purchase. Um, was the price too high? Did they get distracted? Did they did they did something look broken? This is exactly what the kind of insight This is exactly the kind of insight that helps you form a better hypothesis. So, what to look for in session replays? Um, confusion. So, mouse moving around erratically, frustration, clicking a bunch in somewhere that isn't really a button or something like that. Missed elements. User never scrolls where products are. So, there could be products at the bottom of the page the user might be interested in. They just weren't u wasn't clear to scroll. So they didn't didn't see them dead ends. User clicks something expecting it to work but nothing ever happens. So in this case I think the sign up button or the login button doesn't do anything on the site. So that you know hypothetically could be an issue. Um yeah session replays um turn your hypothesis from a guess into a confirmed insight. Um now you know why users left and you can fix that with confidence. So we'll just put a screenshot of the replay. What did you observe in the session replay? And what is the optimization cycle? I watched the replay and observed uh the user getting stuck. I know it was me in this case, but just testing sake getting stuck on the product page. The optimization cycle involves um improving the flow from the product page. This helps me because I was unclear until now why the user was stuck. You know, hypothetically, this person would be um would have run into a um maybe a UI issue. Um, in this case it was just because I clicked off but yeah that's what we can do here. What did you observe in the session replay and and what is the optimization cycle? I watched um session the session replay and observed the user getting stuck on the product page. The optimization cycle involves improving the flow from the product page. This helps me uh this helps me um uh this helps me improve or increase the flow or the amount of customers that go from product page to checkout. Um, it was unclear until now why users were getting stuck. The optimization cycle what you just did is the core of datadriven optimization. So first we measured collect data on user behavior hypothesize form a theory about why users behave that way. So that's oh that was unclear. collecting data is, you know, through this or the um the make go back um here or like the the landing page to payment submitted flow. Um the change is implementing a focused improvement that was in and improving the uh product visibility here using cursor and then measuring was comparing results afterwards. Repeat the cycle continuously. Um small improvements compound over time. Definitely smaller things are the best option here. Cool. So now we're going to do AB testing um like Booking.com. Ready to level up? This section is for those who want to take their skills further. Booking.com runs thousands of AB testing uh AB tests simultaneously. They don't guess which button text converts better. They measure it. You'll use the same you use Post Hog's built-in experimentation to test a buy now versus complete purchase button while staying privacy compliant. Cool. So, in the secret mission, get ready to test setup post hog AB testing feature. Create an experiment with two button variants and track or track conversions as your goal metric and deploy and measure which variant wins. Uh in the second mission, I'm setting up a slashB testing to test which style of uh payment button leads to higher conversion. This removes guesswork by giving us the opportunity to gather data to gather data on both aniously. So what are you doing the secret mission? In the secret mission, I'm setting up AB testing to test which style of payment button leads to higher conversion. This removes guesswork by giving us the opportunity to gather data on both at the same time. Awesome. What is AB testing? AB testing, also called split testing, shows different users different variations of the same page. Half C buy now, variant A, half C complete purchase, variant B. Um, this could be an entire page difference. It could be an order of instructions. It could be really anything. Um, it removes guesswork from optimization. Um, and we measure which one gets more conversions. Awesome. So, now we're going to go into Post Hog. Let me resize it. Um, find the experiments app. So, we'll go to search and go to experiments. Click on that. Click new experiment. Enter a name. Uh we're going to do checkout text test. It's all lower case. Check out the screenshot button variant. And we can do a hypothesis. Um, so changing the button text from buy now to complete purchase will increase conversions because it's clear about what's happening next hypothetical. Okay. Configure your experiment. Um, awesome. Cool. So, um, in exposure criteria, leave that one as default. Um cool. So here um we can we're suggested we update this to um use first scene variant. Um and we don't filter out test users. this. We're doing this not because necessarily it's like um you know in prod this might make a bit more sense to um leave this as is, but we're doing testing and we we want to see our test data in AB testing. So we're going to we're going to leave it as that. Um cool. So we'll call this um test. So the goal is uh tells P post hog um what how to measure success. Um cool. So if we click add primary metric, click single use. Um we can give this a name. Um really anything is fine. Um just go with be the same name we used here and we can apply the same flow here. So page view landing I say the same flow I mean the same flow we set up before for the funnel. Um and then we can just add clicked product then we can add buy now. And last but not least, we can do payment submitted. And yeah, our goal is to increase um conversion window limit. Um we can leave that experiment duration and step order is sequential. Um you could do any order. Um but really we're interested in got people in this test experiment going from the start to the end in that order. Um yeah and we can see some events uh in this in this setup so far which is good. So we can click create. We can click save as draft. Close that. Um and yeah then we can click launch as well. Cool. So we see zero exposure so far. Um, that makes sense. Nothing's um have done anything with this experiment running. That's cool. So, now we're going to go implement this into cursor. So, implement the experiment in your code. Now, let's add the experiment to your checkout page using cursor. So, we're going to open cursor back up again, and we'll send this prompt off. Cool. So I can explain what these uh what curs is being asked to do. Um I need to implement post hog AB testing for my checkout button. I need to implement um so yeah I need to implement AB testing my checkout button using post hog um refactor the button rendering in this component to use um the use post hog oops um hook from post hogreact um requirements correct import do not use um this correct import logic so instantiate the client um instantiate the client, retrieve flag value safely. Um so get feature flag feature flags will be set um to be test or control. Um and then there's two variants test or control as we just spoke about and if it's test you see the complete purchase and if it's control you see place order uh and insert the import um the import and the hook logic where they belong. Cool. Okay. Again, just as important, we um push this to Versel um to make sure this is all working. And we do this by pushing our code to GitHub again using get add commit and push. So, we can actually just copy this. Um I'll clear this and I will just paste that in. Um, it'll just do a push. Then we can go and check GitHub to see if this went through correctly. Should be fine. Cool. That looks correct. Give it a second. Awesome. So you can see the green tick again here. And if we click details um you can move to versel. We see it's um deployed. So that's great. Nice. All right. That yep. So we can check first. We can see it's deployed. It's great. Um, oh, we launched a little bit early. It doesn't really matter, but we did it. Um, let's get ourselves open. You click the launch button up there. Um, so we're going to test our experiment now. Um, and we're going to use incognito. Um, this is really just a catch all in case um, Postg doesn't want to capture events uh, from test users. That's kind of um, yeah, just just to be careful. We can we'll do a few on here. Um, just just cycles through. see if it picks up anything, but it may not. Um, and we'll just take note of this. So, complete purchase. I believe that the other one, uh, we can look at the cursor prompt. Um, so control is place order. Um, test. So, complete purchase. So, it's currently on test at the moment. So, it's displaying there's a 50% chance of it displaying this. there's a 50% chance of it displaying the other one um which is the um what is it place order which is the control group. So that's like by default it'll be that. So if we click this we should get a full run through um go back to store. Um and then I might just open this into a new incognito tab. I'll just paste this link in. So we'll go from here to clicking on the product, clicking by now, and then place order. So I think that's the control group there. So that should be enough data, but we'll see if um do two more just to be just to be thorough. So maybe this one we actually just end it here. Um we'll end it here. So we won't actually click place order. Uh one last one. This one will just go to here. So hopefully now um we'll be able to see if we open um post hog it could take some time for information to come through. Um there you go two exposures. Um very good. So this we sort of touched on why incognito. Um but post hog assigns variance based on users distinct ID usually stored in a cookie. using incognito gives a fresh identity each time. So you see both variants. Um so we're going to monitor results now. Um so we can what to look for conversion rates per variant statistical um significance is there a difference or meaningful or just random noise um sample size how many users have used each variant. So we're going to wait until post hog shows significant um before drawing any conclusions. As rule of thumb, you need at least 20 conversions per variant. Um, so in this case, obviously we only have one, so it's not really that relevant, but you can see here, um, the conversions, um, drop off. So that's, you know, really interesting bit of information, um, that, you know, we can we can pull together. So we're getting both. We saw the two different buttons. Um, that's really what AB testing is about. And you know, one group may, you know, may see a very large conversion improvement, a much lower drop off. Um, and so that that would could be the this um this test. In this case, it looks like the test is significantly worse because all all the users me I I dropped off by that point. Um so, you know, that's that's all real um interesting information. Cool. So once post hog shows statistical significance if test wins update your button to complete purchase. So that's like the act on doing this. So in this case it would be ideal not to change and if this was an issue it it's not the button isn't clearly isn't the issue. Uh if control wins keep by now your original button was better. Um that's what happened in our case. Uh and then click complete experiment and post hog um to to go through and and set this up or to set this up to like verify that you're finished and you know any anyone on your team can can understand that. Awesome. So what you built um you've set up proper AB testing a proper AB testing framework feature flags so test and control um which is what um it's 50% chance of triggering one and it dep that renders a different button on the user screen um goal tracking so that measures actual conversions that's this stuff here primary metrics exposures privacy compliance that keeps um PII so personal identifiable information um out of your experiments. There's no phone numbers or addresses or anything in here. Um, statistical rigor um that tells you uh when results are meaningful. This is the same approach that companies like Booking.com or Netflix use to optimize their products. Small measured experiments that compound into big improvements over time. Awesome. So, let's take a screenshot of this. Have my cursor in it. What did your AB results reveal about your checkout button? I created an experiment testing checkout button variance. The winning variant was the control group. This showed that I should not change the button. Why did your AB test results what did they reveal um about your checkout button? I created an experiment testing checkout button variants. The winning variant was the control group. This showed that I should not change the button. Great. Um, so ideas to go further. Uh, you want to keep experimenting? Try these. Test price display. That's $9.99 versus $10 versus less than a coffee. Um, changing numbers there, maybe having text instead. Test button colors, test page layout, test test social proof. So, adding number of customers or purchases week is a great way to test uh whether your customers are really unmotivated perhaps to to make the purchase. Remember, one variable variable at a time. Test um the button text first, then move on to colors, then layout. Um no point doing multiple. That will just um make that effectively one change. So, you know, if you're really keen to change an entire page, you could do that, I guess, but you you're only testing like everything at once. You you can't make a big change and then just pick out one element and then think that's hey, users are on this page because of this. It's um it's everything. Cool. So before you go, I'm going to clean up our resources. Um, nice work completing this project. Let's clean up the resources we created. Resources to delete. Um, so we're going to decide uh whether to keep or remove Post Hog tracking. Um, so we can keep it. Um, we can disable it temporarily or remove Post Hog entirely. So we can uninstall it, delete Post Hog and Analytics.js JS we created remove the initialization code. Uh remove tracking calls from components. Optionally delete um pro uh the Post Hog project from the dashboard. Um I'm going to go ahead and keep it. Uh I I don't mind. I'd love to see more users on my app. Um so Post Hog's free tier has no ongoing cost. You get a million events each month free. If you're continuing the Phop series or want to keep the tracking uh your apps analytics, you can keep Post Talk account active and keep the tracking code in your project. No action needed. Everything stays as is. All right, that's a wrap. You've built a privacy first checkout analytics um platform that reveals where your um users drop off without storing card numbers, billing addresses, or PII. what you learned, how to set up post hogg for privacy first analytics, tracking users behavior without storing PII, building conversion funnels that reveal dropoff patterns, and using data to form test hypothesis. Um, you also learn what you can and cannot track under PCI, DSS, GDPR, NCCPA. That's various um privacy compliance laws in different places, California, Europe, elsewhere. and the measure hypothesize change measure optimization loop cycle. Got a quiz here. What is the primary goal of setting up privacy first analytics? To store all your data to track where users drop off to exclusively monitor performance to integrate your social media platform with marketing campaigns. Um I would say it's to track where users drop off in the checkout flow without storing sensitive information. according to the project. Why is it crucial to avoid tracking PII? It slows down performance. It's not technically possible to avoid uh legal liability. Which three post hog products are recommended for selection during the initial setup in this project? um feature flags, AB testing, data warehousing, product analytics, session replays and experiments, CRM, marketing, user authentication, database management, and this guy. What is the recommended privacy setting for sensitive input fields such as credit card numbers? Um, when using post hog session replay, they should all be visible, only be visible to administrators. They should be masked with asterises. They should be fully visible. They should be automatically deleted session afterwards. Uh I think it's B in post hog funnels. What does a cliff in conversion rates um between two consecutive steps indicate? a healthy journey, significant problem or broken experiment, an issue with the analytics, the users are highly engaged completing steps very quickly. A significant problem or broken uh experience with the product um at that specific step. Awesome. What's next? Um you've completed the post hog analytics setup. You can now track users user behavior without storing sensitive data. Build conversion funnels to identify drop off points. for hypothesis and test improvements. Run AB testing to optimize conversions. Um yeah, and make sure to join the network community as well. Join the discord um to stay connected. So what were the key tools and concepts you learned in this project? The key tools I used include um Postthog and Vel. Key concepts I learned include PII um AB testing and uh funnels. How long did this project take you to complete? Took me approximately uh 1.5 hours. The most challenging part was um getting the AB testing to work. I it was most rewarding to see the um AB testing buttons changing the skills I learned. I want to build a larger e-commerce app. A larger track commerce app. Thanks for this project. Why did you do this project today? Um I did this project to learn how to track customer analytics on post hog. Another analytic skill I want to learn is how to um use AB testing to change. We do A B C testing three experiments in one page. Cool. All right. So, shows mission accomplished here. So, we can click that. Um, wow. Look at that. How good. So, we can share documentation. Um, look at this amazing documentation. All our screenshots and text. We can edit stuff here and update screenshots. Um, it's great. Awesome. All right. Well, that was another project in the 21 to 21 projects. 21 project 21 days. Um, final project in the fin ops, financial ops series. Um, it was on Post Hog and looking at setting up experiments and understanding data in your web app. Um, I hope you have enjoyed um, the series. I'll be back next time um to do some disaster recovery stuff. Another requested project slowly ticking through them. Um yeah, super stoked. All right, I'll I'll I'll see you guys later.","This detailed summary captures the essence of the final project in the **AI x FinOps Series**, focusing on establishing robust, **privacy-first analytics** to drive e-commerce conversions using **PostHog**.

---

## Mastering Privacy-First FinOps: Building High-Conversion Checkout Funnels with PostHog

This project, the culmination of the **FinOps Series**, details the critical process of implementing advanced, privacy-compliant user analytics using **PostHog**. The core objective is to gain complete **visibility without risk** into the e-commerce checkout flow, enabling data-driven optimization while strictly adhering to global privacy standards.

### 1. Core Objective: Visibility Without Risk (Privacy Compliance)

The primary challenge addressed is the liability associated with traditional analytics platforms that often capture sensitive information. This implementation ensures strict **privacy compliance** by avoiding the storage of **PII** (Personally Identifiable Information)such as card numbers, emails, and billing addresseswhich is necessary for standards like **PCI DSS** and **GDPR**.

*   **Key Takeaway:** The goal is to track **behavioral data only**, allowing developers to see exactly *where* users abandon their purchase without ever touching sensitive payment data.
*   **Technology Stack:** The project builds upon an existing e-commerce setup using **Vercel** (hosting), **Next.js** (framework), **VZero**, and **Stripe** (payments).

### 2. Implementation: Setting Up Event Tracking

The first step involves integrating **PostHog** into the application and defining the entire user journey through key **events**. The process is streamlined using **Cursor** (AI coding assistant) to automatically generate and implement tracking functions.

*   **PostHog SDK Integration:** The **PostHog SDK** is initialized within the **Next.js** app hosted on **Vercel**.
*   **Critical Events Tracked:** The full conversion path is mapped by tracking sequential **events**, including: `page view landing`, `clicked product`, `checkout initiated`, `payment submitted`, and `payment succeeded/failed`.
*   **Deployment:** Changes are deployed seamlessly via **Git** integration with **Vercel**, ensuring the new analytics tracking code is immediately live and verifiable.

### 3. Data Analysis: Identifying Conversion Drop-Offs

Once data is flowing, raw events are transformed into actionable insights using **Conversion Funnels**.

*   **Building Funnels:** A funnel is constructed to visualize the step-by-step progress from the landing page to successful payment.
*   **Diagnosing ""Cliffs"":** A **healthy funnel** shows a gradual decline in users at each step. A sudden, sharp drop (a ""cliff"") between two steps indicates a significant **problem or bug** in the user experience that requires immediate attention (e.g., products aren't visible, a confusing form, or a broken button).
*   **Browser Breakdowns:** The use of **breakdowns** (filtering funnel data by **browser** type) is crucial for diagnosing technical issues. If conversions are significantly lower on one browser (e.g., Firefox), it reveals a rendering or functional bug specific to that environment.

### 4. Optimization: The Data-Driven Improvement Cycle

The analysis leads directly into an iterative improvement process known as the **Optimization Loop**: **Measure, Hypothesize, Change, Measure**.

*   **Hypothesis Formation:** Based on where the drop-off occurs, a **testable hypothesis** is formed (e.g., ""The button is not compelling enough"" or ""The landing page is confusing"").
*   **Session Replay:** **PostHogs Session Replay** feature is used to visually observe user behavior leading up to the abandonment. This turns a statistical guess into a **confirmed insight**, revealing confusion, rage clicks, or missed elements without compromising **PII**.
*   **Actionable Fixes:** Hypotheses guide focused improvements (e.g., increasing product visibility, clarifying CTA text), which are then deployed and measured for impact.

### 5. Secret Mission: Advanced A/B Testing

The project concludes with the setup of a professional **A/B testing** framework, mirroring the techniques used by major tech companies like Booking.com.

*   **Experiment Setup:** **Post",2026-01-16T01:49:28.796232
NextWork,Meet the Humans of NextWork: Krishna Kapadia,4_hKtOO9Cv4,"me. >> I need Yeah, I need a haircut. That's a great photo. >> Hello. Hello everyone. I'm Maya from Nextwork and today we have Krishna who is on the limelight. Um Krishna is someone who has really helped shape next to where it is today. He's joined at a very early stage. He is the founding engineer at Nexwork and he's moved from places like Reagan to Canv and it's so amazing to have you here Krishna. >> Damn, it's great to be here. That's an incredible introduction. Hello Maya. Hello everybody. It's great to be here. >> Amazing. Um thank you all for joining the humans of network. Tell us where you're joining from. put it in the chat if you've got any questions. Great time to ask. Let's um start from the beginning, shall we? Krishna, >> uh tell me um a little bit about what got you interested in engineering. >> Yeah, absolutely. I mean, for as long as I can remember, um, I'm 28 now, so as long as I can remember, when I was a little kid, I just loved like building things, you know, Lego or like the the cheap $2 shop knockoff Lego that you used to get. I I'd love playing with that, building stuff uh with like wood, hammers, nails, all that kind of stuff. That was always super super interesting to me growing up. Uh, and then one day, uh, my parents brought home a computer. >> And then, as you do when you're like 12, 10, probably earlier, 8 years old or something like that, you click around, play the play all the games. I remember there used to be this like pinball game, which is cool. And then slowly I just started, you know, looking more into computers. And then I found coding. I thought that was cool. And then it sort of kind of just my my love for building stuff sort of evolved. You know, it went from building things physically to building things digitally. You know, I found that I could build things much quicker digitally than I could, >> you know, in in the physical world. So, I thought, hey, >> this is pretty cool. Uh, and I took to that really well. And then, yeah, that's kind of what what got me into engineering. I always had this cur very curious uh builder mentality. I love to learn how things worked. >> I remember one day my mom brought me a remote control car. It was like this big. You could drive it around. I thought it was really really cool. And the first thing I did was I took it apart and my mom got really really >> No way. >> Yeah. Yeah. I took it I took the whole thing apart. Like I took the the case off. I unscrewed everything just to figure out how it worked because I just loved figuring things out. And then yeah, I guess that's kind of led me to where I am. I am now, you know, building product, building next work. >> That's amazing. That's really cool. It's it's so um unusual to have that kind of clarity so early on that you wanted to build things. You want to know how things work and to take a toy and break it apart. >> Yeah. >> People do that. >> Was uh your mom okay with that? I >> absolutely. >> Did you put it back together? >> She was >> Well, I didn't know how I I took it apart. I was like, ""This is great."" And then, yeah, putting it back together again was uh the hard part. That's my dad had to do that, but but yeah. >> Okay. >> Happy. >> That's cool. That's cool. Um, and I I also appreciate like I was going to ask how did you know that you wanted to do computers, you know, computer science engineering as opposed to mechanical or something more hardware related. But um, yeah, the answer about like the quick >> Yeah. Yeah. Definitely this the speed the speed at which I could build something. I initially I got really into woodwork and building things like clocks and cabinets at school and this kind of thing like saws, hammers, nails, but that takes a long time. >> And then when I got into computers and that that type of building was so much quicker, I could get what I wanted to get done so much quicker and see the change so quickly. I thought, ""Oh, this this is this is what I want to do."" >> It's very it's very simple for me really. Like that's just what I wanted to do. So, >> yeah, it's a bit of that. Is there a specific aha moment that you remember that like >> this is it maybe selecting your degree or >> I remember when I my dad always used to ask me oh what what do you want to want to be you know what do you want to be you need to know what you want to be and then you can kind of focus focus on getting there and breaking that goal down and like figuring out step by step. >> Yeah. >> And initially I wanted to be an inventor. Same sort of reason, you know, building building new things, building things. And then I think I think might have been my uncle, oh he asked me, what are you going to invent? I was like, well, that's a great question. To be an inventor, you kind of have to know what you want to invent. M >> and then I think from from that point I really thought of like oh so what what can I invent and I looked around like well everything's already been built but like digital stuff that's not all built yet. >> So I guess that's kind of what what took me down this path. It's there's there's no one exact aha moment, but it's a small the series of small moments throughout my life that kind of drove me in this direction. >> Yeah, it's really cool. And uh how did you make it like what was that first job? How did you get that first job? >> What was that process like? Yeah, I mean the f the first job first job in my experience is is the hardest one to get. Um I it I initially I tried to get into a program that helps you meet a lot of employers and then it's called summer of tech here in New Zealand. And the whole idea is it's one three-hour event where you go and you meet a whole bunch of different employers looking to hire someone and they interview you and this goes on for a few more days and then they'll offer you the position or not. >> So this was in my first year of uni. I signed up for this >> first year of uni. >> Yeah. Yeah. First year of uni. And then unfortunately I wasn't successful. I didn't get a position. And it's it's very common for that to happen. Well, I thought, well, what am I going to do now? You know, I've put my heart and soul into this. I wasn't able to achieve it. I was like, ""All right, great. So, if if no one's going to give me an internship, I'm going to make my own internship."" So, I I met up with um my well, my parents at the time ran a factory and I said, ""Hey, you guys need a need a website or something?"" and they said, ""Well, we don't know. Come have a look."" So, all right, cool. So, I went and I I had a look at how they were doing things. And one problem that stood out to me was invoicing at the time. It was an Excel spreadsheet. >> And they were just they were just manually typing things in there. And I thought, hey, this could we could make this into a whole system, like a whole website you can go to and you can fill it in and it'll automatically send the emails out and all this kind of stuff. So over the course of about three months, I built that and that was a really cool experience, especially when the stakeholders are my parents. So I could ask them at any time of the day or night, hey, what do you think of this? Look at this. This is what I've done. Have a look. What do you think? >> Yeah. >> And they ended up using it and integrating it. And that was kind of how they did invoicing and customer management from that point forward. Anyway, >> uh the following year I signed up for summer of tech again and I knowing now that I had this project under my belt, it gave me something to talk about. So I talked with a whole bunch of employers and I went I went through the stages and eventually I got a position at a company called Ray. Um, but I guess the the core things really that I learned from that experience is people when you're looking for a job, people are looking at you and they're asking the question of what what can you do for me? You know, if I'm trying to employ someone, I'm trying to look for a person that can do this particular job or at least learn to do this job. So having projects that you've done or apps that you've built or groups that you've been a part of that shows you know you took initiative or you can maybe you can build something maybe you led something maybe you helped someone in some way and those things helped me get the job but I think all up in terms of applications I probably would have applied to maybe 80 or 100 different positions before I ever got the internship. >> Really? >> Yeah. Yeah. >> What's it like to deal with >> 78 rejections? >> Um, after maybe the first five or six, it's it's it's kind of like, oh, okay. You know, it's not it's not that big a deal, but the first few definitely hurt. Put your heart and soul into this application. I I I really really want this. And then it's just like, ""Oh, hey, try again next year."" Or, ""Oh, we're looking for someone with some experience."" You know, you get a little bit down, but you eventually you get there. It's just a matter of when. You just got to like stay consistent with it. Yeah. >> That's really cool, Christian. I really like the story about the initiative that you took as a first year moving on to the second year college student and um saying hey what can I build what's useful and and just starting at home and with your parents I don't know if universities necessarily prepare you for that I don't know if you had all the skills to do that um given what you had learned in your first year. So you were learning on your own. What was that like? >> Yeah, definitely definitely. Um I learned to code I'd say fairly early on around 12 or 13. So I'd already been coding a lot before I got to university. So a lot of a lot of what I know up to knew up till that point was selftaught like web development, databases, how to make an API, all these type of things. It was all was all self-learning. Um through yeah just YouTube and googling stuff and trying to figure it out. Um yeah, I think throughout throughout university I learned core principles that I didn't know beforehand. >> Yeah. >> Things like data structures, algorithms, that's the kind of stuff you learn in uni. >> Yeah. >> But a lot of the stuff, even the things that I rely on now, the vast majority of it is has been selftaught. >> Yeah. >> Yeah. Especially when you're in tech and and with in the day of AI, you always have to upskill and keep learning, right? Yeah, 100% 100%. Every day something new comes out or there's a new innovation that comes out that can change dramatically change how how we learn things, how we program things, how we architect different things, different services we can use. And a lot of the time learning those things is self-directed. You know, you need to >> you need to go out and search for these things and try and find, hey, what's what's the the new thing I can learn? where is the industry going so I can get ahead of it. >> Very cool. I'm I'm going to come back to that question and ask um how you you know stay up to date but before that I still want to know that journey of um so you we've got university ray gun and then how did that happen and uh what's that like ray gun must be a very different setting. >> Yeah, absolutely. Absolutely. I mean, Ray Gun I spent quite a bit of time there and at I think at peak we had about 30 people roughly and then I joined Canva and Canva had a you know 1,500 PE person engineering team alone. So it's very very different completely different scale different size of business and different stage of business. >> So it's it's very interesting going from you know this a smaller business to a to a much larger one. >> Yeah. >> Yeah. >> What what's the culture like? um the the workload, the environment and and and now at at Nextwork, you know, it's really different. >> Yeah. Yeah. For sure. For sure. Yeah. I guess throughout my career, I've done every level of of the size of company, you know, like very very small company, which we are at now, right, to like a sort of midsize company and then a really really large company. >> Yeah. I think at every at every stage different you learn different things. I guess >> when I when I when I joined Ray Gun, I learned I learned that uh you know at that size of of engineering team, you have maybe like 15 15 people. You get to learn know everybody. You get to understand everyone's strengths, their weaknesses, how they think, what they care about. Um, and but then you go to a to a larger company like Camper and then you don't know everybody and there's no way you're going to know everybody cuz you know every week or two weeks they hire another round of people >> and you know you go from from knowing a code base fairly well you know about solid understanding to going to a codebase that is so large you'll never have a full understanding of every part of it, you know, >> and it's very >> that's very um it's fairly daunting when you when you try and you you join a new company and you pull the repo down and it takes like 10 minutes to pull down from GitHub because then you're like, ""Wow, this is a massive massive codebase."" >> Um but yeah, I don't know if that answers your question, but that's kind of highfold the differences in terms of technical anyway. Yeah, it's really helpful. It's really insightful. Um, what about cultural? I think you also mentioned like you get to know everyone and then when you're in a big company, it's just your team. >> Yeah, sure. Yeah. Culture, I think, is when when you're smaller, culture is much easier to to maintain and build. you know, when you're when you're really large, culture becomes more siloed into the direct people that you you interact with. So, let's say you're in a team of five, >> your culture might be different to the next team, you know. >> Yeah. >> Um, but when you're, for example, at this the size of of what we are, you know, the the culture is the same and it's so tight-knit. I think >> yeah I think the the the challenge really becomes how do you take a culture that we have now and how do you expand that out you know to 20 30 and then I mean canvas size you know 2,000 3,000 I think that's that's really challenging um in terms of like what the exact cultural differences were I think it depends on each every company where every company is different >> I think for I think size size definitely has a really big role to play >> in that. But every like every every company was fun. Um, but I really like the the culture we have here, you know, it's we we work hard, but we have a lot of fun, a lot of laughs doing it. >> I I wake every every morning excited to go to work >> and I know I'm going to have many laughs throughout the day. >> Yeah. Yeah. We we we are a pretty awesome team. I I I do have to say um it is it is wonderful working in this team and um I think even if there are other small companies and startups, I don't know if this kind of culture is easy to find. >> Yeah, for sure. For sure. Right. It's I feel like it's really unique and that's what I love about it. It's unique. >> Yeah. Um Krishna, I was also wondering, you know, with with your experience of working at Reagan and Canva, how do you think that shaped your the way you think about products and and teams and and now and how you bring that into next work? That's a good question. I think I think it it taught me that culture is really important and it's it's something that you have to be very intentional about >> both whether that be engineering culture the the how how we structure code you know from that technical level or or how services uh interact with each other all the way up to you know how people interact with each other. I think um yeah, I think I learn I learned how how a a really strong engineering team functions. Like for example, at uh at Ray Gun, we were quite a small engineering team like 20 people or something like that. But um we were very very tight-knit and because of that we we each of us excelled in our given field >> and we had each other's back and we were able to interact with each other really well and um that's that's something I I learned uh very important that we we do really really well here >> as well. Yeah. >> Cool. Um really fascinating to to hear about how those experiences stacked up and how that shaped you into you know what you bring into next work and I mean I I have to ask with with your experience and background skills I mean you're an absolute rockstar at what you do you could have joined any number of product companies but here Here you are at Network. What drew you in? >> Yeah, I mean when I when Amber first approached me about joining Next Work, uh it was it was very compelling. You know, the the sales pitch was really easy. you know she said hey we want to build a place that you know that learn learning is right now is fundamentally broken and we want to build a platform where anyone can come and learn anything >> and I was like okay that's that's interesting very interesting cool I like that and then so that was that was the actual like the core mission side of it I I really liked that >> I liked being able to that the the vision was really big. You know, often a lot of companies will have a vision of, oh, we want to achieve this, you know, but Amber came in with, no, we want to change the game. We want to change the world. This is how Nextwork is going to be how people learn, you know, and I found that I found that really, really cool. >> Um, the second one was being able to build a product from the ground up. At the time we didn't have a product as as it is now, right? We didn't have a product. We had zero users. Um so that as well, you know, having built built things in the past, I was like, ""Wow, this is really really cool. Really cool."" So those are the two main drivers as to why it was a it was a really easy yes for me to to leave Canva and and come >> join Nextwork and and build the platform. Kra, man, like that sounds crazy. Like there's not a proper app and you said yes, I'm going to come and and join this team. Isn't that like crazy? >> Yeah. Yeah. I mean, I guess in reflection, it does it does sound pretty wild. Um, yeah. when when uh Amber Amber showed me she she showed me designs she had written on paper and she had a big like big design book and she was showing me that oh this is what the UI is going to look like and this is where like all the steps are going to go and this is what we call the highway and I was like okay this this is really cool you know like it was really clear that she put a lot of thought into what the product was going to look like and she knew exactly what it was going to be in her you know, there was no question of what are we going to build? It was it was very clear. >> And with that type of clarity, especially from from an engineer's perspective, right? Like when you when you when you're tasked to build something or when someone comes to you and says, ""Hey, I want this feature. I want to build this."" >> The question you ask questions back to try and oh, what about this? Have you thought about this? What happens if the user does this? What happens when something like this other thing happens? And every question I asked Amber, she had an answer for. >> Yeah. >> And that was really that was really really good. So it was it was really it was it was easy. I was like, ""Okay, great. You know you know exactly what needs to be built. I can join and I'm going to I'm going to build it."" >> That is >> and I think it's worked pretty well. And I got to say Krisha like the work you do is just absolutely phenomenal. The way you operate it's I like Yeah. Is there is there a favorite memory you have at Nexwork? >> Damn. I wouldn't say that there's one favorite memory. >> Mhm. I think there's there's been like a lot of favorite moments throughout throughout my time. I think the one that comes to mind now is John the photo of John holding the firework like a couple months ago. I think that was really cool. >> Um just to the times that we've spent together as a team. >> Yeah. >> I think those are the times that I which car park sesh Matt says I don't know. You'll have to ask Maximus that one. Um, the times that we spent together as a team, uh, I think the parts that I've I've remember most fondly, >> you know, like going to the sauna or >> going grabbing food or going for walks, >> the the retreat, like all all these different things are are things that I I really like. And that's that's why I kind of spoke about before like culture is so >> so important, you know, because if you don't like coming to work every day, your work is going to reflect that, >> right? >> You know, so I think we've done a really good job of making sure everyone has a great time while while we we do work. And I think those are the the things that I remember really really well. >> Yeah, absolutely. I love that. Um, I love that the favorite memories that you have at Nexwork are the ones with the team. >> Yeah, 100%. >> That's really special. Yeah. >> Yeah. 100%. I'm very much >> a team Very much a team player. Very much a team player. >> Yeah. >> Yeah. What about a favorite feature or a favorite moment like when you're building something at work? Sometimes it's like, you know, finding a a bug or resolving the code or um like an aha moment that we're like that made you feel like this is why I'm at Next Work. This is just we're just building great things. >> Yeah. I think I think when we first released the app and there it was it was basically just a blog, right? The app didn't really it just had the the projects there. >> But to see to see and all you could do was complete. You could mission complete. That was it. >> And it was that was a really memorable moment. A part I was like, ""Oh, this is this is awesome."" Just to see the amount of people that came through, click click the button and said, ""Wow, you know, this project was awesome."" Um, yeah, just just to have the amount of people use what you built and then got value from that. I think that that was awesome. That was awesome. And that's that's something that you constantly chase over and over and over again. >> Yeah. And you've seen so much. you've seen like going from a blog to what it is now from to the automated generated um documentation the PDF and then from the PDF to the live documentation and then being able to edit. I mean we've like there's so many things that have been added on. What would you say is your favorite feature? >> That's a great question. I feel like every time I build a new feature, it it becomes my favorite. I think there's been a lot of really cool ones like um the notes feature was a good feature. Uh live docs I think is a really really good feature. But my favorite feature right now uh that we that we've released right now is uh the ask feature. >> Definitely. I think ask is >> ask is a feature that provides so much value >> and even yeah I mean even people in the chat are putting ask as well like it's a >> it's an incredible feature. It took it took a little bit to build but we got there and and just the amount of value that that one feature alone provides is phenomenal. So I think that's probably my favorite feature. >> Yeah, that's true. It's as as someone who is leading community and seeing all the errors that come up, I I really appreciate the ask feature, too. >> Oh, 100%. Yeah. Yeah. I remember when we didn't have the ask and you do a project and you get stuck and you're like, ""Oh no, how do I figure this out?"" You go on Google, you try to figure it out. But now, I mean, it's it's right there. You know, you can you can even highlight a section of the the project, click the ask button, and then you can ask directly about it, >> which is awesome. >> Yeah, great conversations in the in the chat. Put your favorite feature. What's your favorite feature? Uh there was a really good question from Sloth, I think. Um and she asked, ""I want to know how you guys come up with new features."" That's a great question. That's a great question. Um, we how we come up with features sort of I guess is comes from two places. Uh, number one, what we hear from the community, you know, a community a lot a lot of people will say, ""Oh, hey, I I it would be awesome if we could do this."" And then internally as a team, we see that we go, ""Oh, man, that's actually a really good idea. Let's build that."" So that's one bucket. The other bucket is where you know Amber has a vision of where we want the product to be. >> Yeah. >> And so we like you know not not just one year from now but you know 5 10 years from now. >> So we we try and break that down like okay what is what does the product look like in five years? How can we build some of that now >> and bring that value to to the to the learners now? So really it it really is those two sort of buckets. I guess there's a third one as well which is oh we've seen some technology develop in a really cool new way. >> Um and how can we use that technology to help improve the learning experience you know so those are really the three sort of buckets that we we generate these ideas from. >> That's such a good answer. Very well thought out and very easy to visualize too. the three-part the three component answer makes sense. I hope that answers the uh question you had sloth. Um you mentioned that you know Amber has a vision of what next would look like and what that you know how that feeds into the features that we that are released. How do you envision next growing? >> Oh it's a great question. I see I see next work o overtaking a lot of universities a lot of other platforms even YouTube as being the place where anyone comes to learn any skill you know I mean the way the way we're able to teach the way we're able to capture people's attention right now >> in a in a stepbystep guide sort of way you know and the videos we have the content we produce and the learning tools that we've developed. I think that right now that's worldleading and it's only a matter of time before we overtake other platforms. So that's that's where I see next work, you know, evolving in terms of volume. >> That's so cool. It um it definitely is very exciting to see folks just loving next work the projects the content and I mean so many of the folks in the audience they've already like expressed their love for next work so sending love there um so we talked about the future of next work what about future of engineering Krishna It's a big question. It's a big question. I think engineering right now is going through a fundamental shift. You know, when I when I started doing software engineering and coding, it was very much a here's a problem. How do we let's break that down. Let's write the code and then let's release that. Right? If you kind of imagine it like one it was one one thing one pipeline >> then AI came along and we thought okay how can we take that what parts of this pipeline can we automate or give to the AI to speed up you know oh maybe the AI can write this portion of the code and then we can review that code >> and then you know all these little parts or even even making videos what how can we use AI to make a video and all this kind kind of things. So that was sort of level two, you know, and that we we hit that, you know, two years ago, two and a half, three years ago maybe. >> Now we're at a stage where it's how many of these pipelines can we do at once fully automated, you know? So before I used to work on one feature at a time, >> then I started using AI to speed up the certain parts of my workflow. And now it's about how many different things can I work on at once >> and what what of those things can I fully offload to AI >> and then all I have to do is review that and then merge that and release that out. >> Yeah. >> Right. So it's gone from the engineering landscape has really gone from writing code yourself. >> Yeah. >> To getting AI to write code for you. But you know a lot of people say oh software engineering is over you know the day of the coder is gone like AI is everything now and to some extent that's true right like we have a view internally where you know we we don't want to be writing every line of code ourselves you know why would we write it when we can have an AI write it for us >> right >> but as someone said in the chat you still have to know what it's doing. And this is this is the core the core part. It's up to the engineer to understand the architecture, the patterns. >> Mhm. >> How you in your mind would build the feature yourself. >> Mhm. >> And then get the AI to adhere to that, right? M >> so that it frees you up as an engineer >> to go from oh how do I build this one thing to oh let's make sure that these five different things are being built in the right way at the same time by these AIs. >> So you go from a one to one >> like one engineer to one problem to one to five >> you know five problems at once. How can we do that in the in an efficient way while keeping code quality really really high >> while making sure our down our downtime isn't affected at all >> so we have constant like really really high up times. >> Yeah. >> And also that the code base is evolving in a manageable and maintainable way. >> Yeah. >> So yeah like the short of that is it's changing a lot. >> Yeah. No, I I I think you know you you touch on so many interesting points and it's such a like needy answer that you gave the future of engineering and the role of AI >> how what what are jobs going to look like? I think someone in the chat mentioned it's engineering is going to look like managing AI more than um coding itself. >> But then you do talk about the need to understand and have that technical foundation being able to understand architecturally like what makes sense. How do you want to organize um your structures, your files, how you want to um set things up, set up your environment and how you can build on it? >> And uh in today's day, you know, I I think it was um Sloth who asked what do you look for in a developer? What do you look for when you're hiring? How do you um yeah what do you look for? Let's start there. >> I think what we look for in a in a really strong engineer is the ability to to one like arch have a strong understanding of architectural patterns and scaling. That's one. Um two to be able to write code. I think even in the world of AI, a really strong engineer still knows how to write code. They need to be able to understand what the AI is is, you know, is building and outputting. >> Um, I like to think that if you can give an engineer a problem and they can solve it in their mind and they already know what the code is going to look like, >> so all they need to do is get the AI to execute on that, >> that's a that's a good sign, >> you know. That's a good sign. But I think a good a really good engineer is more than just tech the tech more than just the technical aspects. I think a really good engineer is someone who's able to talk to people, right? Like in and in my position anyway, right? I I talk to marketing. I'm across design, >> product, >> you know, and all of these, even community to some aspect, content, but there's all these different parts of the business and you need to be able to interact with every single one of them and explain concepts in a in a non-technical way, you know, I think that's super important. and being able to manage expectations between all the different stakeholders. Like we could use ask is a great example of that. You know, um there's product stakeholders there. There's design like we need to make sure that one the ask feature solved the problem. We need to make make sure that it's designed in a way that's easy to use and understand, right? And then we also need to be able to to ask yourself, oh, what what are some improvements here that we could make to improve the experience for the learner? What are people saying that they want? >> Yeah. >> And to be able to think about all these things and communicate with every business unit >> uh and then take all that information and then create the feature. I think that is what makes a really strong engineer, a product engineer. Um, so that's really what we look for. >> That is so on point, Krishna. Like it's such an underrated skill to be able to communicate. Um, and it's part of like showcasing your skills too. You know, you you build all these things. You're technically very proficient, but you need to be able to communicate whether it's in your portfolio, whether it's in written format, whether it's in um verbal. How do you build that skill? >> It's a good question. I think by by you know one on one hand you have learning the skill and then you have executing the skill but then you also have showcasing the skill you know and to get good at talking about your project or feeling confident enough to show it to other people you have to take that first step you you know, oh, >> hey, let's let's share this what I built to Facebook as an example, you know, and then then you can be, okay, great. Now I've shared it there. Now I can talk about it. Um, it's it's always like the what's that term? It's like the the journey of of a of a million miles begins with a single step. Something like that. You know, you have to figure out what the first step is and then just take that and then do the next one and the next one and the next one. >> Yeah. So, you got start somewhere and and then with time and experience, >> it's a muscle that you just learn to exercise and build. >> Yeah, for sure. Definitely. Definitely. Yeah, it's definitely a a skill that you can learn, you know. I don't think it's a skill that you're born with. I think everyone can teach themselves, you know, how to communicate, how to present themselves in a certain way, how to be able to talk to people in a certain way, how to take feedback even. You know, a lot of my job is, you know, you build something and then, hey, this part this part sucks. We need to we need to shift focus, you know, or maybe this didn't solve the initial problem that we had, you know, now we have to shift again, you know, taking that kind of feedback initially is really hard. >> Yeah. But over time, you know, over time you get you get better at handling that feedback and how to give that feedback to other people and all this kind of things. So it's it's it's definitely a a skill you you you can learn over time. >> Yeah, for sure. I I think for product engineers, you you hear feedback on your designs like your you know the the front end like whether it's working or not. Um, and then there are bugs and or or errors that come up and and it's I don't know. Do you take it personally? How do you make sure that you stay separate? Like that's a that's my work >> and I'm here to make it better. >> Yeah, I think that's that's the key to it. That's the hard part. I think when I first joined the industry, I remember my first PR that I that I worked on and I I took me like a whole day. I made the PR and it was for a simple bug fix for some datetime uh bug that we had and I I I was so happy. I was like, ""Yeah, this is great. This is going in."" I thought I knew exactly what I was doing. And then that PR got ripped to shreds. You know, the review was like, ""Oh, this is wrong. This is not good. Do it this way."" It was very it was very matterof fact like, ""Oh, this is this is wrong. Don't do it like this. Do it like this."" You know, there was many comments on that first PR and I I took it I took it personally. I was like, ""Oh man, I thought this was great."" You know, but but apparently not. But it's it was a very humbling experience, but at the same time, it's through that experience that I learned so much in such a short amount of time. >> Just being in in an environment like that >> forces you to detach yourself from from your work, you know, because you you are you are more than your work. Some when something goes wrong in a feature that doesn't mean that ah I'm I'm the problem. You know, you can't account for everything. Some things just happen. Like how was I supposed to know that this one function in one particular browser version didn't work on this one device? >> You know, you wouldn't know that. But just over you just make these mistakes and then you learn to detach from that like oh okay I I made the mistake yeah I didn't know no one would have known that you know how how would you know this but it's it's the willingness to go oh okay this this was because of my my the problem I introduced I need to fix it so then you just go in and you fix it and and you move on and everyone moves on >> such a great answer Krishna and uh I I really appreciate you bringing up your first PR experience. And the reason I I also, you know, really appreciate that is um because right before this session, Sloth in our audience had mentioned that she had her first PR and she's going to get it reviewed and she was so nervous. I honestly I don't know how it went um today, but uh I think it's so apt and so timely that she's going to come out of that meeting and she's going to hear you saying don't take it personally, whatever it is, you know. I think that's um really awesome. Oh, she says um she uh she did really really good engineering. >> Nice. Nice work. Awesome. Well done. First PR good engineering. Nice. That's that was better than mine. >> Um Krishna, there's another question in the chat. >> Yeah. >> And this is more on the AI and engineering. Uh Sean asks, ""With your vast experience in working with different sized development groups, what do you think or how do you think AI is going to change the work environment from your perspective? From your view, how would you how would one best prepare for that change? And what things would you concentrate on that would have the most bang for the buck? Great question, Sean. >> Oo, that's a really good question. There's a lot of parts to unpack in this. Yeah. >> What I what I will say is to the best way to to prepare for the innovations in AI is to be as adaptive as possible to be to be flexible. When when there are when things change at such a fast rate, you need to be able to adapt to those changes. Especially with AI, one day a new tool could come out and it could fundamentally change how we write code, right? I mean, we've seen that already with Claude code and, you know, cursor and other tools. And what really worked well for us was we see we saw Claude code coming and we're like, ""Oh, okay. Let's jump on that now."" It wasn't a, ""Oh, we can't. No, no, no. We don't know what's going to happen there. Let's wait and see."" No, we we had the hunch that this is the future and this is how code is going to be fundamentally written and then we jumped on that. >> And I think that h having that ability to adapt and think for yourself like oh is this is this going to be the that next step up and then make that make that shift. I think that that is what will will set you up for success. >> Nice. Great answer. The adaptability. >> Yeah. >> Learn to learn. >> Yeah. Learn to learn and learn to love learning, >> you know, cuz you learn to love >> learn to love learning because you're going to be doing it the rest of your life, >> you know. I I learn new things every single day. >> And that's I don't think that's ever going to change. >> Amazing. um when you're um hiring or mentoring engineers, is there any one thing that makes someone stand out to you? Yeah, I think having having built things in the past is something that that really stands out to me because it shows even if if it's something you've built in your your spare time is a massive plus in my opinion because it shows it shows initiative. It shows you were able to build something out of your own idea that you thought about yourself to solve some problem that you had or a problem that you'd identified and for someone else. >> Mhm. >> I think that's super important. Um what I said before around explaining technical concepts to non-technical people and just being able to communicate generally across across the business. I think that's a that's another trait that I really look look at. And another one is understanding product. A lot of engineers think that being a really good engineer technically makes you a really really good product engineer. But in my experience, those those two things aren't exactly one for one. What makes a good product engineer is thinking about things from the users's perspective. How is how is the user going to interact with this? What problem are we solving for them? >> Yeah. >> Uh a common thing I well or something that I've heard before that I really like is don't make as an engineer don't make our problem the user's problem. Just because something's easier for for us to implement as a engineer in in a certain way doesn't mean that that's the right thing for the user. Maybe it's a worse experience for them. Maybe it doesn't solve a problem that they had. So you always have to think back from the user's perspective. >> Yeah. >> Like Yeah. Like at the end of the day, the user is everything. you know the the learner is everything for us and if we can't solve the learner's problems or build things that the learner finds valuable you know we're we're not on the right track >> basically you know >> so the fact that and I mean you know we are on the right track because we can see you know there's there's a lot of people here there's a lot of people in the community a lot of people find a lot of value from the stuff that we build um so yeah it's a bit of a long-winded answer but I That's what Yeah. I hope that answers your question. >> Yeah. Yeah. Um yeah, there was quite a lot of in there, but um all very useful to know um if someone is looking to get into a engineering role. Um, there's also a question from Roy and I think it's a perfect question as I transition into the rapid round. Um, because I'm mindful of time too. Great questions and great conversations happening in the chat. Love it. Roy asks, okay, so and and that'll be my first of like many questions. Roy asks, ""Um, if AI was turned off tomorrow, how would you adjust as an engineer? >> I just I just saw what Pano replied to that one with."" That's funny. Um, if AI was turned off tomorrow, how would I adjust? Um, I'd just go back to doing engineering how we used to do engineering. you know >> um AI is such a is such a productivity booster for us >> um but it doesn't replace engineering >> y >> you know the core way we build product and the core way we build features has not changed and will not change >> y >> it's just the way at which that happens you know if AI was turned off tomorrow that's fine no problem we'd still be building features that would solve value for our users um in the best ways possible. >> Yeah. All right. Quick answers. First thing that comes to your mind, >> favorite programming language. >> Oo, Typescript. It's not a language, but it counts. Typescript. >> Okay. Okay. Uh, first thing you do when you open your laptop. >> Cahoo. What are you talking about? Stop. What? Um, sorry. What was What was your question? >> First thing you do when you open your >> First thing I do when I open my laptop is I I check my calendar. >> Huh. You plan for the day. >> Yep. Yep. every morning plan for my day. Takes like 10 15 minutes, but sets me up right for the rest of the day. >> Nice. Um, coffee or tea while coding? >> Oo, I don't drink coffee. Um, but I have like masala tea every morning. So, masala tea. >> And I also found out today sometimes even >> Yeah, I do. I do like it. Night owl or early bird? >> Early bird. Early bird. My My bedtime is my bedtime. Like, you know, I think I'm I'm in bed by like 9:30. 9:30, 10:00. I'm in bed, you know, like I need my sleep. >> Okay. Uh, a tech tool you cannot live without. A tech tool I cannot live without. Claude code. It's the easiest one. >> Um, a non- tech activity that makes you better at what you do. >> That's a good one. Um, I'd say going to the gym. I think have have going to the gym like phys physical increasing my physical stamina has increased like my mental stamina. >> I think those those two things are fairly linked. >> Yeah. >> Yeah. >> Nice. The most fun bug you've ever debugged? >> Fun bug? I don't think I don't think those exist. Um uh I I back at back at Ray Gun I took I took down we had an SDK and that took down like a 100,000 apps when I made a release and that was really bad. So fixing that was probably really good. So that's that's my Yeah, Pano was there can confirm. Yeah, that uh that's probably the the funnest bug. >> All right. If you weren't an engineer, what would you be doing? >> Oo, that's a great question. If I wasn't an engineer, what would I be doing? I'd probably I'd probably start like a toasted sandwich shop with some with like a milkshake business on the side. I've always wanted to do that. I feel like one day when I when I retire when I'm like 60 or 70 years old that's what I'll do. >> Where's the chai Krishna? >> Oh yeah, of course. Of course. Yeah. Yeah. Yeah. Of course. >> That'll be there too. >> Sandwiches, smoothies, and >> that's the one. >> Um this is a question I cannot tell you who asked um this question. What do you have to say about the award you won last year? What was the award you won? And can you speak on it? >> H my what was it? My memory. Oh, my memory is so bad. My memor is so bad. I can't remember what it was. I can't remember what it was. But I remember I remember it was I I remember Yeah. like it was meant to go to someone else, but I just I just had to I had to it was given to me. Um, but I can't exactly remember what that award was. >> Something about HR violations. >> Nah, I don't know about that one. I don't know about that. That's got to be AI. That's got to be AI generated. Yeah. Yeah. Most improved. Most improved. I'll take that. I'll take that photo. That's No, that's definitely AI. That's That's definitely AI. >> Most AI violations. >> Most AI violations. >> Oh, that's crazy. >> Um, what's one word that sums up Next Work for you? Oo, Groundbreaking. >> Groundbreaking. What a great word. I love it. >> I think it's groundbreaking. Definitely. >> I love it. Great word choice. All right, there's a lot of questions coming up, but I I know we're at time. Um, really enjoyed this session with you, Krishna. I just want to ask one last question before we wrap things up. What's one lesson or principle that's carried you through your career so far? >> It's a good question. I think yeah I think it's what I said before like don't don't make the our problem don't make your problem like your problem as an engineer the user's problem. >> I think that that has popped up again and again throughout my career and it's it's I think it's something that really sets you apart. you know, being able being willing and able to go the extra mile for the the customer or the the the user is is is my will set you apart 100%. >> That's amazing. Great answer. Thanks Krishna. Thank you so much for sharing your story and your insights with the community. I think it's so wonderful to hear about your journey into engineering um and the lessons you've learned as you've made it to next work and just crushing it here. Um loving all the comments and feedback. I think you should go through it. You've got a lot of love coming up. >> Oh, thanks everyone. Thank you very much. >> Yeah. To everyone who's joined, thank you so much for joining this episode of Humans of Nextwork with Krishna. Stay curious, keep creating and we'll see you soon. >> Nice. >> See you all. >> See you everyone. Take care. Thanks for listening. >> Don't forget to if you're on YouTube, don't forget to subscribe. Smash that like button. And yeah, thank you. Peace.","**Meet Krishna Kapadia: A Founding Engineer at NextWork**

In this engaging interview, Krishna Kapadia, a founding engineer at NextWork, shares his inspiring journey into the world of engineering. From a young age, Krishna was fascinated with building things, which eventually led him to develop a passion for coding. He recounts how he used to take apart toys to understand how they worked, showcasing his innate curiosity and desire to learn.

**Key Takeaways:**

1. **Curiosity and Initiative**: Krishna's story highlights the importance of curiosity and taking initiative in one's career. He built a website for his parents' factory, which demonstrated his ability to identify problems and create solutions.
2. **Adaptability and Continuous Learning**: Krishna emphasizes the need to be adaptable and continuously learn in the ever-evolving field of engineering. He encourages engineers to stay up-to-date with the latest technologies and innovations.
3. **Effective Communication**: Krishna stresses the importance of effective communication in engineering, not just technical skills. He believes that being able to explain complex concepts to non-technical people is a crucial trait for a successful engineer.
4. **User-Centric Approach**: Krishna advocates for a user-centric approach in engineering, focusing on solving problems for the end-user rather than just creating solutions that are easy to implement.

**Krishna's Journey:**

Krishna's career path has taken him from working at small companies like Ray Gun to larger organizations like Canva. He shares his experiences and the lessons he learned along the way, including the importance of company culture and teamwork.

**NextWork and the Future of Engineering:**

Krishna discusses his role at NextWork and the company's mission to revolutionize learning. He shares his vision for the future of engineering, where AI will play a significant role in augmenting human capabilities. He emphasizes the need for engineers to be adaptable and willing to learn new skills to stay relevant in this rapidly changing landscape.

**Rapid Round:**

In the rapid round, Krishna answers questions on various topics, including:

* His favorite programming language (TypeScript)
* How he adjusts to new technologies and innovations
* The importance of physical activity in improving mental stamina
* The most fun bug he's ever debugged (taking down 100,000 apps with a single release)

**Conclusion:**

Krishna's story is a testament to the power of curiosity, initiative, and continuous learning. His experiences and insights offer valuable lessons for engineers and non-engineers alike, highlighting the importance of effective communication, user-centric approaches, and adaptability in the ever-evolving world of technology.",2026-01-16T01:49:34.623751
NextWork,Secure Payments with Stripe | Interactive Build Lab,vK31JB6gh-s,"All right. Hello. Hello. Hi, sloth. Sean. Good to see you guys here. Today I'm doing the build lab and I am going to be doing our newest project secure payments with stripe. If you have been wanting a phinops project at next work, I highly recommend that you check the series out. Yesterday we released a project ship a landing page with Vzero and Versell. Really, really fun project. I completed the project. So, let me share the link. In fact, let me share the link for the entire series. and it is the Phops AI series. So, putting the link in the chat here. And the newest project is the second one, secure payments with Stripe. Very excited to do this project with all of you. Where is everyone joining from? Anyone new to Nextwork? All right. So, for all of you who might already know this, just bear with me as I tell everyone some of the details about Nextwork. Nextwork is a project-based learning platform where you can learn anything from AWS cloud to AI. And this month we are doing a 21 and 21. That is we are releasing 21 projects in 21 days. One project a day. And today is day seven. We've made it for a week. Seven new projects have been out. And I highly recommend that you check it out. If you have any questions, feel free to just put it in the chat. and let me know if I can help you find a project that you're looking for. Whether it is AI, DevOps, Finops, security, we've had all these projects in just the last 7 days. And stay tuned for the remaining 14 days where you're going to find out um all the other projects that are coming up. I do have to say we have a little bit of a sneak peek that you can check out right here. I've put it in the chat, a little bit of um promo. Sometimes we have build labs led by different folks in the community, by different folks in the team, the next team. And this week we had Pano, our engineer, who built uh did a build lab yesterday and then I will be doing the next two build labs and then we've got Azam and Nikio doing a build lab. So very excited. Check it out. You will also get a bit of a preview of the upcoming projects. Excited. Anyone doing the project along with me? Sean, I know you already started this project. Are you done? Are we doing this together? Sloth is at work and we'll be able to do the project. I see U and I see Yana. Good to have you guys here. Let me know if you are following along in this project with me. All right, let's get started, shall we? Okay, so what is this project all about? Luckily, I did part one yesterday with Pano as he led the build session. We have this project is secure payments with Stripe. So, yesterday we created a landing page. We have our ecommerce product. Um, and in fact, let me see if I can pull it up here. Ah, I used a different browser. Hold on. Let me fix that quickly. And if you've done the project, share your landing page here with us. It's out on Versel, so let me know. It's really cool to see all the different um projects you all have, you know, showcased in your Versel all the the marketing products that you've included in your landing web page. I am pulling up Versell right now. I will quickly show you my landing page, but very curious to see all of yours. Okay, let me share this. All right, this is the this is the page that I built. couple of hours ago if that. And because I'm using a different browser, I want to log into Versel here. Bear with me. How's everyone doing? Let me take that offline. Who's doing the project along with me? Oh, I have a code that I need to use to sign in. Yeah. So, this is what we created yesterday and that was V 0 vers, right? And today we're doing secure payments with Stripe. And yeah, we basically using AI, using the help of cursor AI, we generated this website with our product and we tweaked the UI design, changed the button angle and then deployed it. And it's also a part of our git repo. So really good project to help you get started with publishing a web page. Maybe I mean here in this in this project it's a e-commerce landing page but maybe it is your personal website that you want to showcase. So, um, it's a great project to get started and see how to do these things and use AI to build things and deploy things. All right, good to see you here, Harry Brewer. We've got Zdev Roy. Nice. All right, I'm just getting started. So I gave a little bit of a background context on the part one and today we are continuing on and we will be building um in this project we'll be integrating stripe checkout into a nextjs application with proper security practices. You'll use cursor 2.0 O to build server side checkout sessions, configure versel environment variables for key management and implement web hook signature verification. Nice. Good stuff. So nice little architecture diagram here. Is Stripe free to use? Yes, for development. Stripe's test mode lets you build and test complete payment flows without processing real money. You use test API skills, API keys, and test card numbers throughout the project. No credit card required to get started. Great. So the prerequisite is that the project builds on ship a landing page with v 0ero and cell. You will reuse your deployed nextjs application and for sale project from part one. Amazing. How will you build it? First you will set up stripe with proper key management. learning why publishable and secret keys exist and where each belong. Then you'll create a server side checkout flow that ignores front end submitted prices. Finally, you'll implement web hook verification so you only f fulfill orders that Stripe cryptographically confirms. By the end of this project, you'll have a secure checkout flow that redirects to Stripe's hosted payment page. Serverside price calculation that's immune to front-end manipulation. Web hook signature verification that prevents that prevents fake payment confirmations. And we've got a secret mission, a discount code system validated entirely server side. Nice. Exciting. All right, before we begin, let's get a quick quiz to get familiar with the concepts. Ready? I need your help to do this. Okay, let's do this. All right, question one. Where should the Stripe secret key be stored for a Versell deployed application and for local development? A directly embedded in the front end JavaScript code for both environments. B in a publicly accessible GitHub repository for easy access. C as an environment variable in versel and in a envelopal file for local development or D within the stripe checkout URL parameters. Okay, we've got local local and C. Yep. All right. Three for the same answer, shall we? Nice work, guys. Yep. We want to add the secret key to our local file, the environment variable. Amazing. Nice work. Which tool is used in this project to generate the initial e-commerce web app components from a plain English description. H which tool is used to in this project to generate the initial e-commerce web app components from a plain English description? Any answers? Oh, yeah. Don't push and to GitHub. Good. Good. All right. Is it Stripe? I don't think so. Vzero.dev to generate the e-commerce web? I don't think so. Versel isn't useful to generate the web app. So, let's go with cursor, shall we? Oh, which tool is used in this project to generate the e-commerce e e-commerce web app components from a plain English description. Okay. Um, cursor is an ID used for coding and interacting with AI. Not for generating. Oh, that's right. We did it on Versel. Ah, my bad. My bad, guys. See, this is why I say I need your help. All right. Oh, no. I didn't see the right answer. Yeah, we should be able to go back on the quiz. H. Okay, question three. What is the main security difference between a Stripe publishable key and a secret key? A. The publishable key is for live transactions while the secret key is for test transactions. B. The publishable key starts with SK test and the secret key starts with PK test. See the publishable key is used for issuing refunds and the secret key is for creating payment tokens. Ded the publishable key can be safely exposed in frontend code but the secret key must never be exposed in front end. Sean says D All right. Anybody else? Is D it? Okay, let's do D. Yep. We never want to expose the secret key in the front end code, but you can do that with the publishable key. Nice. What is the primary purpose of Stripe web hook signature verification? A to confirm that the web hook event originated from Stripe has not been tampered with. B to provide real time updates on payment status to the user interface. C to encrypt the payload of the web hook for secret transmission or D to automatically retry failed web hook deliveries. What's the answer? All right, Harry Brewer has a big description on Conjure open source. Nice. Developed by Cyber Arc makes sense as to why you're sharing it. Sean says, ""A the primary purpose of Stripe web hook signature is to confirm that the web hook event originated from Stripe has not been tampered with."" Okay, well done. All right, question number five. What is the primary purpose of integrating Stripe in this project? A to generate React components for the UI. B to securely accept payments and manage transactions. C to deploy the application to a hosting platform. Or D to manage the project's GitHub repository. All right, I see AB. Anyone else? B. 70% Sean. It's correct. All right, last question. Why is it crucial to define product prices server side when creating Stripe checkout sessions? A to allow for dynamic pricing based on user location. B to reduce the load on the client side application. C to prevent malicious users from manipulating prices sent from the front end or D to ensure that all payment methods are supported by Stripe. H see 100% from Sean. Okay. Why is it crucial to define product prices server side when creating Stripe checkout sessions? Okay. All right. Let's go with C. Nice. Well done. Okay. So, the the explanation gives a little bit of an answer. A malicious user could modify front-end JavaScript to send price one for a $599 item if your back end accepts the front end price, they pay 1 cent instead of $5.99. So by defining the price on the server, we ignore any price the front end might send. Nice. almost made it, but it was a question I knew the answer to. That's okay. All right, let's go with step by step. If you're new to next work, all the projects have three tracks and depending on your style, you can go with step by step some or on your own guidance. Every project has a embedded video tutorial which you can check out on the side and watch as you go along. This was a trick I learned from King just yesterday. All right, so connect Stripe to your app. Time to connect your app to Stripe. We will set up a Stripe account. Get your API keys and configure them so your app can accept payment securely. In this step, get ready to create a Stripe account and get your API keys. Configure keys in Verscell and locally. Okay, exciting. In this step, I am going to create a Stripe account and get the API keys. Get the API keys and configure the keys in Brazil. This is important because I I want my app to accept payments securely. All right. Nice. So, as you know, with every task that's complete, you get your documentation that you can check out anytime. And you can also edit the text here depending on how you want to present your documentation. Exciting. Good stuff. Okay. Have you deployed Versel project from part one? Oh, if I haven't, I can do it here. So, if you're joining for the first time and you haven't done the project one of the Finop series, you can still catch up right here. Luckily, I have my Versel project and I'm getting ready to add the Stripe payments. Do you already have a Stripe account? I do, but I don't know if I should use that. H, we'll have to find out. Let's set up um Stripes test environment or let's check it out. Let me log in to Stripe and see how have you guys used Stripe before? Did you have to create an account? Did you have an existing account? Where's everyone at? Roy, I want to give a shout out to Maya for being a rock star with these backto back events. Beast mode. Thanks, Roy. I feel like you should have a special banner on your PDF docs at the end if you answer the pre and post quiz at 100%. I like that idea. I am going to make a note of that. And you know what? We should add that to the feature idea. Hold on. I'm going to digress a little bit. Open our feature ideas. Add it right here. Oh, look at that. Harry Brewer, we are just on it. Add quiz results in the documentation. And if you like this idea, um, upload it. posting on behalf of Harry Brewer. All right, check this out. So, if you like this idea, go and like it. Nice. Okay. So, let me quickly log into Stripe. Okay. Stripe. I I like using a different browser. Um, I'm going to sign in. I sign in with my next work. Hold on. Hold on. Signing in. Make sure there's nothing private that I'm about to share. On that note, Hair Brewer is using Stripe for the first time. Okay, thanks Sean. Thanks for the heads up. Where is my stripe? It's going to believe it's going to make a separate sandbox for you, I believe. Okay, logging in. Signing in now using my Touch ID to sign in and okay some struggles with signing in. No worries. I am going to try a different route. Let me know where you are at in your project. Are you currently creating a Stripe account? Okay, almost there guys. Thanks for waiting. I need my authenticator app to log in. Let's log in really quickly. Okay. So now I'm logged in and uh is there a way to create a different account? If you're not in test mode, click developers. Okay, stay in test mode. Okay, I think this is I don't know. I wonder if I need to create a new account. How do I create a sandbox? Do I have to go ahead to find out? All right, Roy, where are you at? Five browser is a must. Yes, I think I'm at four. I'm not at five. What browsers do you guys use? I know Brave is a popular one. Once you're in the sandbox, you'll be safe. I have a disclaimer on my device. I had an existing account as well. I just followed the project and it created a sand box in the developer section. I don't see that just yet. So, I'm a little nervous. Where is the sand? Oh, switch to sandbox. Oh. Oh, okay. Now I see it. And Interesting. So, okay, I might have to create a new sandbox because I think I can see the whole team's sandbox. Okay, let me create a new sandbox. Maybe I'm going to create new sandbox. Maybe I'm getting ahead. Let's check it out. So, it says I have to locate the API keys. H. Do I locate the API keys here or in the sandbox? And I would have liked more instructions on the test mode. Okay, good to note. I think I am going to create a sandbox. You know what? Let me share this now. So that so this is I think an existing sandbox, but we'll see. We'll see. So, I'm going to um go back to the guide. All right. So, if you're not in test mode, go to developers overview and test mode. So, this wasn't so clear for me. That's okay. I think it might not be it might not pertain to all of you. Thanks for the tip, Sean. Roy says, ""I haven't started yet. I got a few things to close out. I'm sure you'll catch up pretty quickly."" Um, let's see. Now, we want to locate the API keys. This is the sandbox. In the Stripe dashboard, you'll find API keys in the menu. And you'll see two keys. Okay. All right. I'm going to create a new sandbox if I can figure out how to do that. Create. No, let me hide in this. Okay, let me create one. One second. And I see we've got UD who's still here. Tell me where you're joining from. Are you doing this project along with me? Okay. AI phobia or mm mm phobs. I like it. All right. That's what I'm going to go with. And I want to create an account. So nothing will be copied. Okay, let's try that. All right, sharing my screen now. And all I did was create a new sandbox here so that everything matched the screenshots. The other one had some existing data and it might have been from another instance. So yeah, thanks Sean. To create a sandbox, go to the top left corner where it says new business sandbox dropdown. You'll see create a new sandbox. I think I figured it out. And here we are. All right. Locate your API keys now in the Stripe dashboard. Find API keys. Where are you API keys? Oops. I wasn't supposed to show you that, man. All right. So, let's um let's see what are these two keys. Yes, I did find the the API's keys. Um there was a publishable key and a secret key. Um the publishable key is safe for front-end code and can only create payment tokens. the secret key um can do a lot more and you don't want to share that, right? Upload a screenshot of your Stripe API keys. I don't know if that's a good idea, but let's do this. It's not the entire key, just a little bit. All right. What is the difference between the publishable key and the secret key? The publishable key is used for front end code. It can create using the using using the publishable key. We can create payment tokens. The secret key is used for the back end. Using the secret key, we can charge cards, issue refunds, and access all data. Nice. All right. And if you ever want to know more about any of this, you can always click on the little bubble and the ask feature will automatically fill in the question that you that you see before that bubble and you can always ask. I love um the way Roy describes the ask feature as it's low-key distracting because you can go and ask anything and have a whole conversation with ask and then not get the and and take more time to complete the project. I think that's a great thing. I think if you ask all those questions out, you're just learning more and it's never a race on how quickly you can complete the project. The more you understand while doing the project, the better. Add secret key to Versell. The secret key must be stored as an environment variable, not in code. Go to your Versell dashboard. I don't know if I'm logged in. Let's see. Versel. Oh, I am logged in. Okay, cool. Go to your cell dashboard and select your ecommerce project. Okay. Navigate to settings and environment variables. Oo, where are you? Settings. Is it here? Oh, settings over here. Okay, maybe it here. All right, settings and then environment variables. And here we want to add a new variable called stripe secret key. H create new and I love this feature where you can just copy and then the value should be the secret key. Click to copy. Done. Click save. Save. Oh, and a little popup says added environment variable successfully. A new deployment is needed for changes to take place. Okay. Hey, don't I need to redeploy? No, we'll redeploy after adding the web hook step. Okay, so I think we can dismiss this. And then now we want to add keys locally. How do we add keys locally? Let's create a N local file. Right click and create a June project file. Okay. So, let me open up construct. Oh, okay. So many windows now. Right click on your project explorer. Okay. New file. and I have my new end file. Okay, we go back to our Strat dashboard and copy our secret key. Okay, just making sure that Yep, we had to save the secret key for Stripe. Okay, so copy the secret key. I think I might have it here right copied and then add it to the file and I hit save. Upload a screenshot of your end file. All right, adding your fear and done. Why do we need why do why do we store the secret key in environment variables instead of code? I don't know. I don't think we were told, were we? H why do we store these variables? Who's got an answer for me? Why do we store the secret key in environment variables instead of code? We've got new folks here. Shri and Zubar. Good to see you guys here. Where are you guys joining from? Okay. Um, I stored the secret key in the end logo because it is more secure. The next public prefix Not sure if this should be there. So, I'm just going to make a note of that. Okay. All right. Okay. I'm going to hit done and continue. What if my secret key gets exposed? Rotate it immediately. Go to Stripe dashboard developers API keys ro key and then update your versel environment key uh envir variable in the stripe dashboard. Click developers. Click the menu and select W key and you can reset it. Isn't that cool? Nice. Okay. Build the checkout flow. Great. Your Stripe keys are configured. Time to build the actual payment flow. You'll integrate Stripe Checkout, the same hosted payment page used by Shopify and thousands of e-commerce platforms. By redirecting to Stripe's page, card details never touch your servers. This drops your PCI compliance scope from 300 plus requirements to around 20. Oh, that's nice. And if you want to know more about what PCI compliance is, what does that mean, can always just click on the bubbles and have a pre-populated question and get the answer right away. Nice. Oh, I see TP. TP is typing. Hi, K Superman. Good to see you here. All right. Are you doing the project along with me? Just so you know, I'm going to put the project guide the link in the chat if you want to follow along. All right. So, now we're going to build the checkout flow. Strap keys are configured. Now, let's build the actual payment flow. You'll integrate Stripe Checkout, the same hosted payment page used by Shopify and thousands of e-commerce platforms. By redirecting to Stripe's page, our details never touch your servers. This drops your PCI compliance scope from 300 plus requirements to around 20. I just read that, didn't I? All right. So in this step I'm going to create an API route for checkout sessions on Stripe. This is important because it'll ensure that the user's card details don't reach my servers and the users, my customers in my econ commerce business will feel more secure that we don't have access to their card details, right? So, by using Stripe, you are protecting both yourself uh and um your customers. Awesome. Oh, superman says, ""I just woke up. I'll be starting the cloud road map."" Yay. I'm more interested in cloud DevOps part. Thanks for asking. Fair enough. But if you wanted to know how to you to deploy a website with VCEL, it's great project and you should check out the AI DevOps projects that we've out as well. TP says, ""I'm new to API security. Please help or please I need guidance on how to build an AWS API gateway using access controls like HTTPS and rate limiting. So you know what I will come back to that question but great question you know whenever you have a question I think it's an opportunity just post it here post it in ask anything it's you know there's always an answer and it allows anyone to come and answer the question for you so TP I don't know if you can hear me. Do ask all your questions and I will come back and give you a more type out a more detailed response so that you can see it. All right. Now we open our project in cursor and load the ecommerce project. All right, let's open cursor now, shall we? Okay, I have cursor. And what I want to do now is run this text into my cursor chat. Let me create a new one and close the other ones. And in the chat, I add run this. Oh, does it have to be on any page? No. Right. Run this e-commerce web app by first installing the packages required and then running the app. Okay. Subramana. Subra. Subraman. Nope. Um, where are you joining from? And uh what what name do you go by? Um I'll definitely do projects in AI DevOps. Nice. I was searching for AI DevOps and that's how I found the community. Really? That's so exciting. How did you find us? Did you find the projects? Did you find the Discord server? Which one happened first? I'd love to know. Let me know where you're joining from and what you're doing. Okay. So, cursor has some instructions for me. Let me also just check that I'm following the project guide. Okay. So, let me let I'm going to trust cursor and allow it to run. Okay. It's planning the next moves. All right. More suggestions from cursor packages are installed. Started the development server. Okay, let's go run more. Okay, I'm going to run. Okay, the ecommerce web app is running. Packages installed. Development server started in the background. The app is available on my local host. All right. Now the question is how do I open it in the cursor's built-in browser? So, Cursor has a built-in browser that lets you see your app and select components directly. We want to open cursor's browser. So, do control shift B. Okay. And enter localhost 3000. Yep. Okay. Just checking that it matches what cursor said. and enter. Tada. That's what I built. Exciting. All right. Ooh, I see messages. Um, Subu, nice to meet you. Joining from Mysore. And I found the YouTube channel. And then I went to I found the YouTube channel then went to the channel from there and then I got to the discord and then the website. I love the way you people building projects along with the A. Yeah. Have you done an extra work project yet? It is so much fun. You are going to love it. The cool thing about it is, you're right, you we're building projects. You've got the project guide and you also get this documentation. Look at that. As you build your project and you fill out all the tasks, answers, you get a nice documentation that you can then share on your LinkedIn. Um, you can download it as a readme file. You can add it to your GitHub portfolio. So many things, you know. So, I love it. Welcome to Next Work. I'm so glad you're here and I'm so glad you tuned in. Good to have you. Let me know if you have any questions, if you're stuck on anything. We do live troubleshooting, too. All right. So, now I'm going to upload a screenshot of the app in my browser. All right. I love Sean's product that he's selling in his e-commerce website. You know, after the session earlier today, I went and watched the video again and just cracked up. Okay. What can you do with cursors builtin browser? Um, cursors builtin browser lets me view my app and select UI elements to send directly to cursor AI chat. This is useful because I don't have to worry about copy pasting file pots anymore. Remove the anymore. You know, I don't know if you are anything like me, but I will read my text again and again and again. For the longest time, it used to take me forever to send out an email because I'd read and reread and read again. And I feel like sometimes it's the same way when I'm filling out the tasks. like does this sound right? And the good thing is you can always come back and edit. So if you find some typo or you want to explain it better, you can always edit either in the task or directly in your documentation. How cool is that? All right. Now let's check out. Oh no, let's create the checkout API route. We need a server side API route that creates Stripe checkout sessions. This is where we set the actual price. Never trust prices from the front end in cursor chat. Send this prompt. Install the Stripe npm package. Create a checkout API route at um app API checkout route.ts that uses Stripe to create checkout sessions. Let me expand this. Defines the product price server side. Never accept prices from the front end. accepts product info from the request body, but looks up the real price server side, creates a Stripe checkout session with the server side price, returns the checkout URL, uses Stripe secret key from environment variables, and then redirects to success on completion and checkout on cancel. So, this is an example of a really good prompt. When you're working with AI and building things with AI, it's really important that you have good prompts to also efficiently use your AI credits. So, I'm going to type this in. Okay. And um there's a little bubble here. Why define prices as server site? Yeah, we define it so that if we have a hacker, a malicious user who wants to get the get a um get the products we're selling for a fraction of the cost and manipulates the front-end price. Then we still have prices on the server side that accounts for that piece of security. Right. All right. Running. All right. It's go time. Roy is joining us. Yay. Party. Yeah. Sean, how are you doing? project um stripe is it which step are you on? And there's a message from Subu just the basic AWS setup. I had done setup already so it took me like 5 minutes but as you said the project and documentation showcase part is too good. build projects and you have your portfolio ready. Who doesn't like that? 100%. Let me give you a 100% emoji. I like it. Go time. Amazing. Roy, exciting. All right, Sean, how are you doing? We've got Louie. Louie, where are you joining from? Have you converted me? Yeah, I was just toggling around with my profiles. So, I don't know if I'll lose anything. Oh, I think I'm in the middle of this. Hold on. Let me digress really quickly. File, new window. And then, oh wait, if you look at my other profile, I think my default is white. Light mode. I don't know. Maybe I'm getting old, Sean. I just sometimes I feel like I can't see anything in dark mode. Oh, my e store is black screened. Ah, I didn't create that. I mean, I was okay with the black. Yeah, but black does look very stylish running those commands. Oh no, tool call ended before. Oh, sorry. Was socializing a bit. Let's do it one more time. Come to the dark side. Okay, let me focus here because last time it timed out on me. Create checkout API route. Checking if Stripe is installed. Stripe is installed. Checking the existing checkout route. Updating the checkout route to match the requirements. H. Okay. All right. Everything's looking good. No liner errors found. Checkout API route is set up meets all the requirements. Amazing. And keep hold on let me just check the project guide and not get ahead of myself. Okay. All right. So, I'm gonna say keep h am I supposed to see that? A screenshot here would have been nice. Upload a screenshot of your checkout API. about 30,000. Sorry. Local host 3,000. Is that not our H. What's going on here? I see voice comment. I see this project series labeled as PHOPS AI. Is the Phops in the future project builds in the series? Ah, I I think you're implying that this isn't these three projects alone isn't Phops and so you're waiting for one more. Is that right? What am I doing wrong? What is duck do there? Local host 3000 and I don't see anything. So, it isn't running per se, is it? H. Let me go back to the project guide. And what am I missing? Okay, let me try running again. Maybe I can ask is the e e-commerce web app running? Is the e-commerce web app running? It must be the post hog analytics. I never heard of it and I can't wait to learn it. Exciting. The server isn't running. Okay, let's restart the dev server. And while that's going, um, try Project IDX theme. I'm using that and I love it. What is Project IDX? Can you tell us more? Oh, he's left. He's not there. Okay. So, now apparently we're running. Okay, good. Now I want to check if um what did I just do? I created the checkout API route. Take a screenshot of your checkout API route code in cursor. Where would I find that? See this? It's in this file. Review. Okay. API app API checkout. Yep. Okay. So that is the new cat code and I will take a screenshot of this. Let's why must prices be defined the server side not in the front end. The prices is defined on the server side because if the front end controlled it, a malicious user could modify the prices and manipulate their shopping experience, shopping costs, manipulate they care or what do I want to say? Modify the prices and um buy the products. for a fraction of the cost. For a fraction of the cost done. Nice. Okay. So, now we want to connect the checkout button. Let's wire up your checkout button to call the API in the cursor browser. Enable element select mode. Oh, where is that? H. Oh, here. No, I don't see it. It's Oh, near the terminal. Is it? Where? Where am I supposed to look? So, this is the split screen. So this one, go to your browser tab. Thanks, Roy. Silly me. Okay. Enable the Yeah, it does say in the cursor browser. Enable element select mode. This is it. Okay. Hover over and click the checkout purchase button. Check out purchase button. Where is it? I don't have a check out purchase button. Did I miss it from the previous project? Was I supposed to have a check out purchase button? I don't have it. Let me check again. products. Oh, it's in buy now. Okay. The selected component gets added to your chat. Okay. Whoa. I'm trying to understand how my page is defined as well. So I've got explore product and then I've got the buy now. Okay. Right. So I hover over and click the checkout purchase button and the selected component gets attached to your chat. Ask cursor to implement the stripe payment API here. Okay, I'm going to read the instructions again. In the cursor browser, enable element select mode. Okay, that mode is selected, right? Select element. Yeah. Okay. Hover over and click the checkout purchase button. Let me go back here. Hover over Am I missing something? It does not seem to work because it should be highlighted blue over the area like this. Okay. Why am I not seeing that? What about the second tab? Yeah. Okay. Let me try one more time. Let me refresh and then Okay. So, I I click the buttons and it works. And if I click on this, nothing. I wonder what version of cursor do you have? Hi Richie, good to see you. Did you go for your morning walk bminton? Oh, did I get to this step in the last project? Oh, well I did. Um, I did tilt my my button. Are you supposed to put in the fake visa number of this? Yes, that sounds right. Is it mentioned in the project? Is there anything about 4242? Yeah, that's correct. It's a good test payment number. Yeah, Roy, that's my visa number. Good one. Good one. You're so funny, Rich. All right, let me see. Let me quickly check the previous project and see if I am missing something. Okay, in this project, if I go down all the way to my dev flow, I did all this Yes, I was able to do it at that time. And that's why you see this tilted button. So, it's not a cursor version. problem. But something is off, isn't it? H. What could be it? Maybe ask cursor to do the task. You know what? I might actually ask cursor. All right. So let's see what am I trying to do. I am maybe here. um hover over and click the checkout per purchase button and then select that component. H let me ask how do I select a component to modify? Is there a setting or Yeah. And I'm going to change it to ask so that it doesn't modify anything. Yeah. Sometimes I I'm too natural when I ask a question. So after I ask a question, I'm like, ""Oh, did cursor understand? Okay, let's see. What is the Oh, I should have asked a better question. So funny. Um, ask the browser enable select mode. Yes, that's a better question. Look at that way. You're so awesome. Yeah, maybe. I don't know if the ask was a good place to do it, but let's see. Let us see. Okay. Browser, dev tools. In the cursor browser, right click on the element you want to inspect. Nothing. What is going on? The dev tool panel opens up. Okay, you know what? I am going to change to agent and say turn on enable element select mode. My purchases are going through, but it doesn't look the same as the image in the project. That's possible. Is it very different? Put it in the chat. Let's check. Command shift E. Command shift E. Let me try that. I mean it is selected. Right click menu. Um, if you can't find it, share the cursor version you're using. I don't say I just did it like today. What version are you guys using? H I wonder if if I close cursor and reopen it might work. Okay. You don't understand why it would, but okay. I probably need to run the server again. Okay. Where's everyone else at? Shane, good to see you here. Sean is at the test payment um entering Roiy's credit card details. Roy is probably ahead of me. Lori, hi Lori. I Louie and UD are very silent. Let me say hi. Louie Lou with two eyes. Oh, it's not Ly. Maybe. Is it Is it a one one UR? Nope. How do I find you? I'm unable to tag Lori. Isn't that crazy? Oh, I see you. I didn't get a notification for this one. Also, still working on my displays troubleshooting hardware at the moment, but I can hear you. That's interesting that you didn't get the notification. This is what mine looks like once I complete the order. Okay. Ooh, the tandem bike is on its way. That is so hilarious. Let's see. This is my Chrome. I have Oh, it's different. So the project shows that it is payment successful but yours says order confirmed. I think that's just a AI generated setting. Oh hi Lori. Lorie says I am first timer from nextwork YouTube channel. Where are you joining from? I think that's a UI difference as well. All right, Sean, I'm almost tempted to have you um share your screen cuz I feel like I'm stuck. I think I need to um figure out what's going on with my cursor and why it is try now it's working. See, you know why? You know, I don't know how to explain this, but something with tech is reboot. Yeah, the the way to the fix things is just reset. I don't understand it. Great job. Reboot is always the fix. All right. Thanks for hanging in there with me. Um, let's see. Let's go back to this And here we want to hover over click the checkout purchase button and when you select it then attach that gets attached to your chat and then ask cursor to implement the stripe API here. implement the Stripe API here. H this usually is a copy text. Okay. So select it and then oh implement the stripe API here. The end result will be worth the effort. Oh, for the uh setup for the new setup displays troubleshooting hardware at the moment. Okay. Okay. I know I have four monitors. Two of them are gamer monitors. Whoa. Okay. I have a buy now and add to cart. Luri, where are you joining from? And good to have you guys here. Oh, this curve one is a beast. Discord looks insane on it. I never enjoyed the curved monitors. Is that something that you have to get used to or is it like either you love it or you hate it? Oh, Lor's joining from San Diego. Amazing. It's easy on the eyes. Your eye should be more native to the curve. All right. Okay. Let's keep it. Integration is ready to test. Click buy now. Okay. Wait. Let me should turn this off. Okay, let me go back to the project guide. Implement Stripe payment API here and then test the payment flow. Click the check out button to start the payment flow. I don't know what's happening. Hold on. Let me minimize the ask and I'm turning off the component select option. Okay, now it's all right. Look at that. And it's gone to my stripe phops um sandbox. Oh, it says 349 here. Okay. Nice. Exciting. Okay. Right. I am redirected to checkout.stripe. Took me a little bit of time. Uh I feel like cursor is a little moody today with me. And then now I want to test payment card or roy card and put anything that's in the future. Um and any CVCs. Okay. Should I put boy roying? All right. I'm paying. Payment successful. It's charged my fake credit card. Oh, Slot says, ""Guys, I had a PR of a big chunk of my project and there's a meeting soon. I'm really scared. This is my first internship. You got this, girl. You're going to be so great. The PR is going to be fine. I feel really anxious. Focus on breathing. Remember that the worst case isn't is you nothing bad's going to happen from the worst case. You're in an internship and you are here to learn. Everything is going to go well. Worst case, it's an opportunity to learn. It's all. It's all. and maybe dance a little. Go to the bathroom, do a little dance. Uh remove that anxiety and just have fun. Just go in there in the meeting, smile. Got this. Oh no. Sloth left. I was giving all this advice for no reason. I didn't dance and she's not even there. Okay. All right. Right. I will tell S ladder. I scared sloth. Poor sloth. She She's probably saw my dance and thought better to be in that meeting than see my dancing. All right. Payment successful. What? No screenshot for this. That's a bummer. Okay. Verify payments with web hooks. All right. The check checkout flow is live. But here's the thing. The success page isn't proof of payment. Anyone could navigate directly to success without paying. The checkout redirect tells users payment succeeded but you need cryptographic proof and that's where web hooks come in there. How Stripe officially confirms payment completion. Let's first see why this is dangerous then fix it with signature verification. All right. Okay. What are we going to do in this step? In this step, I am going to create a web hook and expose its vulnerability. I will also oops I will also add signature verification to block fake web hooks. Yeah, I think that covers it. Okay, done. Create a basic web hook endpoint. Let's create a web hook endpoint without security to see why it's dangerous. Open cursor's chat and ask cursor to create basic stripe web hook endpoint. Okay. Sloth, you're back. Oh, sloth is not listening but is reading. Okay. Okay. Okay. Sloth, I did a little dance for you. Oh, wait. Senior dev is going to be there. I will just ask questions. It's okay if I don't know anything. I'm an intern. I'm supposed to mess up and ask. I should not be scared. Yes, sloth. Do a little dance before you get into the meeting. Always helps. Maybe in the bathroom before you go. Yeah. Do a talk to yourself in the mirror. You got this. All right, we are we've created a Stripe web hook endpoint and we'll go ahead and keep it. Oh, time for a scary demo. Sloth is going for a scary interview. I'm doing a scary demo. It's all about facing fears. Let's do this. Let's prove this endpoint is dangerous by sending a fake web hook. Okay, I'm using Mac and I'm going to open a terminal and run this command. Receive the true. Uhoh. Yikes. Your server passed accepted a completely fake web hook. If your app fulfilled orders based on this and attacker just got free products and that is why we need signature verification. All right, let's add signature verification. Fixing this vulnerability in cursors chat send this prompt. Ah, so many windows open. Roy, is that what you do before these live streams? Roy, I'm not scared to come on the live. It's like doing a live stream is like coming and seeing my friends. I've got I'm like, ""Oh, Shane's going to be there. Sean's going to be there. Royy's going to be there."" And then if you guys don't show up, I'm like, ""Where is Roy today?"" So, when you weren't there in the beginning, I honestly I was just like, ""Oh, no. Where's Roy?"" And then when you showed up, I was so happy. All right. Updated web hook endpoint to verify stripe signatures. Everything looks good. Hi Net. Okay. So, we've done that and now we want to we sent the prompt and now we want to configure the web hook in stripe dashboard. So, in this stripe dashboard, if I click this, will it open? Yep. Okay, good. Uh, click developers. Okay, click developers. Where's the developers? Oh, bottom here. Okay, click developers and select web hooks from the menu. Okay. Add destination. And select checkout session completed event. H how do I find this now? So I'm at web hooks. Add destination. Check se select the checkout session completed event. Checkout session completed event. Continue. Also select the payment intent. Oh, I should select both. Okay. payment intent failed. Okay. So now I've selected two events correct. Yep. Anything else to Okay. No. Continue. Okay. Select web hook endpoint and continue. and then enter your endpoint URL. Okay. Enter. Except it has to be my app. Oh, okay. Where is it? Um here I'm going to add my app here and remove this. Okay, now I should copy and then copy the signing secret starts with create event destination. I'm choosing the destination. I also recognize that what I see looks slightly different from what's there in here. So, I'm just going to make a note of that. What do I need to do now? Copy the signing secret with I don't know. I'm going to create the destination and then see. Maybe we're missing a step here. Okay. Yeah. So, if you need a create destination, click on the button. All right. Copy the signing secret. Where's the signing secret starts with? Oh, there it is. Okay. Upload a screenshot of your web hook signing secret. Okay. Here you go. Why do web hooks need signature verification? Without signature verification, an attacker could Let's see. How do I want to explain this? Oh, didn't I upload this? Oh, no. Just kidding. I'm in the wrong task. Okay. Without a signature verification, an attacker could um Yeah, I'm running a blank here. ask, ""Can you help me please?"" Why do web hooks need signature verification? Um, without it, an attacker could send fake web hook events. An attacker could send fake about events. Okay. Um, the signing secret proves the proves that the web hook originated from Stripe. Okay, good. Thank you. Ask. The ask is like a little buddy. like you don't feel alone when you're stumped and it's like right there. Super useful when you're troubleshooting errors too, especially with the DevOps projects. Believe me, I've used it like add web hook secret to cell. I just want to check in. Oh no, we've got only five minutes. I only have one task to do. I've got another event to run into as to to prep for as well. So, let me see if I can finish it. Add web hook secret to Versell. Go to Verscell dashboard. Click settings. Click settings in the web hook in the versel. Okay, let's go back to versel and select environment variables. Yes. And Add stripe web hook secret with your secret value. Don't look. All right. Copy pasted that. And I'm going to hit save. And now I have to redeploy. But where do I go to redeploy? Upload a screenshot of your web hook secret in versel. Oh, there it is. How does your server know if a web hook is real or or fake? A fake web hook was rejected because the server knows if the web hook is real or fake by checking the signature verification, right? server knows if a web hook is real or fake by checking the web hook signature. All right. Okay. Let's try this one more time and on cursor. Oh, error. See web hook secret is not configured. No, that's not the error we want. Yikes. All right. All right, I think I need to verify my web hook secret and the configuration. And I have to call it a day for now because I have I have one more event for the day. We've got Humans of Next Work in 30 minutes and I need to really quickly prep for that. So, my apologies that I couldn't finish it in time. So close. So close. Just one more task left and um a whole well a little bit of the web hook um configuration to to set up. But it was super fun and I will be done with this project in like 10 minutes if I focus less than. But yeah, I hope um thanks Roy. You're very very kind and I really love the energy that the community especially you guys um give me. Let's see. Shane says, ""Can I give a shout out to Nexwork? They really helped me get to where I am today as I'm messing with and trying to troubleshoot this new hardware. Mind you, a year ago, I couldn't even think or afford this tech. And now I'm upgrading my foundations and tools. I can go even further now. Just a moment of reflection."" A Shane, that's so sweet. Thank you. Next work is a stepping stone platform helps you learn, build confidence, and figure out your next move. The developers button in the bottom left corner. Hiding. Finally found it. Yep. Roy, looks like you will be on top of the leaderboard tonight unless my setup starts acting correctly. We are only on day seven and there are 14 more days to go. So easy to pass me up the marathon they say. Mhm. Thank you, Roy. Beast mode. I'm trying. You guys are such inspiration. Also, I want to put this up. This is looking beautiful. Oh my goodness. Oh, can you hear me? Yes, but I cannot hear the Yo yo. What's up, gangster? We got the tandem Kubernetes bicycle company rolling in, dog. Sweet. Oh no. Be prepared, Sean. I think everyone's gonna buy this by me. Okay. All right. All right. U Shane, Sean, Roy. So awesome to have you in the build lab as always. You guys are so amazing. I really hope that I will still see you for the next event. You've been with me since morning. I've been doing three events back to back and you guys have been there which is so awesome. Lori, it was so nice seeing you here. I hope you enjoyed and I hope you keep joining. I think for now I'm going to have to say I almost got to the end of the build lab. Looking forward to Roy Shane and Sean's project documentation. Do join me in 26 minutes as we meet the founding engineer, one of the founding engineers of Nextwork. So he's seen next work when it was in dark mode. Can you believe it? Next work used to be in a dark mode. It used to be black and blue and now it's in light mode. H let's go. Let's go hear all about it. Um Christian used to be in Canva before I joined at Next Work. So stay tuned for all the fun stories that we can hear from our engineer. See you soon. Bye.","**Secure Payments with Stripe: An Interactive Build Lab**

The **Secure Payments with Stripe** build lab is an interactive project that focuses on integrating **Stripe Checkout** into a **Next.js** application with proper security practices. The project builds upon the previous **Finops** project, **Ship a Landing Page with V0 and Versel**, and aims to create a secure checkout flow that redirects to **Stripe's** hosted payment page.

**Key Takeaways:**

1. **Stripe Keys**: The project emphasizes the importance of storing **Stripe secret keys** securely using **environment variables**.
2. **Server-Side Checkout**: The lab demonstrates how to create a **server-side checkout flow** that ignores front-end submitted prices, ensuring that prices are defined on the server-side to prevent malicious users from manipulating prices.
3. **Web Hook Signature Verification**: The project highlights the importance of **web hook signature verification** to prevent fake payment confirmations and ensure that only **Stripe**-confirmed payments are fulfilled.
4. **Secure Payment Flow**: The lab showcases how to integrate **Stripe Checkout** into a **Next.js** application, ensuring that card details never touch the server, and reducing **PCI compliance** scope.

**Step-by-Step Process:**

1. **Set up Stripe**: Create a **Stripe** account, obtain **API keys**, and configure them in **Versel** and locally.
2. **Build Checkout Flow**: Create a **server-side API route** that creates **Stripe checkout sessions** and defines product prices on the server-side.
3. **Implement Web Hook Verification**: Add **web hook signature verification** to prevent fake payment confirmations.

**Tools and Technologies:**

* **Stripe**: A payment gateway for online transactions
* **Next.js**: A React-based framework for building server-side rendered applications
* **Versel**: A platform for deploying and managing web applications
* **Cursor**: A tool for building and interacting with AI-powered applications

**Best Practices:**

1. **Store sensitive keys securely**: Use environment variables to store sensitive keys, such as **Stripe secret keys**.
2. **Define prices on the server-side**: Prevent malicious users from manipulating prices by defining prices on the server-side.
3. **Verify web hook signatures**: Ensure that only **Stripe**-confirmed payments are fulfilled by verifying web hook signatures.

By following this build lab, developers can create a secure payment flow with **Stripe** and **Next.js**, ensuring a seamless and secure checkout experience for their users.",2026-01-16T01:50:32.819261
NextWork,Connect with Community,V2gFivvYz7o,"Hi Roy. Hi Shane Sloth Sunny. Good to see you all here today. This is connect with community and I am my from the network team. So good to see all of you here. Some familiar faces. Well, some familiar display pictures and some new ones. I'd love to know where you all are joining from. We've got Fiso Garanchi. Good to see you guys here. Hi Roy. Are we in a position to hear your amazing radio voice today? I have to say I'm so impressed with Shane and Roy. They are neck to neck in the leaderboard. Um, it's always like who's going to finish the project first now. All right, let's see. I'm very excited about today's project. We are We just made this project. We just went public with this project or we made this project public. Secure payments with Stripe. This is the second project in the Finop series. The first one is ship a landing page with V0 and Versel. If you've completed it, you probably realize that it's a super fun project. very exciting, kind of chill, not too difficult, but it gets more intense with the remaining projects. You get to add Stripe um and actually facilitate payments in your e-commerce website. This is just a landing page. I think this uh project is really good even if you're not keen on a ecommerce landing page just a good website build simple um you learn a little bit about prompting AI to do to build and create a a website for you and I think I think it's cool to check out I I'm not done with my project yet so um I don't have a document mentation to share. But if you if you check out what others are working on, you can see so this is Shane's documentation. Check out this one. You can see what everyone built and created. Um, this is his landing page. Um, and you can see like the products that came in as well. You can take a look at uh the different angles. So if you know the project, you would know the little minor changes that everyone made to to their project and how they were they got creative with with it. We've got Shane. Um that's Roy. Yeah, Shane and Roy are just neck to neck. And this one was a holographic AI companion. Oh, an AR wristwatch. And of course, this is AI generated. And I think that's where we all got to be a little creative in the project, depending on what we wanted to sell in our e-commerce page. I think this project is great to get started with creating a website, deploying it into a platform like Versel and really understanding how easy that can be. What do you think, Shane? Sean, I see you. Roy, who is this project really good for? You can always ask too um who would benefit from doing this project. Let's see. Sean is here too. Sean finished the project too, right? I think so. Let's see. We've got Pano Amber Roy. Where are you, Shane? Sometimes it takes a little time to show up. So yeah, this project is ideal for several types of learners. Beginners in web development, especially those new to React or Nex.js, JS individuals interested in AI assisted development, fast deployment workflows, those wanting to build e-commerce landing pages, those who want to learn modern development patterns. Roy says, ""This project is good for anyone with an idea. an idea to build a website and wants to deploy it. Yeah, even if if you're thinking about starting a business, I think this is a good place to start. Get your hands a little get your fingers a little wet. Um see if if this is something these are tools that you'd like to use. Blue Moon, Asma, and Ibrahim. Where are you guys joining from? We've got Roy, Shan, and Shane all joining in from the US. I'd love to know where Bloom is joining from. From London. Oh, good to see you here. Ooh, Germany. Okay, we've got US and Europe in here. Awesome. What do you guys do? Are you students? Are you working? And it must be pretty late over there. Where in Germany are you, Ashma? Near Berlin. Sean says, ""I'm almost done with the stripe. You're done with the stripe project?"" The one that came out today. That's this one. Wow. Sean, I'm putting you up on stage. Hi, Sean. >> Hey, what's up? How you doing? >> Nice to hear your voice. >> Yeah, you too. Um, yeah, I was I did the stripe with um Cahoo uh today on uh while he was live streaming on YouTube. Oh, right. Yes. When we >> released the project, we released the video, too. That's awesome. Sean, how'd you find the project or you still working through it? >> Yeah, I'm in the last uh stage, but I keep getting distracted playing around with AI uh image generator and video gener generator. So, my my fake company is called um what's it called? Uh, and Kubernetic bicycle company, Kubernetes bicycle company with Maximus Maximus and Mcloven as the CEO founder actually created a little video. It's kind of funny here. I'll post it. Let's see if I can post it. Paste it. Yeah, right here. I just made the You're just being silly, wasting time, being distracted. I mean, when you're learning, we have fun. >> I don't know if it's going to work. I just created it. It might work. >> What? Oh my goodness. >> Did it play for you? >> Hold on. Hold on. Oh my goodness. Oh my goodness. I can't I I I cannot even get myself to watch. I cannot hit the play. I'm so excited. Okay, ready? >> Yeah. >> Yo yo, >> what's up gangster? We got the tandem Kubernetes bicycle company rolling in dog. Sweet. >> Yo, what's up gangster? We got the tandem Kubernetes bicycle company rolling in dog. Sweet. Yo yo, what's up gang? >> Wow, I love it. >> That's supposed to be Nick loven. >> Wow. Wow. Wow. >> Daddy's bicycle company rolling in dog. Sweet. >> So good. Let me this link to the team. So funny. I can't I might need a minute to recover this stuff. I'm gonna put that I'm going to put that on my my my landing page of the company. Oh, >> I love how how much fun you're having with with this is inspired by an extra project. >> Yeah, I'm gonna put it on the front of the of the company page because uh you see if I can find the company. They're the founders, right? And it's a tandem bicycle because Kubernetes, you know, attaches, you know, things together. Oh, >> wait. >> My cheeks are hurting. >> What are you What are you selling, Sean? I'm selling Hang on. I'll pull it up. I'll I'm selling bic tandem bicycles. >> Whatever you're selling, Sean, I'll buy it. Hi, everyone. Hi, Kate. Good to see you, Tanya. And on YouTube, we've got Mr. Cheese Bunny ID. So good to see everyone here. Very cool. Bunny ID is joining from India. What about you, Mr. Cheese? So cool. Um what we're what we're talking about today is this project. This is the first project of the Fin series. So this one was released yesterday and as a part of this project we get to sell whatever we want here and Sean our legendary next work I don't know our next work legend I think let's let's just say that um he is selling a Kubernetes bicycle with Maximus and Mcloven and he has created a nice video on VO is here. Hello, can you please make my Tik Toks for me? Oh man, that's hilarious. I love it. just having fun. >> My goodness. >> All right, guys. I'm sorry. I'm just laughing so much. I'm just enjoying. Is anybody finding this as hilarious as I am? Sean, that's so good. Yeah. Oh, what's up gangster? We got the tandem Kubernetes bicycle company rolling in dog. Sweet. >> It actually does look >> maybe not the side as much. >> Rolling in dog >> there. Right there. It's so good. >> What's up, gangster? >> All right. All right. Yeah. Thanks for >> Now you know why I'm only 80% done. because I get I go up on these little tangents, but I'm having fun. Order confirmed. >> Roy has bought it. Oh my god, you guys are so funny. >> Got one. >> Confirmed. Hey, did >> Roy, did you did you uh did you purchase it on the website? Because I'm only 90 80% done. So, you just confirmed that that uh that uh that it worked. That's crazy. >> Yes, it is using VO. Check out this project if you haven't already. I think it is super fun, >> very light, but you do learn how to use Versel and Vzero. And if you're completely new and you just want to get started with building something, shipping something out, I think I think it's a great and then you can go extra and have ads created on VO and embedded into your website. Have you added it to your website, Sean? No, not yet. I I wanted to finish the project first before I I mean I imagine I can just just post it in a link on there. Uh yeah, let me do that right now actually. See, that'll be hilarious. >> Nice. Yeah, exciting. Um, I think this is such a fun project and the build lab yesterday was with Pano and it was super fun, very light and very very useful as well. really good insights from Pano with his engineering lens and backend perspective on how AI is useful, how you can keep it, how you can make sure there's no unnecessary build, how to review AI code. Good good questions that came up in the build lab as well. Uh, good to see you here. Tanya, King, Asma, have you guys started this project? I know that King is working very diligently through the AI DevOps series. Let's check it out. The AI DevOps series are these projects here. Wow, we've got 60 people who have completed. Amazing. Very cool. And it it's not even been a week. Nice. And then with the second project that is part of the series, it's containerizing rag API with Docker and then deploying it with Kubernetes. I think that's where King is at. And this is the spiciest of the four projects. So prepare, set aside some time for this. It's um definitely a good one, but intense. So yeah, a lot of um troubleshooting with this project. Nice. Nice. King King, you're a pro too with um with all the troubleshooting. I think if you can set up everything in one and yeah, two is probably how is um how would you say uh of of the four I think the first one was the easiest. Hey, did you check out this new feature that we added? It's the refreshed. So now you can tell when a project was last updated. So if you've got um you know maybe a different like because everything is changing so quickly AWS console updates quite frequently and this way you can just keep an eye on oh this is a new project. Oh this has been refreshed. This has been updated and you can check out and keep an eye on what are the new stuff that how how we've been updating the projects. Yeah, it'd be cool if you know because it happens especially with the AWS console. Um, the screenshots sometimes don't match just because AWS has updated and we haven't updated. So, if you tell us that we need to update a project, it's great feedback. We can update it and you will see a refreshed date. Yeah. Very cool. How are you finding the project, King? Let me add you on stage. Tanya Asma, are you guys in a position to speak? I'm just inviting everyone. Hi King. >> Hello. >> Hello. Hello. >> Yeah. Yeah. So, >> yeah. >> What was your question again? >> Oh, yeah. You've completed the part four and now you're on to part four. Oh, sorry. You've you've completed part three. >> Part four. >> Yes. >> And now you're on to part four. >> Yeah. >> How are you finding it? >> Uh definitely a little spicy, but it's still worth the time. So I get it. >> Yeah. >> Yeah. without doing these projects, maybe you can just take on the Kubernetes courses online and all, but then you will still not really know how to connect everything from one stage to the other. So, I I find it very interesting to do that. >> Oh, I like that you're saying that you can always do the courses on Corsera. Yeah, you can always take on Corsera courses or Udemy courses. You always see those ones. But then like if you really want to take tackle a project, maybe you can learn um cube um Q car how to do all those pling commands. But then now how will you actually use it in a project and how will you move from one stage to the other stage? Now that's where next comes in. So >> yeah, >> thanks for sharing that feedback. Okay, >> you're welcome. >> Do you find that after doing a project you are equipped to do something of your own? >> Yeah. Yeah. Actually I am. >> Yeah. >> Yeah. Yeah. even um I think I think that's these are right now our next projects are designed to be quite small easy to complete easy to to learn things that are not so easy to learn and it's good to have that feedback that you know it is useful and if it is um if it takes more time to do projects We really want to know. Um yeah, even the difficult things that um that you want to learn can easily be broken down. Like if you want to do calculus, if you want to learn how to do calculus, you got to learn algebra and then work your way up to calculus. And you can always portion it and and make sure everything is is you know it's it's never overwhelming. >> Yeah, I get it. >> Once and then and then once you learn all these things you you should be able to apply that and and build your own things. >> Very true. Yeah. I plan on trying to build something myself. maybe looking for something out there and then see how I can build it from scratch myself. >> Yeah. Yeah. I'm really keen to know what you build. Uh what I've been thinking is how it would be epic if after the 21 in 21 open up like a open challenge for anyone who wants to build something based on all the things the tools that you've picked up through an expert projects and then see what people come up with >> wow okay sure I will try that >> something that's useful and meaningful to your role to your needs to your personal system professional system something that you can say ah I made this without a project guide cuz I learned all this skills Okay. Yeah. Okay. Yeah. I think that's the ultimate goal. >> Yeah. Yeah. Let's um Yeah. I'm I've been brewing on this idea and really keen to know what you all think about it. Is is that something you would be interested in doing? >> Yeah, sure. >> Yeah. For me and have a >> I'm cool with that. >> Yeah. You cool with that? I know Royy's typing. I'm kind of waiting. Oh, yeah. That would be nice. I would like to start doing open build. What does open build look like? You know, I had a Next Work community member share that they have all these projects that they want to build for us. They're like, ""Next work is great. I want to be part of the project um development. These are all the ideas I have."" And great ideas too. And so I've invited the learner to present some of the ideas and we'll see how it goes. Yeah. Build a project live on the demand. Ooh, I would totally want to join that session, Roy. So, what does that look like? You ask people for ideas and then you start building. I think Sean has um definitely done those kinds of really cool sessions where he just builds and you can watch and ask questions. Roy says, ""I ask Chaji to generate a random project, then do a time limit of 1 hour or two and then build it. That is so cool. That's so cool."" Yeah. It's really cool how Chad GBT and other AI tools have really unlocked a lot of possibilities for us. Learning is easier, faster and more accessible. It's a good time to to to change the way education looks like. Don't you think? >> Yeah. Yeah, that is right. So I also wanted to ask so has a new project been released today? >> Yeah. Okay. So I'm glad you asked. Asthma also has a very similar question. What is 21 days? Asthma, we are releasing 21 projects in 21 days and starting on Jan 9th which was seven days ago. We have been releasing new projects. So we have released seven projects from Jan 9th. So let me walk you through the projects. We had a DevOps and AI projects uh DevOps and AI series. And even though we didn't do this in order, so this was day 1, day two, day four, day five, we sneaked in a security project. And we're planning to sneak in a security project every Sunday. And by Sunday I mean New Zealand time. So every Sunday on a New Zealand um week we have a security AWS security project and so this started on Friday. This was a Saturday we had a security project and then deploying rag API to Kubernetes and then automating testing with GitHub actions. That was one set. So this was 5 days. This is the security project is right over here. Okay. How many Sundays in 21 days, Rory? Tell me. So that is four projects. This is the fifth one. And then we had the new series which is the Phops AI and PHOPS and this is day six. This is day seven. So let me share this project as well. Let me share the series with you. So, we've got um day six, day seven, and then tomorrow will be the last project in the series where we can analyze the payments. So, yeah, check it out. Is there anything that excites you, Asma? Yeah. Yeah. I I I think we will shortly add 21 projects in a tile so that it's super easy to find. So these are all the tiles we have and the projects can can get a little lost in here. So maybe we'll have a tile for 21 and 21 so that you can be part of the challenge. Um, this is this leaderboard is slightly outdated because it's day four, but as of day four, this is how we were doing. Yeah, but definitely slightly outdated. This was on day four. We have got Shane and Roy neck to neck. Shane says, ""I like that."" I know why. >> They're always keeping up with the pace of the project release. >> Yeah. Yeah. King, I see you in there. Oh, no. Now I'm far behind. So, >> no, I don't know. I I think you're still there. Hold on. Let me see if I can pull up um I can pull up the >> latest. >> Yes, it might be it it won't look as beautiful. Bear with me here. >> It's fine. >> Oh, King, I don't know. This doesn't look right. I feel like you've done more. So, you tell me if this is right or wrong. This is of um true as of last night. And so, that's like over 12 hours ago. So, you might have caught up since then. Let's see. >> No, I haven't done this one this project yet. >> Let me >> Yeah, I plan on doing it this week. Where is it? One second. Pull it up. Okay. This is the recent most recent read leaderboard I have. We have Boy and Shane at five. Someone in Argentina at 4. This is again 12 hours old. >> Okay. >> Yeah. More than 12 hours old. So um we have Sean, someone in UK, Sammy from Belgium, India, Korea, US. Oh, I I think this is um No, he's not here right now. Got Yeah. India, Ghana, US, South Korea, Belgium, UK, and US. It's a go time. Yes. Harry Brewer. That's that's the name. Harry L from US is Harry Brewer. And we've got Ahmed from South Korea who's king on the leaderboard. Exciting. Yeah. So, Asma, does that answer your question? Let's let's get your names up here on the leaderboard. So, says while I was doing the >> Yeah, very true. Sorry. You may continue. >> Oh, no. You were saying what is very true. >> Um, I So, Roy and Shane are super speedy. I'm not going to try and compete. I get that. Believe me, I get that. >> I guess we are seeing we are seeing those who complete the project the 21 days project first. We already know their names. >> Hey, you never know. Do you know the story of the tortoise and the hair? Uhhuh. I think I've heard of it before. >> It's a very classic Indian fable. So maybe it's a it's something that Indian kids grow up with. Anybody else familiar with the story? Ah, so yes, of course. Okay, Shane knows. Slow and steady win the race. That's right. That's right. It's um we it's so there's a the two toys and hair decide to get into a competition and they're like let's race and and the hair looks at the tortoise and I'm like, ""Oh, of course. Let's let's do this."" And so they start their journey and the hair is super fast just hopping along and the tortoise takes its time very slow steady and it just keep keeps going and the hair is just like hopped its way almost at the end and it's like I got so much time I am going to take a break and the hair just takes a nap and the tortoise just keeps going slow and steady and the tortoise wins the race and by the time the hair wakes up it's like oh no I missed it. So, you know, the moral of the story is slow and steady wins the race, but really it's it's consistency that matters. You just keep at it every day. And it's something that we really care about. It's a it's a value at Next Work. I think I think Amber always says whenever we are building things and releasing things, she always says it's not a sprint, it's a marathon. >> Yeah, I've heard I've heard those famous words before. >> Yeah. And to do 21 projects, that's intense. It's it's not a sprint. It's not one week. It's not two weeks. It's three weeks. So, a lot of consistency there required. So, Roy and Shane, we'll we'll just keep an eye out for you. Are you really going to win it? Everyone is betting on you. We'll find out. >> I don't know. My number one bet is Roy. Your number one event is Roy. >> Yeah. >> I don't think Shane's gonna like that. >> Yeah, >> because Shane Shane is Shane is um keeping up with his computer, fixing stuff, you know. So, I'm betting on Roy. >> Oh my goodness. We are placing bets here. Shane, you got to say something about this. Shane, are you sleeping? Oh no. Shane, come on. I know. You know, he built his he he he he set up his computer, his new computer, and I got to say, he is he is six on six. Today's project is the seventh one. We're just getting started. We just released it like less than 3 hours ago. So, I don't know. I think I think I think Shane's pretty competitive, so we'll see. >> Okay. >> Shane is also a busy busy person. I don't know when he makes the time. This is true. This is true. So says, ""Speaking of bets, my first project is about a way of betting. Your first project tell us more."" So can you join us on stage? >> Hello. Hello. >> Hello. Hello. Hope uh Holly hope of you are having a wonderful day. When when I see you guys, my day automatically becomes wonderful. >> Same here. >> So, the first project is about writing a martinale report. Now, have you guys heard of martingale in terms of betting and roulette strategies or something? I haven't. >> This is your first project um as part of >> Yeah, for your Yeah, this is my first project for the master's course. So, yeah, Sean Sean is right that you double the bet each time you lose. So, let's just say if you have a bank roll of $10,000 and you're playing at a casino, you are you want to win initially you start off with the bet of one $1,000. >> Mhm. Now, if you if you uh win cor if you win, you basically get twice if you win you basically get those $1,000 added to the bank roll. And then you other if if you lose the bet then you lose those $1,000 and then you have to bet twice as much money $2,000 in the next turn. >> Okay. >> So essentially you're just bet it's like a betting strategy. Okay. >> Have to and uh essentially have to determine the probability of You have to kind of code up a way to determine to write a software that will perform pro probabilistic experiments involving an American roulette wheel. >> Can you explain that again? So I have to write a software that will perform probabilistic experiments involving an American roulette wheel. Probabilistic experiments. That is very interesting. Okay. And then tell us more. So essentially it is building a simple gambling simulator where I will have to revise the code that is given to me to simulate 1,00 successive bets on the outcomes or spins of the American roulette wheel given the betting scheme and the pseudo code. So, they give you a pseudo code which explains the betting scheme. And my job is to basically just simulate 1,000 successive bets on the outcomes or or spins of the American roulette wheel. >> Very cool. >> Yeah. Each series of 10,00 successive bets is called an episode. You should test for the results of the betting events by making successive calls to the get spin result function. Yeah, it's basically you're b in all simple terms, you're basically building a simple gambling simulator as uh one for project one and then you're building and these projects add up in the end to create a simplified AI based trading system. >> Mhm. Yeah. So that's what the class is about. So >> the entire class project is to deliver the simulation or modify the the simulation that's given to you. >> Yeah. So I'm just going to show going to show it. >> Yeah. So you are basically going to develop a simplified AI based trading system which is consisting consisted of over of eight projects and each project that I complete is a step towards building that intricate system which synthesizes is machine learning with practical algorithmic trading strategies. So for this project, you will write software that will perform pro probabilistic experiments involving an American lead wheel. So it will help provide you with some initial feel for risk, probability, and betting because purchasing a stock after all is a bet. The stock will increase or in some cases decrease in value. You will submit the code, the do great scope and everything. >> So that's so cool. >> In this project, you will build a simple gambling simulator. And you will revise the code in the Python file to assimilate 10,000 successive bets. just answering sloth slot. this project I cannot share it because it's really it's um only restricted to Georgia Tech students but I mean you can just uh just shut up search up Martinale project and try to see uh you can um perhaps like you could just share the code you can just look up any Morton guild project on YouTube and see what you can do with it. >> Is this um machine learning for trading? >> Yeah, it's machine learning for trading. >> Hold on. I think I just found it online. So maybe we can share this one. Is this the one? So does this. Do you want to share this and see if it looks similar? >> Yeah. >> This is from 2020 though. >> Fine. Summer class is really intense. H Yeah. Yeah. I see the Martin Gale project. Hold on. Is the Is this the professor's website? Lucy Labs. >> Um, this is I guess it's Lucy Labs. Yeah, this is professor's website. Yeah. Yeah. >> So, >> usually professors make their things public. It's not syllabus isn't a proprietary information. It's I I come from academia so I I know. >> But yeah, you you can check it out. Sloth. Very cool. Yeah. So that's the whole project. So first up that I'm going to do is try to see try to understand what is expected of the gambling simulator. >> Test it test the results. >> Yeah. >> Yeah. And then you have to track your winnings by storing them in a numpy array to be just set to zero just before the first spin. Is this something that you have to break down and share like the experiment one, experiment two is um cuz I know this is an older one. >> Yeah. But it's basically the same thing. >> Okay. That's what was explained in canvas. So what you have to do is with these experiments, the purpose of these experiments is to actually see how well your betting strategy works. >> Mhm. So for you are plotting several figures for experiment one you're plotting figure one by running the simple simulator 10 times and tracking the winnings plot all 10 runs of wide using map lip functions and then we run the simple simulator 1 times then you're plotting the median instead of the need. >> Yeah, I think I think that is the expectation with Matt lab matt um mattplot lab functions. That's Python. >> Yeah. Yeah, that's what Yeah, that's what the course requires you to write it in Python. So, a more realistic gambling simulator is that you're basically just using you have a limited bank rule because if you run out of money while betting that's it, you're done. So, this is where experiment two comes into play. Mhm. >> And then you have to write a report after you're done with coding, which is also quite interesting. Uh because not only you need to not only the thing about this project is that only do you need to make sure that the code works, but at the same time write the report. just pressure as well because they can take up they can cut off some really crucial points if you are not explaining things correctly or anything. So yeah, you're only allowed three submissions to project one but unlimited submissions. Cool. Yeah, but yeah, that's just the basic promise of uh this project. >> Amazing. Thanks for sharing with us. I really want to know and see how you develop that and I'd love for you to share with us. >> Yeah, for sure. Project is exciting. It's just that the due I'm not a big fan of the due date. like it's just going to creep up on me and be like, >> ""Yeah, I mean it's a a good exercise to plan things out."" So, I had a professor who would say that don't ask for extensions for for the due dates that I I set. If you want extension, then just tell yourself that the due date is a week before and then you will have gotten yourself a week extension. >> And that's a good way to look at it. >> Yeah. I and and for me I always find that if if I have whether I have 5 days to do something or 10 days to do something I end up using all the time that I have. So it's better to say that I'm going to do five I'm going to do it in five days and then I get five extra days to refine it. >> Yeah, that's a good way to look at it. Yeah, for sure. Um, we've got Sean who says, ""For the report, try using quant by Lumi Weld."" Interesting. I use it in my models. What models? Oh, for your um prediction models. Yeah, yeah, yeah, yeah. Sean, you can unmute. Or maybe you cannot. >> Oh, I totally forgot. I'm here. I'm sitting here typing away trying to, you know, love the chats. Oh. Um so what got me into coding was uh this guy that that mentored me for like eight months and that that was my first uh exposure to coding or IT world besides you know laying person and um this guy who used to work for um a hedge fund out of Canada he he 36 anyway um making this side story too long he he broke away and built his own sublanguage in Python and it's called Lumiot or Lumi wealth. It's and so part of building theory trading bots in Python was we needed a stat um you know stats it's called a tear sheet and it takes whatever you built in Python and when you run it will create and format its own statistic report. So if you're dealing with it's just a function. So it's like a a Python library um that he that he uh you can pip install LumiBot. Now I know that's the trading um algorithm and platform and structure, but it should work in the roulette um you know theory because you're still dealing with numbers and ratios and and probabilities and stuff like that. Um but yeah, that's that's my jam. That's what I I got uh I I first got into Python with um but uh so that's why you see Lumi well uh quant stats by Lumi. That's that's what that is. It's just a tool. It's a tool within that that environment. >> But hey, I'm just trying to help. So >> yeah, hopefully that's helpful. So no, >> this is good resource. Yeah. Nice. Very cool. >> Yeah. And I I use that Martindale um technique in like three or four of the bots that I wrote, just exploring basically exploring exactly what you're talking about. exploring, but there is a a cliff, if you will, you know, because you can only uh lose so many, for lack of a better term, so many hands before you're unable to double down. It's exponentially, right? You start with a dollar, then you got to double $2 and $4 and, you know, before, you know, get in $2,000, you only have 10 10 grand. That's only what, four or five evolutions before, you know. So, if you get five losses in a row, you're broke, you know. So, um I use it in real life in in trading when when a position backs up or, you know, or goes negative in a in a very very bad way. Um it's kind of like damage control. Um it's a circuit breaker that I put in it. Um but there is a cliff there. There is a cliff. You know, you you can only uh double down. you can't double down in, you know, indefinitely. It's a way to manage manage loss and recover losses once you go go negative. And and a lot of times in the stock market and and investing or especially something that's moving fast like cryptocurrency or options or something like that, um it can recover just as quick. So, you're trying to in essence from a layman's turn, you're buying time before you you uh you get back before the momentum of whatever you're investing in turns the tides and starts back the other way. It's it's shorter distance to getting your head above water. But it it's just there's multiple I could talk for days about stocks and stuff like that. So, it's just one technique in in your quiver of things to do. But, um that sounds like an exciting project, man. And that'll be fun because there's a lot of variables especially because got to ask are there one zero or two zeros on on the roulette wheel because that changes the the algorithm substantially just adding that extra two zero key. So you might want to ask them hey is it one zero on the roulette wheel or is it two? >> If you say American >> what's that? Since it's an American roulette wheel, it's going to be >> Yeah, but Vegas uses both of them. I think if my memory serves me correctly, I think the roulette wheel is from Egypt. Some I'm not sure though. >> It definitely is one American. They specified American roulette. So the European style layout has a single zero and the American style layout usually has a double zero. The American style ruler table with a wheel at one end is how is now used in most casinos because it has an higher house edge compared to a European layout. There's two zeros. In other words, there's a zero and then there's a that's green, right? And then there's a double zero. So, there's two places on the wheel that that have zeros. >> Yes. >> Very cool. We have a major update from Bill Monty. Good to see you here. It's been a while. Could you give us a quick intro for everyone who is um new uh in in meeting you where you're joining from and and tell us your big update? I'm so excited. Uh technical issues. Ah, >> yes. Belmont a king, we'll miss you. Will you be there for the build lab? The next build lab is in an hour and a half. Let me double check with um Discord. Hold on. Uno momento. Wait a minute. Let me just check. Yeah, we're wrapping up the session anyway. Yeah, today I've got three events. OMG. I've got um our new lab that's going to come out in the build lab. I mean, we'll be doing this one in about an hour and a half. And then this one if if you can, I I wouldn't miss this. You get to meet Krishna. Hope to see you there. Good, good to see you here today, King. Take care and so hip to see you. Thanks for joining. Bil Monty, tell us tell us tell us. Oh no, technical difficulties. Let's see one more time. >> So, is that Stripe class the same one that just happened just a second ago? Like, >> yeah. >> Okay, cool. Yeah. So when Cahoo runs it, it's like a a demo and that's going to get embedded into the project so that anytime anyone wants to do the project and needs an extra reference of a video, it's going to be there. And so his primary goal is to complete the project and explain all the steps and explain the technical concepts involved in it as well. When I do it or when the difference between that and a build lab is I mean we time block it to 2 hours. The projects are meant to be done in 2 hours. So it's also a way for us to make sure and test out whether it is actually um com you can complete it in in 2 hours and everyone is a little different. So 2 hours is a good buffer for that and then um we do it together with the community. So a little bit of a difference in in the styles of the two. Cool. >> Yeah. But Sean, if you're already done with the project, you should still join the build lab. And >> I am I will I am >> I won't disrupt things too much. >> Oh, you you never disrupt anything. I don't know what you're talking about. >> I was in trouble. >> Never crack up. maybe sharing that video to me right at the beginning of the call was trouble cuz I couldn't stop laughing >> after this. I I think I want to go watch it again. >> You hear Maximus said, ""My true deep fake. My first true deep fake."" That's funny. >> I I am saying sweet all funny like funny sounding too. Hilarious. >> Hilarious indeed. >> Like a little girls girl girls voice or something. It's crazy. >> That was all prompt. That was all prompt engineering. >> Amazing. Crazy, right? And you fed images or videos? >> Yeah. So, I got on my phone and um iPhone, you know. So, if you hold the picture for a long time, it does that little silhouette. it it it traces the person and pulls them out of the picture. So, um, so any any picture you're looking at, you know, so I just held up held it real quick and then I just my phone I sent it because it says share or I use vast.ai and spin up a a real expensive uh VRAM card. One second. One second. Sorry about that. Can you hear me? >> Yeah, I can hear you. >> Yeah, sorry. Continue. >> So So if um because you know all these images and animations are really really, you know, hardware heavy, you know, they're using high high-end graphics card. My my computer doesn't. That's what you pay for with Gemini. you know, they're using the, you know, the the H200 video cards with, you know, 130 VRAMm, you know, and and so when I go to VAST, you know, I stumbled across Vast when I was trying to, you know, spin up my own deepseek when I downloaded the whole AI agent, realized that my computer like was way out of realm of of running that thing efficiently. So vast AI, you can spin up 20,000 $40,000 worth of software for like 60 cents an hour. You rent it. You rent it. So uh No. Uh all righty then. Shane. So uh yeah. So um I forgot what what I was talking about. Shane distract me. But so um yes spinning this stuff up and creating little videos and stuff you know oh I remember you know what you could do you could use your likeness I think uh synthetic something or other already does this some company uh where you can uh copy yourself then run an N8N uh loop and then teach the class and answer questions all AI you're branding your likeness quote branding your likeness creating and then spin it up on a on a instance on a server you know with that computing power you're not paying 24 hours because you're only paying for usage right >> so you could spin up a production level integrated kind of uh answer you know it would mimic mimic you you know >> but just a concept There's Shane. Alrighty then. Yeah, do that. >> I think it was in reference to his question about if I checked out his project request. So, to answer your question, Shane, I haven't checked it out, but I see it and I will get to it. Um, yeah. I I got to say might be too much information for all of you, but the community has been like popping off and every day I just have so much to read and review and reply to and it's just so exciting. I love it. So, it's taking if it takes me a little more time, please forgive me, Shane. I will get to it. I see it though. I also want to ask Bill Monty if uh his audio is working now. He had some big news to share. >> I'm about to say, can you hear me? >> Yes. >> Okay. Yeah, for some reason it froze up on me on my desktop, so I switched over to the phone. But um yeah, first of all, for those who don't know, uh I'm was Brandon McGomery under the previous um places that we we said like Google Hangout and stuff like that. And >> yeah, >> on Discord, I'm under Bill Monty. I won't get into the full reason for well the last part of that you could probably guess why the Monty part, but the Bill part that's my high school nickname. I won't get into that too much today, but um >> feel like there's a story. Okay. Okay. >> Yeah, there's a story. But uh that's for another day. But today, I do have a major update. So, I uh interviewed recently for a cloud architect/ security officer uh position. Um, and of course this would be my first cloud position specifically that I've had cuz I'm already in cyber security, but I'll be transitioning to a more cloud focus uh sort of thing. Though it's still security of course, but um so uh the funny thing is so I interviewed for that um and I think I had crushed it and then I got the offer letter that same day. This is Friday. Um, so I am now just waiting on an official start date, which it looks like it'll probably be the beginning of February. Um, but yeah, I've managed to get me a into a actual cloud role. The official title is cloud security architect slash security officer. Um, >> so exciting. >> Congratulations. This is huge. >> Thank you. Thank you. It is. And the thing is is that from a commute standpoint, it's actually a lot worse than what I'm doing right now. But I said, ""Hey, um, and it's and the pay raise is only it's super super super modest."" Like basically the difference is probably in my transportation to be honest with you. But because it's a cloud position and that is going to help me a lot more for what I want to do in the long run than what I'm doing currently. I said to take it >> just figure it out and try and soak up as much as possible while I'm there cuz we will get to work with AWS and um Azure uh systems. So, you know, even though I'm not quote unquote as hands-on cuz I'm more like compliance and making sure that the stuff is getting done sort of thing than and maybe suggesting solutions than actually touching it hands-on on the job. I still get visibility into, you know, how that stuff works and how everything goes on. And of course, you know, my long-term goal this year for to put it out there is ultimately to try and get a cloud security engineer type of role and also cloud architect, which generally pays a bit more than engineers, but you know, for the salary that I want, engineer I think would do just fine. But, you know, um right now this will get me closer to to that. And like I said, you know, I'm working on I'm be working on getting my Azure Security Engineer Associate certification, which you know would help with this experience or I should say it'll be a great complement to the experience that I'm about to get here. And then uh my actual next thing after that is to get the three Kubernetes certifications. So the CKA that's the certified Kubernetes administrator and then the CCAT even though the CCAT's not totally necessary but from what everyone says that I be seeing they're like hey the CCAD you might as well get that right after cuz they're there's so much overlap you might as well do it. So I was like, ""Okay, yeah, I'll do that."" And then the CKS, which is the Kubernetes uh security specialist because the way one way to get paid in tech uh for those who may be new or some of you who might already be in the game is uh when you niche down, you can really uh make yourself stand out. So I decided my niche is going to be cloud security and to niche down even further is perhaps container security with Kubernetes as the forefront of that. So um yeah so there that's that's my big update. I am going to also rededicate to completing some networks next work projects. uh that DevOps uh AI series has caught my eye. So, I'm going to uh start with that. Um I'mma resume my solutions architect um track as well cuz I still have two Kubernetes projects that are long overdue to do and then the DevOps challenge and the Terraform. So, I'm going to complete all of that. But that DevOps AI is especially important cuz I have someone that I network with. I should say one of my co-workers, he has a guy that wants us to do a DevOps uh dev sec ops project with him. Uh so if I get a jump start with these networks next ones, I could pick that up that much quicker. So yeah, uh we're doing a lot of work out here, but uh you know, bit by bit, brick by brick, we uh building and we going to try to get somewhere. >> That's just amazing. Thank you so much for sharing the the big news, the big update with us. It is it's amazing. It's amazing to hear it. Congratulations. Congratulations. Welld deserved. I know how hard you've been working on it. How hard you've been working on the network projects and building that resume and really added. I remember I think it was the three tier project that you were stuck with the cloudfront. >> Yes. Yes. >> And you came in and you were resilient. you were like I am going to make sure that I learn from this and yeah and and it's amazing that you have now landed a cloud engineering role and and I also love that you have a very clear idea vision of what that trajectory is going to be look like is going to be looking like for you like what you want to achieve is very clear and you've got milestones and dates by end of this year. I want to be here. I I think it's amazing. >> Thank you guys. I appreciate it. Yeah. And that that three tier project was something and that's that's a a few other things of course to add. It's like I feel like >> it's kind of hard to stay focused cuz it's like oh I need to do this I need to do that and that that that cuz like also on top of those CS that I mentioned I also it's like well if I'm going to be an engineer I probably need to make sure I know Linux and uh Python as well at a minimum and like oh I got to learn those too. But I mean granted I'm not totally unfamiliar with either of those partially because of my educational background cuz you know I do have a degree in computer tech and we you know worked with some Linux and definitely Python. And I took a scripting course uh you know uh millions and millions of years ago but um yeah you know just all this stuff but and how it all ti learning how it all ties together and how it can make you resilient in this job market and you know both now and in the long run to have all those skills. You know, another thing I was saying, I'm sure you've told some of them something similar or other guys like, you know, looking at like job descriptions and for the type of positions you want to see. All right, what kind of skills are they looking for? And you know what I see consistently a lot is like infrastructure as code, CI/CD pipelines, you know, those are some of the gaps that I currently have that like, you know, if I fill those then it's really going to be another level to get to. So yeah, like I would say like yeah, for people who are, you know, wondering what you can do, look at job postings and see the skill set that they're looking for. And I can tell you Nex works has a good chunk of those. Uh, like I can tell you one interview that I had, I might have told y'all this before, but a cyber architect position that I interviewed for um a few months ago that uh it was a it it had it was basically AWS focus and it was like, hey, if I completed all those networks project that I meant to, I don't know if I would have got that job, but I would have had a lot of a better chance cuz they definitely asked about those CI/CD pipelines and Terraform and uh even a little bit of Kubernetes etc. So, you know, hey, you say, yeah, look at those job descriptions. And like I said, I think Nexwork is doing a pretty good job at putting out the projects that are relevant in terms of like what's going to help you build the skill set. And then just make sure that cuz another thing I would say too is this is just my own advice of course everyone has their own but is go through the projects again or at least just relook at what you did so you really understand what it is that you're doing and what it was accomplishing so you can understand how everything works. Cuz the key is with interviews. It's one thing to get past, you know, the AI systems and all that and get your resume in front of an actual human to look at it. That's just step one. But if you make it to an actual interview, those projects help in the sense of you understand how this stuff works. So even if you don't have direct on the job experience, if you can speak to it, that gives you a lot of a a much stronger chance of being able to land those type of jobs that you're aiming for. Uh if you have those skills, cuz those ski, like I said, you have those skills and you understand it, you can speak to it in an interview, it helps you a lot. So >> I'm so happy. I'm so happy to hear everything. I just I'm just >> Am I? >> Yeah. Right. >> Do you mind if I add something? >> Please do. >> Um, it's nice to meet you, Bill. Um, and congratulations on your new journey as a cloud architect. Uh, you're about to start a new journey and wish you the best on your milestones and your on your road map. And you may know this but I would like to offer some recommendations as a fellow senior cloud engineer. Um it's it's a new journey and you know if I had to get advice from and recommendations from somebody starting out I would say you know it's a different realm especially when the technical parts kind of you're going to start seeing the technical parts and sometimes as a solutions architect you kind of do more writing. So it's enjoyable and if I had to recommend some frameworks for that I would say to uh solution architect and and cosay system engineering a lot of system design and a good one is uh AWS well architect framework those would be like your bread and bed >> bread and butter yes your bread and butter to kind of build out your foundation in that realm. um it has helped me a lot and one of the biggest things especially in the the position um is ownership of the systems you built. So if I had to do any recommendations if I was starting off those would be it and and I wish you well and I I I I could see many many uh uh opportunities built upon that besides that that's hope you do well. >> Thank you. greatly appreciate that advice and well wishes. >> Thanks for sharing that, Roy. If you have any links or additional resources, feel free to drop that in the chat. I think that's such a useful thing for someone who's just getting into that role to to know and be aware of. Appreciate you giving those tips. But really, Brandon, it's um >> Oh, yeah. Sean, go ahead. >> I didn't mean to interrupt you. Um guys, I gotta go. I just wanted to say, you know, hey, congratulations, uh Bill Monty, uh before I stepped out. Uh my hats off to you, man. That makes the hair on my arm stand up. That's just rockstar. Congratulations. >> Bye, guys. >> Bye, Sean. See you soon. I'll see you at uh the other sessions later today. Take care. Yeah, thanks for um sharing that again. I would um I'll definitely convey it to the the team um how um you know your feedback about how nextbook projects being helpful for you in your journey. And I think it's um worth celebrating how you you did it. You know, this is this is the dream, right? Uh so many of our learners come into the community to the to the app because they want to be a cloud engineer and they want all the beginner projects to help them get into that space and um it's wonderful to see our very own making that dream happen. So, thank you for sharing it. >> And I and I'll just add one more thing um before we go. Uh I think the big challenge for someone like me is that having already been established in the cyber space professionally. The big challenge is was trying to build enough of a skill set and knowledge base to where you are appealing enough to recruiters and hiring managers that you don't have to take a pay cut to to get, you know, those cl cuz it would be a lot easier. It's like, hey, if I could make 20, 30, $40,000 less than what I'm making now. Oh yeah, I could easily get some, you know, junior engineer or arts physician or whatever. So to be able to get a position and not have to take a step back financially, uh that's just the biggest win that of all is that I I'mma be able to get both the experience and not have to go backwards financially. So that's that's great and of course build upon this. >> That's amazing. Yeah, I'd love to know more and and maybe we can connect again and and know what that process was like. Um, you know, how many applications did you send before you got to this one? Um, the interview process, what was the most difficult questions and and just learning about that process. You mentioned that um the date the start date is not known to you yet. Do you have an idea when? >> So they just asked me could I start on the 26 but I told them um I would prefer to start on February 2nd so I can give my current company a full two weeks. Um, so they just need to confirm and then it's likely it's looking like February 2nd um that I would be starting. >> So excited. I'm so excited for you. I can't wait. >> Thank you over >> in the future. Um, you know, like the interview process and all of that. Though I will say here two quick things as well. Um, so this this job is actually reached out to me. Um, they reached out to me for the position. Uh, it was pretty quick cuz I think they reached out Tuesday, I interviewed Friday, they offered Friday. That's that was the timeline. >> So, cuz they really needed someone. So, and the other thing is I'll give one thing away on the interview. So they they told they asked about um what a VPC was and I told them and actually the network explanation I remember in my head like that kind of connected it for me and the funny thing is fun fact they said oh you're the first person to get that right. I was like oh uh that's kind of when I knew it was like oh I probably got this. But yeah >> that's awesome. That's awesome. Amazing. Um, do you know on what basis out to you? >> Huh? >> On on what basis did they reach out to you? >> Oh, um, so it's uh it might have been LinkedIn or um one of the other sites that like my resume is on. um that it might have been uh one of the government sites uh cuz I'm in geek. So I think they probably reached out through that or at least they found me on there and then you know he saw you know cuz my resume does have like the cloud skills on there and stuff and then you know the specific um what I want to say um sector that this is in you know I do have a good amount of experience in that. So really the the main gap was the cloud part and like you know like they knew in the interview that like hey most of my experience is more like on prim systems and on prim stuff though I do have a little bit of cloud on the job in terms of like you know products and whatnot but uh even with that acknowledgement I pretty much crushed all the cloud questions anyway. So, um yeah, >> well done. Well done. Um Brandon, um in in the chat says, uh that's lightning, especially in this economy. Congratulations. Yeah, great stuff. Thanks so much for sharing. And I also I'm I'm very intrigued the fact that recruiters reached out to you. You're doing something right. whether it was the resume, the LinkedIn, whatever it is, and and I'd love to know. >> Yeah, fun fact, this is the third job in a row that I got it because someone reached out to me. So, um, yeah, it's been a while since I've gotten a job that I had to apply for. Now granted, I was applying and it it it feels impossible to get called backs even when you think on paper you're like well qualified for these jobs. But a lot of it is uh you know ghost job postings but also making sure you have a resume that can get past the ATS systems and then you know if you can do that then we got the recruiter then the actual technical interview and hopefully you know like I said in the gov tech space it's not quite as bad in terms of like oh yeah we're going to go on like five interviews for one job like it's not like at uh usually it's just maybe two interviews tops, you know, the recruiter just getting a feel for your skill set and then the actual for real technical interview and then that they make their decision from there usually. Um, but like you know, like I said, don't have to deal with the nightmare of like, oh yeah, we got an interview with the PM and the technical lead and then these other teams and then it's like, no, everyone that needs to be there one interview, but that's a whole another rant. But luckily, I don't have to deal with that. But I hear other people do where it's like, hey, you got like four look, rarely it should be one interview, but two interviews max for you to know someone is right for the job or not. But that's a different conversation for a different day. >> Yeah. Yeah. If you have clearances, you're basically set for life. Yeah. I think there's um another extra learner who's in the DC area and just struggling because he hasn't applied for the security clearances. >> Yeah. >> Yeah. That security clearance like it's I will say that is a massive massive help. is a massive massive help. Now, this job is smart and is so competitive that that alone it used to be that if you had a security clearance, a security plus, and you could just spout off some terms, you was going to get a job, right? >> It's not like that now. Uh now you actually got to be able to like speak to some stuff and do some stuff along with that. Like it is very competitive, but it but you can still make something shake. But yeah, it's it's not just, hey, security plus clearance, can you breathe and walk at the same time? Okay, cool. You're a security officer now. Like, it's not like that. But, um, you know, it but you can still make things shake if you know how to position yourself and continue to build the right skills and keep up with what's going on. >> It's a good point. >> Thanks again for sharing. Um, question. Do you prefer going uh being referred to by Bill Monty or Brandon? >> Uh, good question. Um, dang. Now I think about it. >> You guys could call me Brandon. It's fine. Um, >> okay. But even the hesitation, I feel like it's also an invitation. Like either way is fine. Bill Monty is slightly preferred. Although >> yeah, like it it really it really doesn't matter for me cuz like you know obviously like my family and my mom and professionally I'm Brandon and then I'm Bill Monty like in the Discord and amongst some of my friends some of my friends I'm still Brandon. My high school friends are more Bill. But yeah, it either one works and I'm genuinely genuinely mean that. But I guess since I was introduced here as Brandon originally, we'll just default to Brandon. But like I said, anyone in here, whichever one you want to do, it's fine. >> That might be a me thing. I think I might be the only one who recalls you as Brandon. Uh, who else? Like maybe Stephen Kenmore. Yeah, that was the crew back then. >> Yep. >> All right. I think that's such a lovely way to end this session. Wow. We talked about um the 21 and 21 the new projects that uh have come out now with the AI pinops and today I'll be doing a build lab in less than an hour. My goodness, I have to complete the previous project before that. So much fun though. I got to say the build lab yesterday I was more focused on um the community chats than the doing the project but I'll get that done and we've got a human humans of next work session. So join in to come and chat with our awesome engineer who has seen next work at its early stages and is what it is now. Amazing. And we ended the session with a lovely note from Bill Monty who just informed us that he's landed a cloud engineering role making a huge huge win for for for him for next work for just everyone who is trying to get into the cloud engineering space getting into the dream role that they want, anything is possible. So, thank you all for joining in today and I will see you soon wherever you are. I will I wish you a very great rest of the day and hopefully I will see you soon today. All right, bye guys. >> Thank you.","This summary captures the dynamic community session, focusing on new project releases, developer creativity, and a major career success story, all within the context of the ambitious **21 Projects in 21 Days** challenge.

---

##  Community Connect: Celebrating Career Wins and Launching New Projects

This session of Connect with Community was a high-energy mix of project deep dives, leaderboard updates, and a massive career celebration, underscoring the power of practical, project-based learning in the cloud and AI space.

### I. Major Career Milestone Achieved: Cloud Security Architect

The session culminated in the exciting announcement from community member Bill Monty (Brandon), who has successfully landed a role as a **Cloud Security Architect/Security Officer**.

#### Key Takeaways from Brandons Success:
*   **Strategic Career Shift:** Brandon transitioned into this cloud-focused role without taking a financial step back, proving the value of targeted skill development.
*   **Project Resilience:** He credited next work projectsspecifically recalling the challenging **three-tier project**with building the resilience needed to troubleshoot and learn complex systems.
*   **Niche Specialization:** Brandon emphasized the importance of specializing, targeting **Cloud Security Engineer** and **Container Security** (with a focus on **Kubernetes** certifications like **CKA**, **CCAD**, and **CKS**).
*   **Interview Preparation:** He highlighted that next work projects allow learners to confidently ""speak to"" technical concepts in interviews, noting that his correct explanation of **VPC** was a key factor in landing the job.
*   **Recruiter Attraction:** This was the third consecutive job Brandon secured after being directly reached out to by recruiters, emphasizing the importance of a strong professional online presence and targeted skill listing.
*   **Expert Advice:** Senior Cloud Engineer Roy recommended foundational frameworks for architects, including **AWS Well-Architected Framework** and solid **System Design** principles.

### II. FinOps Series Launch: Secure Payments with Stripe

The community introduced the newest project in the **FinOps series**: **Secure Payments with Stripe**. This project builds upon the foundational skills developed in the first project, **Ship a Landing Page with V0 and Vercel**.

#### Project Highlights:
*   **Practical E-commerce:** The project moves beyond simple website building to integrate actual payment functionality using **Stripe**, offering highly valuable, real-world experience.
*   **Target Audience:** Ideal for beginners in **Web Development** (**React/Next.js**), those interested in **AI-assisted development** (using V0 for rapid prototyping), and anyone needing to master **fast deployment workflows** (via **Vercel**).
*   **Intensity Meets Creativity:** While the projects start ""chill,"" they become more intense. The flexibility allows for immense creativity, as demonstrated by community member Sean.

### III. Community Creativity and AI Fun

Sean, a top contender on the leaderboard, showcased his work on the new Stripe project, revealing a highly imaginative (and hilarious) use of AI tools.

*   **The Kubernetes Bicycle Company:** Sean created a deep-fake promotional video for his fictional company, the ""Tandem **Kubernetes** Bicycle Company,"" featuring digital avatars of ""Maximus and McLovin.""
*   **Prompt Engineering:** This showcase demonstrated the fun side of **prompt engineering** and the ease of integrating **AI-generated content** (images and videos) into a live website build.
*   **Practical Learning:** This playful tangent illustrates the core next work philosophy: learning should be engaging and practical, even if it leads to spending time on ""silly"" but skill-building tangents.

### IV. The 21 Projects in ",2026-01-16T01:50:54.397833
NextWork,FinOps project for your resume (day 6),zYvfzaGe8ZQ,"By the end of this video, you will have a live e-commerce website deployed on the same infrastructure that companies like Tik Tok, Walmart, Nike, Stripe, and more use all across the world. This is project number one in our Phops XAI series where we will show you the skills that modern developers are using to build and manage cloud costs at scale. Companies need people who can build, deploy, and optimize without burning through money. And this is exactly what you're going to learn today. We're going to be using platforms like Vzero, Versel, GitHub, and Cursor. And in the upcoming projects, we're going to integrate in Stripe, which is a skill set that any developer would need. And we're also going to look at analytics so that you can optimize your conversion funnel. You don't need any experience. This project is free. Make sure that you are going through and adding in your answers to these questions plus screenshots cuz at the end you will get documentation that looks like this where you can change the theme and you can also share this to places like LinkedIn or the community or download it as a PDF, PNG, markdown, any of these social platforms. It just means that the work that you're doing actually gets showcased to recruiters and it's going to help you stand out and land a","**Unlock the Power of FinOps: Boost Your Resume with a Live E-commerce Website**

Take your resume to the next level by building a live e-commerce website on the same infrastructure used by top companies like **Tik Tok**, **Walmart**, **Nike**, **Stripe**, and more. In this **FinOps** project, you'll learn the skills modern developers use to build, deploy, and optimize **cloud costs** at scale. By the end of this project, you'll have a fully functional e-commerce website, showcasing your ability to manage **cloud infrastructure** without breaking the bank.

**Key Takeaways:**

* Build a live e-commerce website using platforms like **Vzero**, **Versel**, **GitHub**, and **Cursor**
* Integrate **Stripe** for payment processing, a highly sought-after skill in the industry
* Analyze and optimize your **conversion funnel** using **analytics** to maximize results
* Create a professional documentation of your project, complete with **screenshots** and **answers** to key questions
* Showcase your work on social platforms like **LinkedIn**, **GitHub**, or as a downloadable **PDF**, **PNG**, or **Markdown** file

**No Experience Necessary:**

This project is designed to be accessible to developers of all levels, with no prior experience required. By completing this project, you'll not only gain hands-on experience with **FinOps** and **cloud cost management** but also create a stunning portfolio piece that will help you stand out to recruiters and land your dream job.

**Join the Phops XAI Series:**

This project is part of the **Phops XAI series**, where you'll learn the latest skills and technologies used by modern developers to build and manage **cloud costs** at scale. Don't miss out on this opportunity to take your career to the next level and stay ahead of the curve in the ever-evolving world of **FinOps**.

Share your progress and showcase your skills on social media using hashtags like #FinOps #CloudCostManagement #EcommerceWebsite #ResumeBuilder #PhopsXAI. Join the conversation and get ready to transform your career with the power of **FinOps**!",2026-01-16T01:51:08.359274
NextWork,FinOps x AI Stripe Project (step-by-step),BQoSk78uRmY,"Companies like Shopify, OpenAI, Etsy process billions of dollars through Stripe. That is the payment infrastructure that handles over 300 compliance requirements so that you don't have to. And the thing is that so many devs get this wrong. They mishandle keys. They skip web hooking configuration. And in today's project, you are going to learn how to add in Stripe to an e-commerce website that is live deployed on Versel. And you're going to learn all about how to set it up securely. You'll also learn about serverside pricing, web hooks, and signature verification. This is a project you're going to want to add to your resume and it's going to impress recruiters. Let's just get straight into the video. If you want this entire project, head to learn.network.org. This is called the secure payments with Stripe project. And as you're going through the project, make sure to fill in these questions and screenshots because you'll get documentation that you can then add to your own LinkedIn, GitHub, or any other platform. This is the stuff that is going to help you stand out to recruiters. You need to be able to show that you can document your work and actually prove your skills. This is part two of the Phops AI series. So, I'm assuming you've already deployed your Next.js e-commerce app. If you haven't, I'd recommend following this project guide or the YouTube video that I'll put on screen now to actually go ahead and complete this project. It's vital that you get this done. It's going to teach you the basics that you need for the project that we're about to do. All right, but let's get into today's project. The first thing we need to do is actually create a Stripe account. So, I'm going to go to sign up and I'm going to create an account. I'm just going to sign up with Google to make it easy. Create account. Stripe might ask you for a business name here. You can just skip this for now. It's going to ask you for a lot of stuff. Skip this all. Don't skip this one, though. Let's click go to sandbox. And this is going to put me in a test mode where I can safely experiment without processing real money for this wizard on the side. Just go ahead and close that. And now we need to go ahead and find our API keys. So I'll go to settings right here, developers, and then I want to click manage API keys. And here you can see two keys. There's a pushable key that starts with PK test, and there's a secret key that starts with SK test. These two keys are very different. I'd kind of think about it like this. Your publishable key is like giving someone permission to submit a payment form. Whereas a secret key is like giving someone your entire bank account. So let's not do that. Now I need to store that secret key in our environment variables. And the reason we do this is because let's say we're in this scenario here where we've hardcoded our secrets. We've written code like this and it gets pushed to GitHub. Even if our repo is private, this does not mean we're safe. Let's say another team member forks our repo and then it becomes public or former employees they might still have access to our commit history. GitHub has been breached before and once a secret is in version control. It is there forever. You can't delete it later. So that's why we use environment variables. This is where you write good code like this and this secret lives in what we call aenv file. And this file never gets committed to GitHub. It is in our git ignore files. So when our code runs it reads the secret from the env file. the secret stays safe on our machine and on Versell service. It never touches GitHub. So, let's see how we actually set this up. So, I'm going to go to my Versel dashboard here and I'm going to navigate to my e-commerce web app project. Once I'm in here, I'm going to go to settings. I'm going to click on environment variables. I'm going to scroll down to key here. I'm going to call this stripe secret key. I'm going to go back to Stripe, copy my secret key, paste this as a value in VELL, and I want to keep all environments selected. This means that the key will be available in production, preview, and development deployments. And I can go ahead and hit save. Vel is just going to show a popup confirming that everything is saved. The secret is now stored in Versel servers, but it's not in my code. So, I need to do the same thing locally so that my development environment can access Stripe. So, I'm going to open up cursor just here, and I'm going to open my project. and just make this full screen. Close these up. And on the right hand side here, I'm going to make a new file. And I'm going to call this enenv.local. So I'm just going to type in stripe secret_key again space equals and then paste in my key and hit save. So now my local.env local file has the same secret and my actual code doesn't contain the secret. So when the code deploys, Versel injects the secret at runtime. And when I run this locally, Node is going to read this from my env local file. You'll know this is working when your app can make Stripe API calls without hard coding your key anywhere in your source code. One thing I did by mistake here is my secret key should actually have no spaces after the secret key and after the equal sign. Make sure you save that. If your secret key does ever get exposed, it happens. Rotate it immediately. So, just follow this project guide here and it's going to show you how to rotate your key away. But hopefully you've listened to the steps so far and you haven't done that. So the next thing we need to do here is actually build our payment flow. So I'm still in cursor right here and I'm going to open up a new chat window. You can either press control L or command L if you're on a Mac. Otherwise just hit this little toggle in the right hand corner and we're going to say run this e-commerce web app by installing the packages first and then running the app. So I can just go ahead and hit enter and it's going to go ahead and do its thing. cursor is going to go ahead and install all the dependencies that I need. So everything my app needs to run this locally. That's everything my app needs to run. So all the libraries, packages, and frameworks. And it's all done now. So it's started up my dev server. If you don't like running this in cursor, you can also just copy localhost to a browser and just hit enter. And you can also see this on a browser. So in this step here, I'm going to navigate around like I'm buying something. So I can just click through here. And the thing is now I need to create an API route. So what is an API route? Now, an API route is essentially a secure doorway or postal address within our web application. So, it's a specific endpoint that allows different parts of our application or even other services to send and receive information securely. When this user in the front end clicks buy now in their shopping cart, this sends an HTTP post request to the checkout API route. This API route acts as a secure doorway. It then communicates to our backend server to verify the product price server side. Don't worry about server side. We're going to talk about this in a little bit, but this is crucial to fraud prevention. Our back-end server then interacts with Stripe to create a checkout session. Stripe returns a unique URL to the backend server. And then finally, the users browser is redirected to the Stripe checkout page using that URL to complete the payment. So instead, this API route ensures that sensitive payment information and price validation is handled securely on the server as opposed to the users browser. So let's actually do this. We're going to go to the project guide right here, copy this in, and we're going to paste this into cursor and hit enter. So, cursor is going to install the Stripe package and generate the API route. While it's doing that, let's explain what it actually just did. Actually, before we do that, make sure you like and subscribe because it is 10:59 p.m. and I'm recording this video for you because I know this is a cool project, but please subscribe. It means a lot. All right, so this prompt that we just put in, number one, is using Stripe to create checkout sessions. And that checkout session is just Stripe's payment flow. and it contains the product info, the price, and where to redirect the payment. Number 2, three, and four explain why we need server side versus client side pricing. And this is why we have server side pricing instead of client side pricing. Let's look at client side first. So, I'm going to go to the Apple page here. Oh man, I really want a new MacBook. I don't want to pay $1,999. Like, that's kind of crazy. I'm going to select this element right here. And then I'm just going to change this to a dollar. You know what? I'm feeling generous. $1.99. Cool. The price is now $1.99. That's all I got to pay. Do you see what I just did? I just modified the price in my browser. Now, if Apple trusted this client side price, it would let me buy a MacBook for $1.99. And I would buy every single MacBook there is and sell them to you guys for $2.50 cuz I'm that generous. But do you see the problem? We're trusting the client side, which is something that we can't do. That's why server side exists. This is our backend API route that we just looked at. It is secure and it is trusted. So someone might still change the price on the client side, but the server is going to look at that MacBook Pro product one and it's going to search its own dictionary and see the real price is actually $599. The server is going to ignore the front-end price completely and process the payment for $599. In this case, I think the MacBook was like what was it? $1,999, but you get the point. The front end can request the product, but it can't control what the product actually costs. So then if we go back to the rest of the commands, number five is going to create a checkout URL to Stripe's hosted payment page, it's going to look something like that. Number six is going to use our secret key from our environment variables, which we just looked at. And then number seven is going to redirect to either a successful payment or a cancelled payment. So Stripe should be all done here. And now we need to connect this button to the API. So I'm going to actually use Curs's element select tool here. And I'm going to click this payment button. And this is going to add this context chat. And I'm going to ask cursor to implement the Stripe payment API here and hit enter. All right, awesome. It looks like it worked. I'm going to deselect this. Close this up. So now I'm just going to go through the workflow just here. Click buy now. Going to type in my email address. Pay. Oh, and it's working. You can see it's a different URL here. I'm on the Stripe hosted checkout page. And this is Stripe's checkout. So car details get entered on Stripe servers, not yours. You never touch card data, which drastically reduces your PCI compliance requirements. Basically, just a bunch of data rules that you have to follow. And I can enter in some testing here. 242. Add in any dates in the future. 29 any CVC. And I'm going to hit pay. We have a 404 page here. So, something did not go right. So, let's go ahead and say this should take me to a success page as opposed to a 404 page. Please fix this. Going to go ahead hit enter and see what happens here. That's looking a bit better, but I don't need session ID. Take out session ID from there. Cool. That's looking a lot better. But the problem here is the success page isn't really proof of any payment, right? Anyone can navigate to this link/success without actually paying. We need cryptographic proof that the payment has succeeded. And this is where the next step comes in and it is web hooks. This is what secure web hooks can be used for. So, right now in app when someone completes a payment, Stripe redirects them to our success page. But that redirect isn't proof that they've actually paid. Like anyone can type in our website/success and they're going to get that success page. So, we need Stripe to tell our server, hey, that payment actually succeeded. And that is what web hooks are for. So, when a payment completes, Stripe is going to send data to a URL that we specify. And this is done through a post request. And it's essentially how systems send data over the internet. This is going to include things like the amount, status, and the customer details. Now, our server is going to receive this notification and we can save it to our database. We can send a confirmation email and we can fulfill the product request. But there is a problem. Any evil attacker like this guy can send a post request to our server. If our web hook endpoints accept any post request claiming to be from Stripe, attackers are going to send fake payment confirmations. They don't need to hack Stripe. They just need to know our web hook URL and send their own post request with fake data saying the payment succeeded for $599. So let's see how easy that is to do. So I'm going to go back to the project guide here and copy this in. And essentially we're saying create a strike web hook endpoint at app API web hooks stripe route ts that one accepts post request with stripe event data. Two handles the checkout session complete event. Three logs the event details to the console. And four returns a 200 status on success. So I can paste this into cursor and let it do its thing. And cursor is going to generate the web hook endpoint. And the thing is the endpoint just accepts any post request. It doesn't verify that the request actually came from Stripe. So again, if I go back to the project guide here and just copy this in and I paste this command into the terminal pretending to be an attacker sending a fake web hook. I'm going to paste it in my terminal. Hit enter. And you can see that it was in fact received. And if I look at my server terminal, it says that the session is completed and the payment status is paid. You can see here that it also says that it's skipping signature verification and this is exactly what we need. So I can go to the project guide here. I'm going to copy in this command. I'm going to create a new window and I'm going to paste this in. And what cursor is doing is it's updating the web hook endpoint with signature verification. So here is what a signature verification does. So Stripe is going to sign every web hook with a secret key that only you and Stripe know. And this signature is sent in the stripe signature header to verify the signature matches the web hook body. So if the signatures match, it means the web hook is authentic. And it also means our server is going to process the event. So it'll save the data, send an email, that kind of stuff. If the signatures don't match, it indicates that there's a potentially malicious or altered web hook. And our server is going to reject this and it's going to log the event as an alert. The thing is only Stripe can generate valid signatures because only Stripe has that signing secret. So now I want to go and get this signing secret from Stripe. So this time I'm going to go to the bottom left hand corner of my screen here and click on developers and I'm going to click web hooks. From here I'm going to click add destination. And I want to search up in these events checkout session completed. So I'm going to take that and I'm also searching up payment intent. Payment failed and I'm going to click continue. Keep it on web hook endpoint. And here I'm going to call the destination name. Now for the destination name I'm going to name it payment web hook and the endpoint URL we actually need to go back to versel here. I'm going to go to deployments, click on the latest branch and copy this link address. I'll go back into the endpoint URL here. And we also want to add in an API/ web hooks/stripe. We can then go ahead and click create destination. And what we want to do is copy this to clipboard. This signing secret right here. Now we need to add this over cell. So I'm going to click back just here. I'm going to go to settings environment variables. Uh my key here I'm going to call stripe web hook secret. Paste in my whsec value. Click save. So you can see it down here. And we also need to do this in our code as we did before. So let's go to our env file here. Stripe web hook secret equals and then paste in our value. Hit save. And I'm going to tell cursor to restart my dev server here and redeploy my app. So it picks up the environment variable change. Well, cursor actually did what I was about to do, which was add in what I just did. But essentially all you need to do is go get add dot and then we'd go get commit-m add stripe web hook with signature verification and then we want to get push but for me it's all up to date. So versel is going to automatically deploy this. Let's verify that that security fix is actually working as well. So I'm going to run the same uh fake web hook command again. Paste that in. Hit enter. Guys, I'm not going to lie. It's 12:11 a.m. So, I spelled Stripe wrong and that's why it's not working. So, now I need to restart my dev server. Hit enter. Guys, I'm tired. I'm pretty tired, but this is a fun project. I can't lie. So, let's test that command out again. Hit enter. And you got invalid signature. Okay, it worked. Let's go. The signature verification would have detected that this request didn't come from Stripe and it refused to process it. So, everything is beautiful. Make sure you push all your changes. And I'm not talking about these environment variables. Obviously, those aren't getting pushed to GitHub. That's literally the whole point of them. But all the other changes that we made to our app, they should be pushed to GitHub and deployed. So now, let's test with a real payment. I need to use my live versel website for this. I'm going to click into here. And that is because my Stripe web hook is configured to hit my production URL, remember? So, I'm going to go through my deployed site. I'm going to go explore products by now. Let's check this out. maximus@nextwork.org pay. We're going to hit that Stripe page exactly like this. I'm going to put in some fake details right here. Cool. I'm going to complete the payment. And you can see that things were successful. But now I'll go into the Stripe dashboard here. I'm going to refresh. If I go to event deliveries, you can see that the checkout session was completed. We got a status 200. So, Stripe sent the payment confirmation to my server and then my server verified the signature and responded. So, the web hook is now working. Let's go. I have cryptographic proof that the payment succeeded. I am actually so happy. This is such an awesome project, guys. Make sure you're adding in screenshots as you continue. So, you actually get documentation that you can then share to LinkedIn, GitHub, or any other platform. Documenting your work is really the thing that's going to help you stand out to recruiters because it proves that you're actually doing something. Otherwise, you have nothing to show. If you enjoyed this video, make sure to like, subscribe, all of that. And I will catch you in the next one. This is project two, remember? So, there's another one coming out tomorrow. I'm going to get some sleep.","**FinOps x AI Stripe Project: A Step-by-Step Guide to Secure Payments**

In this comprehensive project, we explore the integration of **Stripe** with an e-commerce website deployed on **Versel**. The goal is to create a secure payment flow using **FinOps** and **AI** principles. We'll cover the essential steps to set up **Stripe**, configure **web hooks**, and implement **signature verification** to ensure secure transactions.

**Key Takeaways:**

1. **Secure Payment Flow**: We'll create a secure payment flow using **Stripe** and **Versel** to handle transactions.
2. **API Routes**: We'll set up an **API route** to communicate with **Stripe** and handle payment requests.
3. **Server-Side Pricing**: We'll implement **server-side pricing** to prevent fraud and ensure accurate pricing.
4. **Web Hooks**: We'll configure **web hooks** to receive payment confirmations from **Stripe** and update our database.
5. **Signature Verification**: We'll implement **signature verification** to ensure the authenticity of **web hook** requests.

**Step-by-Step Guide:**

1. **Create a Stripe Account**: Sign up for a **Stripe** account and set up a test mode to experiment with payments.
2. **Set up API Keys**: Generate **API keys** and store them securely in environment variables.
3. **Configure Web Hooks**: Set up **web hooks** to receive payment confirmations from **Stripe**.
4. **Implement Signature Verification**: Use **signature verification** to ensure the authenticity of **web hook** requests.
5. **Test and Deploy**: Test the payment flow and deploy the changes to **Versel**.

**Best Practices:**

1. **Use Environment Variables**: Store sensitive data, such as **API keys**, in environment variables to prevent exposure.
2. **Implement Server-Side Pricing**: Use **server-side pricing** to prevent fraud and ensure accurate pricing.
3. **Use Signature Verification**: Implement **signature verification** to ensure the authenticity of **web hook** requests.

**Conclusion:**

In this project, we've created a secure payment flow using **Stripe** and **Versel**. We've implemented **server-side pricing**, **web hooks**, and **signature verification** to ensure secure transactions. By following these steps and best practices, you can create a robust and secure payment system for your e-commerce website. Remember to document your work and share your progress on platforms like **LinkedIn** and **GitHub** to stand out to recruiters.

**Social Media Post Ideas:**

1. ""Just implemented a secure payment flow using **Stripe** and **Versel**! #FinOps #AI #SecurePayments""
2. ""Learn how to set up **web hooks** and **signature verification** to ensure secure transactions. #Stripe #Versel #Security""
3. ""Take your e-commerce website to the next level with a robust payment system. #Ecommerce #Payments #Security""",2026-01-16T01:51:22.713868
freeCodeCamp.org,Build Your Own Kubernetes Operators with Go and Kubebuilder  Full Course,odP153inZUo,"In this hands-on Kubernetes operator course, you'll learn how to extend Kubernetes by building your own custom operators and controllers from scratch. You'll go beyond simply using Kubernetes and start treating it as a software development kit. You'll learn how to build a realworld operator that manages AWS EC2 instances directly from Kubernetes covering everything from the internal architecture of informers and caches to advanced concepts like finalizers and item potency. Shubhamqatara developed this course. >> Now if you already know Kubernetes, you know that there are concepts and Kubernetes objects like pods, deployments, replica sets, stateful sets, services and so on so forth. But do you know that you can create an object called EC2 instance? No. Well, that's the beauty of Kubernetes because you can extend the current capabilities of Kubernetes and create something which is called an operator. So you can create an operator to control things which are outside of Kubernetes like EC2 instance which we will learn in this particular course. I'm very excited to bring you the Kubernetes operator course from scratch. This 6R plus course is brought to you by Shouhham who has 8 plus years of experience and working in Tago have trained many on open shift holds multiple certifications like GCP cloud professional and devops and this course comes as an outcome of his work at Privago for building custom operators in production. Yes, we'll build a full-fledged working operator end to end from scratch learning why it is even important, how to do and everything about cube builder and then building it end to end. I'm really really excited about this course and cannot wait for you to get started. So before we can build a custom operator for Kubernetes, we need to know what is an operator, right? And before that there is a term that is called a controller that you really need to be familiar with. Now many of you might not might know already what a controller is. It's you know um you have heard about this which is the cube controller um manager. But what does it really do? What work is it that the that the controller is is responsible for? So a controller is nothing but it is think of that as a forever running loop right think of this as a piece of software which we will be writing that is a forever running loop and if I want to write a bit of pseudo code for that it's kind of like this so you always run it and the first thing that it does is it um it observes the state of the resource whichever resource you are writing an operator for you will have a controller for that as well. So if you want to work with pods or deployment you want to work with services you want to work with config map there is a operator for all of those resources. So the first thing that it does is it keeps on observing the state of your resource. If the state is updated again for whatever reason you updated the state, maybe in your deployment you change the image, maybe in your config map you edit the data of the config map. Whatever reason that happens and if the resource is updated, the second thing that a controller manager or an operator or a controller really does is it compares the current state to the desired state. And this is where you put your business logic. This is actually where you define what to do in case there is a drift that is recognized and most importantly what not to do if there is no drift because it's very important to make your operators or at least your controllers uh ident. They have to be amputent. I cannot stress this enough. We will talk about the reconciled loop just in a minute. But this has to be important in terms of if in case your resource needs no change, there should be nothing done on Kubernetes. You should be able to run your controller as many times, but it should not result into a change if there was no change needed. And if it finds that there's a drift between the current state and the desired state, it then does an update. Or you can also say it acts on what logic you have asked it to do what to do in case there was a drift found. And then uh we close this. So it's a forever running loop that never stops and keeps on watching the API server for your resources that you are managing. Now what we are going to build is a cloud uh it's a cloud controller because what we are building will be a piece of software that actually runs on your Kubernetes environment here. Let's say this was your Kubernetes and there you say I want to make kind EC2 instance. Let's put it this way. It goes to Amazon, sees if this instance with this name is already there or not. If it is there, it does nothing. If it's not there, it creates something. So, it's kind of what we would call a cloud controller. Think about when you run on EKS, when you go to Azure Kubernetes service, it is very easy for you to change the service definition, the SVC for example in in EKS to have a load balancer. You can just say the type of services load balancer and in your EKS cluster there is a software which is working which is running that abstracts how to create a load balancer how to make your service as you know as backends of that load of that load balancer it hides away the complexity for you and that is what a cloud controller does. There may be many different controllers that cloud providers will give you in their own EC2 in their own u kubernetes distributions to make your lives easier so that you do not have to know the the nitty-g gritties of it. You just say I want a resource and then you get one and that is what a cloud controller manager would be. Now when I was talking about uh controller we spoke of this term called ident and this is something I actually want to um and you know um explore a little bit with you. So there are few things that your code should actually be doing when you write a controller when you write u the logic for what to do there are some things that can actually be uh that can actually be done. Um and the first thing is a happy path. So what is a happy path? Um you have a logic your reconcile you know this is actually also called as reconcile loop. It's here. This drives the cluster state to your desired state and this is what it reconciles and that is why cubernetes is eventually consistent. And I mean in a way that you make a change eventually which is a very short time again that's why we don't we think it's this but eventually your state is going to match the desired state that you want to do the the cluster of state is going to match the desired state. Now let's zoom in in this path a little bit where we have our um you know um case one where you have a logic your resource got updated and your reconcile function is then triggered. This is where you know this is the start of your uh loop. Let's put it this way. This is the beginning of your loop. So the first thing that you do is uh you get your object from the request. The way it works is when you update a resource in Kubernetes and there's a loop that's watching on that, there's a controller that watches on that. The controller gets a request. The controller can actually get the request that you wanted, which is the API request to the API server and it can get the object data. For example, if you updated a config map, your reconciliation loop, your reconciliation loop can actually get the YAML or the JSON of that config map. So you can verify or you can actually you know um see what has been changed or what updates has been done, what has been done by the user on that. So you can get the object from that resource and you can then observe the desired state from the spec. What you actually do is you see um you define your uh config map for example or let's say a pod. So you have a pod and then you have a dospec in which you define your containers. So you can see how many or what spec is there for a particular resource and then you can compare uh with that spec what is the actual state of the resource if they match you know if the if the number of containers in your pod are exactly what you wanted then you have to uh you have to just you know skip it you don't have to do anything and this is what the happy path is you do nothing and this is absolut absolutely important that you realize you don't have to do anything in this case. You don't have to make any API calls. You just ignore that request to your reconciliation loop because the actual state is equal to the desired state. And that's what happens when you exit your loop gracefully. Of course, I'm not saying you will stop the loop because you have to keep on listening on the request, but you will not make any changes. There's also a second thing that can happen. So uh in this step you have your function triggered. You get the actual object from the request. You see what is the spec of the object. What object is being modified and what is the actual resource of the of the actual state of the resource. And this is where it gets interesting. If the desired state is equal to the actual state that you want, you do nothing. We know about this from the previous happy loop. However, if they do not match, for example, in your deployment, the current that you have in HCD is your replica three. Let's take this example. This is nice. So let's say uh your current uh one that is there is replica equal to three and this is for a deployment which is stored in HCD. This key is stored in HCD. Now you do um a cubectl edit you know you do a deployment and then you give the name of the deployment and then you save that file. First thing that happens is that your reconciliation loop will get this request that okay because I'm watching deployment this deployment is now updated and that is where you made the change to be replica equal to five. What your current is three your desired is five. Now you say okay in your spec you will have replica equal to five. This is what you can do when I say observe the desired state. You get the actual object YAML. You get the actual object YAML and then you observe uh this desired state. So you want five replicas and you observe the actual state which is still replica equal to three. So there is now a drift. the current actually does not match the desired state and this is where your logic would actually come into the picture what to do in case your resources are not matching to what the user has actually asked to do. So there you will calculate some differences. You will probably take some actions. You will do a create, update, delete for the resource. In this case, you will create five more pods. Sorry, you would create two more boards because you wanted five. So 3 + 2 is going to be five pods, which is actual user uh requirement. And then there if your action is succeeded, you update the status field. And then you exit the loop again. This is very important you know uh every every resource in Kubernetes has a dot status. So you have a spec and then you have a status and this is how the reconciliation loop knows if it is actually matching. If for some reason you could not create the pod for whatever reason it may be you can return an error and then you can reue retry doing that action. And this is what makes Kubernetes as healing. It tries again. It tries again with a you know with a back off. You can configure this that if you were not able to do this right now maybe there was no uh let's say you could not create the pod because you did not have enough memory. Your pod would actually not be created or they actually put in the pending state. This is not a good example but let's say for whatever reason your pods could not be created. Maybe you were missing the role based access control in this name space where the pod should be created. Now it will be recued and the way it goes is it goes back to the beginning of the reconciliation loop here and then it is started again and this is what happens when I say you need to recue. Recue means you retry that action and this is what Kubernetes is about self-healing because if you give the rolebased access control to the you know to to the controller it will be able to create resources. It's not like I tried once and I couldn't do it. It keeps on uh you know uh trying again and again. You might have seen this. If you have a pod which needs a persistent volume, um it goes into pending if there is no P lab that the pod needs. But if you create one, the pod automatically gets scheduled. It gets started. You do not have to do that. And this is the beauty of the loop that can reue that you can recue for your um for your cases. And this is this is absolutely the brilliance of um self-healing in Kubernetes. Now one thing you have to be very careful is this. There's also a sad path and this is something you always always want to avoid when you are writing a custom uh controller. The things are pretty much the same. So what you do is you start your loop. You got a request that somebody updated the deployment. You see what they have made the changes to. You see if there's is actually there or not. The change there is if actually if you go to the desired state you have to absolutely do nothing. You have to do nothing. Absolutely nothing. What I mean by that is you do not have to update the resource for anything because when you update the resource let's say here you know um let me talk about that. Now this is interesting. The way it works is I'll go back to the the one where you had to make some work you calculate uh the difference and you update your resource. Now if that action is succeeded if that action is succeeded you will actually be triggering because you updated the resource. This will actually trigger the reconcile uh feature again. Kubernetes controllers they do not know what you have updated whether you updated the spec whether you updated the metadata whether you updated the status they don't know about that they just say okay the resource deployment was updated here so I will re retry my actual you know I I would rerun this from the consiliation loop and now because you updated because you created five pods now your replica is actually five and it will now say okay um I get the object the replica is five that the user wanted and now because I'm running this again the replicas have been already created the state matches I don't have to do anything you have to write your reconciliation boobs ident maybe you uh You got a request and uh your object you get the object you observe them they do not actually make need a diff they don't need any work maybe you have got the same replica was five and then your actual state was also five you do not need anything but by mistake you update the status of last sync you say okay it's just the metadata it does not change my deployment right it doesn't change my containers. It doesn't change the image I'm using in the environment variables. It doesn't change that. I'm just putting as a good person. I want to see when this was last synchronized. And you say that whenever a request comes, even if I make no changes, um I would update the the status.los sync, which actually would then trigger an API call. And you see whenever you update your resource it goes back to the beginning of the reconciliation loop and this is where you would have a forever running loop request comes in. Um okay you got the object you observe the desired state from the spec. I'll zoom in a little bit. uh there was actually no need of any changes on the resource but you by mistake you are updating the status. So Kubernetes says okay the object the controller is looking for has been updated. So it goes back up to the beginning of the loop and then you update the sync uh the last sync again. Kubernetes says I got a new update from the beginning and this loop will continue forever. your resource will keep on updating without having any you know without any stock. So this is very very important that you need to be very careful of um not making any changes if you do not require any changes. Now this is the foundation this is actually the foundation of uh how to write a operator how to write an operator. The controller is the actual logic that you have to have. Now what uh and there are a couple of things uh when you are writing an operator this is absolutely important I think this is a good thing you should read this the most important question you should be asking or your controller should be asking is if there's anything for me to do that means if the current state is equal to the desired state if not exit immediately do not do anything there should be a golden rule as well that you should follow that you should only write to the API server when the actual state differs from the desired state. Where in this case you see here you you are like okay I know that the actual state is equal to desired state. I make no calls to the API server. I do not update my resource. But by mistake you update the last sync which is again a request to the API server to modify the resource. And then the reconciliation loop sees ah there's an update. let me go back and I would restart that uh I I would rerun the loop and then it's a problem. So you always um have to make sure that you only make the changes to the resource when they differ from the desired set. And this is also what a tempotent means that you can run your loop 100 times if the cluster if the machine is already in that state you should not be doing anything. you know it doesn't break anything it doesn't change anything if the cluster state is equal to the desired state that is absolutely important to uh to be to be taken into account and this is what's interesting this is what makes these operators uh resilient which is they are stateless they don't remember what they did with your resource in the last request they they don't do that they don't remember they don't remember if the Paul replica was three or five or seven. They don't remember if you have the environment variable or not. They always always check the resources. They always their source of truth if they go to the required you know place maybe you're writing a a cloud operator they go to the cloud maybe you're writing a database operator which creates a database it goes to the database always runs the query and this is why uh these are stateless. So your container your your controller can actually be killed or the you know the node on which it was running it could be deleted it could crash the container uh the controller will go to another node it just starts from there it doesn't have to have a persistent volume to store the state it doesn't know that and this is why it can crash restart and still figure out if it needs to do something uh on a particular resource or not because that's what you have made it to do you have the logic that it always observes, it always checks the desired state in the current state and if there's anything to be done uh it does it otherwise it says cool the uh resource is already in that uh uh in that state which the user wanted me to do. Now if you talk about um uh this is about controllers but what is an operator? I think you guys might already know about operator in in a in a way because you want to write your own operator but let's just go through that quickly. Um, imagine you guys want a house. You know, you you get a house. Let's say you are living in India. And this is a very good example that I like. Let's say you are living in India. You have a house already. Maybe your parents own one. And one day you decide to move to Germany. The place is completely new to you. You have never been to Germany before. You don't speak the language as well. You don't know German. Now you need a place to stay. you need a house to stay. you call a company uh you know in this case you call a company and the company says hello sir you're moving to Germany we would make would help you make sure your move is easy and simple we have two options one we can give you a full furnished house we will give you a full furnished house and also we have another option where you can just get a simple uh unfernished I'm saying a simple house but let's say an unfernished house. You can choose whichever you want and we would be happy to give you the key when you land in Germany once you sign the forms and everything. The company also says one thing that sir while we are giving you the furnished house we also give you a helper. Now you say what is this helper? What is it going to help me with? The company says if at any point in time you break uh you know a tap, maybe your water filter is broken, maybe the floor is um you know you spill something on the on the carpet. Are you going to fix it? Maybe your bathroom uh tap is broken. Maybe you break um a window. You you never know. You don't know anyone in Germany. you will fix it yourself or you can help you can have the helper do these things for you because you don't know the nitty-g gritties of where the hardware store is, how to call someone if I lose my keys for the house. Let the helper do it for you. So the helper is actually someone who has the full knowledge of this house, who has the full knowledge of how to fix things if they goes wrong. You just have to tell the helper maybe you lost your keys, you know, just tell the helper, go get me a key. He knows where the store is. He has the logic. He has the knowledge of where the store is. He has the knowledge of where to go and in what language, how to speak to the to the person who can make you a key in German and gets you a key. If you have a broken pipe horse, he knows how to fix it. So, think of this guy. Okay, this helper as the actual operator. Now if you want to port this in um in the terms of software, think about you uh have a database which is called MySQL. Now for you uh installation of things is easy now because you have a container you can simply run it and you would be able to get your app your software but what about day- operations? What about maybe you want to do a database migration of your schema? Maybe you want to take a backup. Maybe you want to take incremental backups on a particular, you know, a schedule. That knowledge needs to be either with you or someone who can do this for you. And this is where MySQL not just gives you the database MySQL but also has an operator for you. This operator is actually a controller running internally. So this controller has all this logic. If the user asks me to uh create a database, I know how to make a database. I know how to log into the DB, I know how to create a database, I know how to do that. It knows about it. So you just have to tell what to do. In this case, this helper was the operator. And in this case, this was the MySQL database product we were actually looking for. And that makes your life a lot easier because you don't have to worry about the lower level details. Now operator has two things. One is a custom resource definition and then the other thing is a custom resource. You know how you can do cubectl get pods. You get a response. Maybe you have pods or not. It says yes I have pods or it says no pod found in the name space. But if you do cubectl get apple, it doesn't know what this resource called apple is because kubernetes has its own vocabulary. It has the API resources that it has been told to remember and those are the resources the internal ones that are native to Kubernetes like pod deployment secrets uh services you know um all these things these are resources that Kubernetes knows about. But what if you want to create your own resource which is in our case what we will do is going to be an EC2 uh instance. I could also want to create an S3 bucket. In that case, I need to expand Kubernetes's vocabulary that okay, this resource called EC2 instance. If somebody says uh it gives you a YAML which is kind uh EC2 instance, you know how to create or at least you know what that is. What to do on that? That's a different story. You know what that is. So that if somebody gives uh on this file cubectl create you can don't just tell me you don't know what is this resource you know about that now I have given you the schema of what an EC2 instance would be I have given you this custom resource definition so whatever the user gives you in this kind acceptepic because now your vocabulary has been increased and this is going to be a custom resource whenever you create uh whenever you instantiate a custom resource definition that is called a custom resource. For example, if you created a custom resource definition uh for EC2 instance when you create it and then you can do cubectl get uh EC2 instance. What you receive is an instantiation of the of the definition that is a custom resource on which very important on which your operator your controller will be acting upon. So your controller knows that on a resource type EC2 instance it has been created it has been deleted. If you create a resource called EC2 instance, it knows that on this resource there was an update which is to create the resource. The controller will create that resource for you. If you delete that, the controller will say okay on this resource which I am watching there's a delete operation performed by the user. So it goes ahead and deletes it for you. So without the controller your your custom resources are nothing. They are just kubernetes knows about it. It does not react on that. It does not acknowledge that okay I'm going to do what you want me to do because it doesn't have the knowledge. So while the CR and CRD uh you use them to tell what you want the controller with them is actually the how part of it. How do I do that? And this is what we going to be building. We will be building um a cloud controller which is for building EC2 instances on Amazon. And this is what we will be looking for. Um there's also something which you need to know. Kubernetes is not just a platform now. It is a complete operating system for um you know for people. So let's talk about how Kubernetes is actually expandable and how can you use Kubernetes as an SDK. So what's very important with Kubernetes is to look it from not just a platform where you can run your applications but rather how can you expand Kubernetes as a software development kit and what can you do with that on other platforms that's also what you can do. So the first thing that Kubernetes is so widely adopted by cloud providers by onprem for other softwares is because of its extensibility. Get me let me get a color different. So it is because of the extensibility because of these custom resources because of these operators and because of the controllers and this is what uh we just talked about. Kubernetes also have API first approach. So everything in Kubernetes has an API. Everything your pod is an API. Your service is an API. Your API server has APIs for all of these things and that makes it very easy to u write your code for and there are client libraries for this and that makes it very very easy. You have the SDKs that you can build your controllers on uh for Kubernetes. there is Go, uh, Python, there's Java, there is integration of JavaScript with Kubernetes because there are client libraries for that as well. And you can also, uh, Kubernetes has backward compatibility because it does not just delete API resources, it deprecates them first. It gives you enough time to move towards a different um, uh, uh, you know, to a different API um, version and it also versions its API. So maybe you might have seen uh pods/v1 or you might have seen network um you know um network config /v1 beta 1 beta 1. So this is the version of kubernetes um API. So it makes it very easy for you to develop new APIs without breaking the existing ones and that makes it really really simple or really helpful I would not say simple but helpful to expand your APIs and this is a plug-in everything you can have your networking you can bring your own CNI you can choose from different CNIs quite popular ones are stelium um I think yeah selium is one very popular from isalent which is a company acquired by Cisco. Uh you also have different options for storage. You have different options for runtimes and web hooks where you can intercept everything as a admission controller which could either validate your request or which can either mutate your request. I think for these web hooks we can have an entirely different course for it. they deserve their own time because I don't do justice if I just talk about there is an admission controller which can validate and uh mutate it doesn't doesn't help so probably something to look for in the future for us and this is why different cloud providers because of this extensibility of kubernetes there are different flavors and thousand plus tools that you can use on top of kubernetes so there is open shift from red hack there is suz from Rancher, Tanzu from WMware. Then there's softwares on top of that which is cubeflow K native um cube which is also quite popular nowadays and that's what makes the developers happy because they say what not how. Now if you are working in a platform engineering team uh you want your let's say you know this is a developer this developer wants a machine in Amazon he wants or she wants an EC2 instance and you manage your cloud let's say you are the cloud uh admin who will give them the EC2 instance they come to you you uh say okay I run some commands blah blah blah and this is the instance and you give them that that's okay but this is a very old approach. What you can rather let these guys do and this is what um in in internal developer platforms would actually help you with or you can build your own then you can say okay listen what if you want an EC2 instance you don't have to come to me just give me this YAML which is you know you can explain them explain it to them you can have a Helm chart around this that says I want an instance where you can say the number of instances maybe two the instance uh type which you want and then maybe the you know you can have them give the um the instance uh ID where you can then say the machine the AMI ID that you want to use a very simple thing and then maybe also the port numbers that should be open. They give you this in a YAML format and you pass this from your controller, you know, after you can have a pull request review. So after they have a pull request, they this is stored in GitHub. You have a pull request and then they get an EC2 instance. With this, they get to say what they want. They don't care about how to create resources in EC2. They don't care about BPCs. they don't care about anything and also because you have a githops workflow now you can have argo cd uh deploying these resources and then the controller takes care of creating the e2 instance everything is as a code you can have a githubs very resources um very very simply with this platform uh as as a you know as a as a product which is platform engineering all about so you can have the declarative options you can use Helm to help the lives of developers easy that they can just give you this information. You render the resource and then your controller takes care of that and and this is this is I cannot um stress it enough how how simple it makes our lives easier. Now because you can run Kubernetes not because the thing is you can run Kubernetes anywhere and the reason why you can run Kubernetes anywhere is because of the standardization. You can run this in any cloud you can run this on edge you can run this you can run AI workloads on top of that anywhere you know Kubernetes is standard because it has one pattern which is a controller pattern that rules them all. Um I would say DNS just works again it can be problematic but every pod knows where the where every other pod is. um it has its own challenges depending upon how many number of services you have in a cluster, how many pods you have in a cluster. Scalability could be another issue but for a for a cluster that you have bootstrapped, it just works fine. And then you have config management for your developers which I don't think I need to uh talk about. The single I'm trying to make here is it's not just a container orchestrator. It is a complete operating system. You want networking, it has it. You want memory management, it has that. You want compute management, CPU, uh, storage, it has that. It has disk management, it has it. So, you can actually build and package and ship your software that runs on top of Kubernetes. Uh, any sort of software that you can uh, you know, you can build and run on top of Kubernetes. It's not like you're just using Kubernetes, but you can ex expand it with all of these controllers and these um operator frameworks that we are talking about. And this is why I love Kubernetes a lot. All right. So, this was about how do you use Kubernetes as an SDK. Now, let's talk about um how do you bootstrap Kubernetes? Um how do you bootstrap a Kubernetes operator with uh with a software called Cube Builder? And this is where our journey would be beginning. So let's go on and do some hands-on on writing an operator. So before we can build our own cubernetes operator, we need a place to run this operator on and that is going to be Kubernetes. Now you can build a Kubernetes cluster in GKE. You could probably use Amazon as a managed service. You can build your own clusters with uh QBDM. Whichever way you want to do it is fine because the operator that you are building it will be built into a container image and that container image can be run on any Kubernetes cluster. In our case, we want to keep it simple. So I'm going to be building the operator and I'll be testing this operator which is going to be running on my cluster locally and create instances on Amazon which is external to the cluster just to show that you can manage infrastructure that is external to your Kubernetes environment. And this is why Kubernetes is really popular because it lets you uh use it as a SDK as an operating system of the cloud which we will also talk about in the future. So K3D is a Kubernetes distribution by Rancher which has many other distributions like K3S which is also a very simple lightweight Kubernetes distribution. It also has RE2 which is more hardened and for security if you are working in the governance um and K3D it lets you create containers or rather it lets you create Kubernetes clusters in containers. If you have kind you can use kind. If you have K3D you can use K3D. If you have a sandbox cluster somewhere, you can use that as well. The reason why I'm using this local is because it's very lightweight. It does not cost me lots of resources. It's free of course and it's very fast because it's running on my computer. So for K3D, we can install that very simply. Just go to the installation script and you can download that with either cur or you can download that with wget. I would suggest you go with the latest version. And once you have this downloaded, you can do K3D or K3D version. And I've got the latest version of K3D, which is 5.8.3. And the Kubernetes version that I would be using when I build a cluster with K3D is going to be 1.31.5. But there is a newer version of Kubernetes. What if I want to use that better? We are DevOps engineers. We are uh cloud engineers. We like to have a single source of truth for all of our applications which is why we do githops right and wouldn't it be nice if you can just version control your clusters as well uh that right now I have got one cluster which has two agents maybe I want to increase it let me put into GitHub and that is exactly what K3D allows you to do with a very simple cluster config file and this one has lots of options which you can go to K3D uh and look on the documentation. However, I I've kept it very simple. This one gives me one master. K3D allows you to create multim masteraster multi-node cluster. Again, I'm just going with one because I don't need high availability. And second, I'm going to be using two agents here, which is going to be the worker nodes. And this bit tells me the version of the Kubernetes that I want to use. And that's the one which we will be using. You also need Docker because K3D uses Docker because it creates containers in which it runs your Kubernetes cluster which runs containers and that's a whole inception going on out there. But these are the two things that that I would be using. If you have any other distribution of Kubernetes, you can very simply use that. So I've got Docker um running on my machine. I've got or stack which is actually giving me docker in in the background which is giving me a runtime in the background I would say and to talk about K3D its architecture is fairly fairly simple. So what it does is that this is how it looks like. So you have your laptop or you have your computer on which you want to create multiple Kubernetes clusters. Now as a developer I might need different clusters for different applications. I might want to promote them from dev, testing, QA just to have a pipeline going for a complete software development life cycle. That's also possible for me too. And that is where K3D shines really well. When you make a cluster in K3D, it creates a separate Docker network for all of them. So they are completely isolated from each other and they have their own tier as as you will. have their own network uh in which they would be talking to. So here you can see I've got one cluster here which is blue and there's one cluster which is green cluster A and cluster B and this is the master node and these are just robots which is our work is cuz that's where the actual work gets done and we have these docker networks created right now if you do docker network list you see the standard docker networks that are created when you install docker however when you do k3b cluster create with this config file which is our source of truth. When you do that, there's going to be a new network created which I just showed you. So we will see that just in a moment. Once this is created when you you know when you ask it to create a cluster not just it creates your cluster for you not just it sets up a gateway for you not just it creates your workers for you it also updates the cube config or rather it can help you to get the cube config and here you can see my context is automatically set to cubectl. It says you can use it like ctl cluster info. And if I do that, that's where my clusters are. That's where my cluster is running. Now if you do docker ps, you will see there are a couple of containers that are just started. And this is our infrastructure for K3D. We have got two agents which is our worker nodes. We have got one server and we also have this engineext proxy container which is there for some reason and the reason why it is there is for you to talk to your API server because you can use K3D to create multiple masters. You need to have a load balancer. So you should not be needing to set it. That's why K3D does it for you. And here it creates a container that is listening on your port on your computer's port which is 5745 and that's actually uh forwarding the traffic to 6443 of the master or in case you have multiple of the masters and that's why you see the Kubernetes control plane is running on 5745 on all the IP addresses of your computer. If you go to this port, you will be talking to Kubernetes. You will be talking to the cube API server. Now, what can you do? Every time you have a cluster, it's good to do a smoke testing. A very simple one. So, we can do cubectl get nodes. There you go. You have got one control plane, one master. You've got two agents which are ready. You can do cubectl get service. There you go. You can do cubectl get pods and some of them are code DNS which is very simple. It comes with a metric server also. It comes with traffic insert which is again uh it allows you to expose your services outside or work as an ingress if you will. Um and it has got a local part provisioner which is for storage. I talked about the metric server already. Now let's try to do some smoke tests. And if you can do cubectl create deployment or kc create deploy it's going to be creating a deployment and it's going to create a pod um k get pods and here you can see it's container creating. If I do k logs and if I can do my deployment this is a log for engineext. That is fairly fairly simple. If you had used engine x this should be nothing new. You can also expose your uh deployment. We want to check the network connectivity between our applications. If one service or one pod can talk to other application in the cluster, let's just validate that. So I could do uh I want to expose my service. I want to expose my deployment called my deployment and the port number for that would be 80. Here you can see it's a service resource in Kubernetes and it has got a cluster IP. Now you know if I want one application to talk to uh another application in my Kubernetes cluster I can use this cluster IP and that's exactly what we would be doing. What we would be doing is here okay um so here we have a pod in our new cluster for which we just created a service. I want to test the networking in my K3D cluster. So I would create a new pod. I would try to curl this service and I should get a response from this pod and I should be able to curl it because it is HTTP cuz I know I just ran an enginex server and this should work because it is a single cluster. You know you cannot by default expose your service IP addresses outside the cluster. However, inside it should work fine. And that is where we can use our trusty curl image. This lets you just do a curl to any other IP address or host name. And that's where we can do k run. I want to use this is my container. I want to create a curl container with the name of curl. This is my image. And I want to connect on the IP address of my service. That's that. Let's look at the pod. This pod is container creating and it's completed already. Crash loop back off. That's fine. Let's check what happened. And if I do logs for curl, it wasn't my crash loop back off. It just started, exited, started, exited, and it's like what is going on? It was not a chron job that runs till completion. Um but you can see here this is the response that you get from the service uh which is engine X and that tells me that my cluster is ready for connection. My cluster is ready for me to build applications and also uh you can probably go to um you can also check from your cluster if you have external connectivity because we would be talking to Amazon. Might as well check that. So we can do k run curl or let's say Google and I could do httpswww.google.com. Do I have a pod now? Uh, Google container creating and let's say and that looks Google to me. Um, looks fine, right? So, we have connectivity between our applications and we also have connectivity now uh to external environments and this is going to be the foundation on which we will be building our application. Um you also would be needing to have go on your computer which we talked about. You need docker git the standard developer tools. So um that's it. This will be our uh our setup. Now I think we should talk about what are you going to be really building in this course and what is a reconciliation loop? How does kubernetes know what you want it to do? How does the controller or what is even a controller in the first place? How do they know that I want to do something? The user has asked me to do something and uh I should do that. How do they know that the state of the cluster is not matching the state of the you know uh desired uh versus current state? How do they know about it? So let's get uh let's let's learn that now. So if you want to know how to build an operator, the best thing to use is an already available framework which is called cube builder. There are also some other frameworks that helps you to build cubernetes operators like operator SDK. However, um cube builder is also one of the very famous operator frameworks that allows you to write your own controllers for kubernetes. This is for people who are using Kubernetes and they want to develop a very indepth uh knowledge of how Kubernetes reacts on certain resources, how the operator loop functions, how is it identities, how would you know um you actually compare the state to the desired state. What is a web hook? How does it work? How do you implement versioning with a cubernetes operator? That's all which is very very inbuilt and which is very simple with cube builder. So this allows you to have a starting point without spending so much time on what is going to be my project structure. How would I you know uh structure my code? How would I structure my test cases? Um how do I generate my um metrics? How do I add a locking into my soft into my controllers? Am I going to have a leader election? How do I implement a leader election? How do I expose a metrics? on what port do I export the metrics? All of that is taken care by your builder. What it does is is it allows you to have a directory structure in which it has the boilerplate code for building your Kubernetes operators already there thousands of lines. Uh instead of you to have to write it allows you to focus on the business logic. It allows you to focus on what is going to be your specification of the custom resources. It allows you to tell what to do in order to you know how to react in case there is a change in those custom resources. That's what it allows you to do instead of uh looking at how do I start with an operator in the in the first place. It also lets you generate the role based access control. It lets you generate the cube um um what's it called? It lets you generate the the customize resources as well in case you want to deploy your operator into different places. It also lets you wrap your operator into a Helm chart for its own deployment. So that um it can be used in any cluster regardless of whether you are running on cloud, whether you are running on prem on wherever you are running. It allows you to version control your APIs as well. So for us, let's get started with that. And the first thing you can do is you can quickly install um install cube builder. Let's go there and installation and setup or maybe I look on GitHub and there should be some releases um that you can you can download. Um we can also install uh using um the installation book. There are many different ways of installing it. Either you can download it from the releases which uh which one uh is working for you. I'm using a Mac. So I have got an ARM 64 because I'm using a Mac and that's my architecture. And once it's downloaded, I think you can also use Buu. I'm not sure if you can but um how can I install that but as I show you there you go. So you can install Cube Builder using a very simple third command. Now first thing that Cube Builder needs or what you do with Cube Builder is you create a project. Now a project, think of that project as a collection of your APIs that you will be building and it's a simple directory structure that allows you to initialize um you know um your your APIs and let's do that now. So first thing we will do I have cube builder cube builder version already uh which is which is available 4.5.1. I think the latest one is 4.7.1. I'm not too far behind but that's okay. So I've got the cube builder and the first thing we will be doing is we will create a project where we will be hosting or we will be you know um building our API. The first thing cube builder uh in it and here is the important thing when you are building your custom operator um let's say you are working in a company called uh example um you want to build your uh custom resources in a certain domain which makes it easy for Kubernetes to know where this operator is coming from. If you do cubectl API resources and if I do less here you can see every resource in Kubernetes is actually its own identifiable API um every resource that we see is an identifiable API c um API resource for example if I uh look at let's say um AI services here for example hub.tra.io io/me1pha 1. We will talk about what the group version kind is. But uh just to just for you to know uh you can define the domain in which your API should be uh declared in which your API should be built. So for example, I could say uh Q builder uh in it I want to be building things related to cloud and let's say I work with um um Netflix for example and my products should be under the domain of netflix.com in this case I'm using cloud.com and the repository in which my um in which my code would be hosted just as a project descript encryption. What it does is it writes the customized manifests for you. So you can have it deployed in different clusters based on your requirements. It writes a lot of scaffolding code for you. And what it does is is it creates you a directory structure. It writes you a docker file which you can use to build your operator into a uh into a deployable image. It creates you a make file that uh you can use to generate your custom resource definitions. Maybe I open this in VS Code. That would make more sense. Um maybe I open this here in cursor. That would make more sense. So it gives you a make file that lets you generate your um your you know uh your RPA lets generate your custom resources, custom resource definitions. It helps you deploy those into a cluster and install them from the cluster. If you are doing a local testing, this make file is really really um helpful. And this is where is going to be your project. Uh this is the project uh information on where uh what is the name of the project? What is the domain under which your project uh is is defined and um and what is the version of uh of of the cube builder project that you are using. Apart from that and this was the docker file that we were talking about. Apart from that it gives you this cmd directory. Now it has already created a lot of files and a lot of folders for you. So let's quickly go through that. The first cmd main.go is actually the entry point of your operator of your controller. So this already is done for you. you would have to worry about what libraries in Go I want to import in case I want to build a custom operator. Whenever I say operator um when I'm talk I'm talking about controller because that is a loop that actually uh does a job for us. So you would be thinking what library am I um supposed to be uh you know importing for example take this the client go and the uh client o package. So this O package is actually the one that allows you to talk to um Kubernetes. It it imports all the Kubernetes client O plugins in case you were using GCP, Azure, uh you want to talk to the clusters. It lets you get the cube config and this is the package that lets you work with. You also have a package for uh importing the Kubernetes API machinery. We will talk about API machinery in a in a bit. uh this lets you define uh do the runtimes that are needed to define a cubidity schema. How do you declare a health endpoint? How do you do logging for your operator? It let you create a lot of codebase and this is the main go which is the main file from which you declare your um your code. This is the entry point for your code. We will talk about that when we um when we write it. You also have a lot of config folders where you define u how are you going to uh be working with your it has some defaults for kubernetes like your services like your customized files. It has customization that lets you deploy your operator to different uh clusters and name spaces. It has the customization for your manager which lets you create a deployment and the name space in which you want it to be deployed. It's a fairly straightforward customization file. It lets you also create role bases uh access control. It lets you create cluster roles, cluster role bindings. Um so it easier for you to be running your operators. Otherwise, if you are managing, let's say you write an operator which listens on a resource called um EC2 instance, but it doesn't have the permission to uh to to be uh you know um listing EC2 instance in a in a namespace. You will not be able to manage those resources in that name space. So without you worrying about how does my role based access control would look like it lets you create a lot of um boilerplate code along with it lets you create the rolebased access control as well uh for you it also gives you end toend testing so you don't have to write your own testing fees it lets you help uh it helps you with that as well and the one thing that is uh interesting with that which I was looking for is the where did that So where is my cmd config hack? I simply I'm missing Oh yes because yeah so this is just the project resource. This is just the project uh as a boiler plate that cube builder allows you to do. The second thing we can do with cube builder. The next thing we can do with cube builder is to actually create an API. And this part is amazing. This is going to be our resource that we this is going to be our custom resource that we will be creating. So what we have just done is what you have just done now is we declared a project called cloud.com. Now with cloud you have many resources to manage. You might have uh things like compute to manage. You might have things like um storage to manage. You might have things like network to manage things in compute. Could be uh your um let's say um EC2 instances you know it could be your AMI in images for example they could be your security groups as well. In storage it could be a EBS uh EBS module. It could be an S3 bucket that you want to manage. Uh in network, you might want to manage a VPC. You want to uh manage a firewall rule perhaps. So the thing that I'm trying to say is you can create multiple APIs in a single project in a single domain and this is what we are going to be doing. we will be building our own API which is going to be in the compute subdomain and it's going to be our EC2 resource. So that is what cube builder allows us to do uh is to create our own little API. So let's do that. I would do cube builder create uh here we go cube builder create API. The group is going to be compute uh and kind is going to be EC2 instance. I want to create the resource. Yes. So this has created the custom resource and the custom resource definitions for me. Uh it has written them on the disk. And yes, I want you to create the controller as well. So it downloads um many different go uh go packages. It also creates a directory called API/v1. And this is absolutely uh important. This is the API the version of our API and we are building a file uh we we building a resource called EC2 types and that is where we define our EC2 types.code. Um now once we talk about um now once we talk about the uh the EC2 type.go we can take a look at that how does it look like and this is where the actual business logic would go for us. This is where the actual specification of our API would look like. Now before you can build your own Kubernetes cluster, I'm sorry, before you can build your own operator for EC2, let's let's see what would this actually look like. You know how you going to use the YAML for that? So if I give uh EC2 operatory, I would probably say um kind is an EC2 operator. Um meta, it would have some metadata. I would give it a name and name would be um my instance and then uh name space would look like uh default um API version. It's defined in compute.domain.com. Um, this is a version one of our EC2 operator API. And then I would have two things. So, every resource you have would have a spec or almost all of them. Uh, and then they would have a status field. And this is something which is very very important. When you are writing a custom resource, you have to define what the resource is going to look like. What is going to be things in the spec of your resource and what is going to be in the status of your resource. And this is what um the the file in API v1 EC2 instances.go helps us to do this. It lets us declare our given um spec for the resource that we are trying. Um, for example, my spec would have um um AMI ID and this is going to be the my dummy AMI ID and I would have a key or I would have an SSH key. This is going to be my key pair that I want to use on Amazon. Uh I would have a instance uh let's say I would have a type. So maybe T3 micro I want to have. And then you could have a storage and you would have uh in storage you would then say um I want a standard disc. Maybe you could say I want a a persistence or you could say fast disk which translates to one of the faster block devices in Kubernetes because you want see you all you want to do is you make you're making the developer life easy. you're abstracting the actual details um from the developers. So they can say okay I could go for a standard disk of size maybe 10 gigs and fast would be of size of 50 gigs that is that is the data that I need and this would be one of the minimum things you can use for your cubernetes cluster and with this spec that you're giving every resource has a spec and that is defined for kubernetes it is defined as at a strruct in collab. So if I uh look at this DC2 operator, I let's say we just keep this simple. We're going to keep these three AMI ID, SSH key, and type. Um this is going to be my things that I want to use and all. Let me just copy the um let me just comment this out. Where did that go? There we go. So I define the spec. Now this is the spec for my uh Kubernetes uh for my operator and I'm going to say my EC2 instance spec will contain an AMI ID. It will contain the SSH key and also the type of the it will contain the type uh of the instance that I want to be using. Now this is where uh it's very important for you to give these JSON tags because when you give a request to Kubernetes about a kind of EC2 instance it needs to marshall your request. It needs to understand what is this key uh and what to do with that is this key is AMI ID this key is SSH key this key is type. So these uh JSON tags are absolutely required for serialization so that Kubernetes can know this field relates to a certain um required um key for example. Then you can also have the status for your EC2 instance. Maybe you want to give out uh things like in in this one you might want to give uh the space as probably it's running if your EC2 instance is running or not. Maybe you want to give out things like um public IP and that's going to be a 1.23. And this is what you will be putting in the status field. So I would say um if I look in here you see to operator I want to have phase um I want to have phase which is going to be a string this is the type of string and I want to have uh let's say I want to have the instance ID as well and I can just simply go for a public IP. So these three things are which I want to um be be having. Now this is very important when you are using when you are building resources like this an AI editor would really help you uh like you can see I'm using cursor uh this really helps you to speed up your development again you are the one who's doing the thinking you are the one who is coming up with the spec you are the one who is coming up with um you know what what should you be showing in the status however it helps you as a as a very good helper Now you got the spec, you got the status because these two things are absolutely important to be um to be in a resource. Now how would your overall resource look like? The instance the EC2 instance would have um the type metadata and object metadata. So when you see any Kubernetes resource this kind and API version this is actually coming from the type meta. So this meta v1 is actually you can see this is a package in kubernetes. This defines the metadata of any kubernetes resource. This go package defines the metadata of any uh resource and has two type of uh you know it has two strcts there. So the kind and API version that we see on all the cubernetes resources it is actually defined in a strct in Kubernetes called type meta. And this is what the EC2 instance would look like. It would have some type meta. So you can see here on if I copy this probably this would make more sense. Let me just copy that all the way here. Uh and this would be there you go. So let's comment that out. Now this is a type of EC2 instance which is the kind of a EC2 instance. So I got that. There we go. So the first thing this kind has is the API version and the kind. The first thing the resource has is the API version and the kind. And these two things are defined by the type meta. And then we have the metadata of the object itself and that's defined by the object meta which contains the name of the object which contains generated name of the object the name space the UID the resource version the creation timestamp every every object would have these two um you know struts declared inside of that which defines what object it is and second which defines what is the object's metadata and then You have the spec where you have defined this spec and then you have the status which defines the status of the resource and this is how an API is created. This is how you declare what resources are going to be in your API. Now I don't have to tell my developers that guys you need to raise me a ticket so I can create you a resource in Amazon. Oh, you wanted 10 gigs. I probably gave you 15 gigs. Maybe I did not hear that correctly. Let me delete and recreate that or resize it. You do not have to do that. If I just give this to my developer, it is so much easier for them. Maybe I can have them a simple UI that lets them declare the name of the instance, the, you know, the count of the instance, what storage they want. It automatically creates me this manifest. And because I already have a Kubernetes operator and a you know a controller listening on top of that, it is very easy for me to track every request that a developer is making for these uh instances because um they are all they can be put into a version control system. They can be put into GitHub and you can use our code CD that makes developers life so easy. They do not need to know about what is a fast storage. They don't need to worry about what is a standard storage. Of course, they need to know the benchmarking of it but they don't need to know it is a persistent disk. They don't need to know the different type of stoages uh Amazon has to offer. It is offloading from them and that is what it makes it very very simple. Now things that you see here um these ones plus Q builder object root true. So these ones are called cube builder markers and they are there for code generation. They are there for custom resource definition generations for you. For example, this one says this is actually a Kubernetes resource. So somebody could say cubectl get EC2 instance. Somebody could say for example here is where somebody could say cubecdl get instance list and that is going to be uh what is returned this defines it also has a sub resource called status which we are defining here above. So this is what cube builder helps you with and in the end we are registering our EC2 instance and EC2 instance list with the cubernetes schema. this function uh it uses the resources that we just created. It gets the APIs that we just declared and it initi registers that with the Kubernetes schema which is actually this function comes from a file called group version_info.co. Now this one it's a very simple file. It uses the Kubernetes schema runtime package uh from API machinery and the controller runtime. What these packages allow you to do is they let you declare your uh they let you declare your APIs and the kind to Kubernetes and here you are saying that you have a group version. So you're declaring a schema group version. The group is called compute.cloud.com again. So you could say your domain uh domain was actually uh cloud.com and then your group was uh compute uh and then uh your compute.cloud.com cloud.com and then your version is v1 and then your kind is e2 instance group and this is how every resource in kubernetes think of that as a URL every object on the web has its own unique identifiable um URL for example um think of that as kubernetes every resource is declared in a group it has a version and it has a kind. Every resource does that. Every resource has it. Pod service. If I do that, maybe I could do kubectl explain service. You can see here it kind is called service. Its version is v1. If you do not see the group, that's because it is in the core group of kubernetes, which is uh which is which doesn't have a name, but it's called the core group. So is the same for pod. Uh if you go ahead um here you can see pod is v1. So this is why you now understand when you write kind we are pod API version v1 you are telling kubernetes that this yaml that I'm giving you it is a resource of kind pod which is declared in this group and I want the version v1 for this resource. Every resource have a group version kind and this code is actually adding your declared schema and it is adding your um declared group into Kubernetes. So it's loading your resource YAM your actual custom resource declaration into Kubernetes. So when you give it a YAML of EC2 instance it knows what spec this resource has. What is the AMI ID? what is going to be um the phase that is running what is going to be the uh the public IP that I'm going to be returning so it knows what is your spec and status that is what we are doing here we create a schema builder so that we can add our own schema and then we have um this this add to schema um it it does add the type in your group version to kubernetes and that's where the magic actually happens this is where you declare what is going to in your a in your resources. Um once you have that then you can also uh look into another directory that it has created for you called the internal controller and that is where the reconciliation logic happens. That is where you get the reconciliation logic uh of what to do. So this one is about custom resource but what to do on top of that custom resource? What do I do with that? That's given in the controller um package in the internal /controller directory and there's a file called your API named_controller.go. What this does is it creates its own package and it then creates your um you know it creates a reconiler. In this reconiler strct it is having two um it imports two uh interfaces. one is the client which gives you the actual Kubernetes client that you can use to talk to Kubernetes clusters and then there's a there's a schema that we can then use to convert between the YAML that you are giving and what Kubernetes knows about you know what is declared in Kubernetes um resources then you have some custom markers for for rolebased access control and this is where the actual reconcile dilation loop happens. This is the one uh this was the actual logic that makes sure your cluster state is equal to the desired state. That's the one that makes sure your cluster state would be um it reacts on the cluster state and looks on the desired state and say this is where your logic will go. This is the heart of your controller. This is the heart of your uh of what you are writing what you want to do with that and then you return a result and an error. Now we will talk about um these two things as well. I'm just running you through the code when we write our own as an example then we will uh we will look into this. Once you have the reconciliation logic, it is actually adding um it's adding uh the controller with the controller manager. So this setup with manager, it uses the controller manager to add your controller too. I think it makes sense if we talk about the architecture a little bit of cube builder and that would be so much helpful. So if I go to architecture, this is the one that will make so much sense. what Cube Builder allows us to do. Oh, wait a minute. Okay, so the when you run, let me go here. When you run uh maybe a little bit bigger would help. Let's say here. When you run a Kubernetes um controller, the first thing that it runs is it runs the main.go program. If you remember, this is from the cmd/main.go which is the file here. It starts with the cmd uh main.go file. So the main go file is the one which is responsible when you build your operator into a binary. Here's a main function that's the entry point of of the operator. So let's take a look at its main file from the beginning. It's part of the main package and it does import quite a few of um inbuilt packages from Golang. However, for it to really be working as an operator, there are many more packages that are imported um and that's from the Kubernetes itself. So let's take a look on those packages. The first one that we see here, this is the O package. And this lets your operator uh use the exec entry point plugins or um you know uh talk to your EKS clusters, talk to your GKE uh cluster API server or using the OIBC if in case you're using for authentication. This one's responsible for making sure that your operators can use the cube config or the exec entry points and they can talk to your cubitus cluster. The runtime package from the API machinery is responsible uh to kind of you know you understand YAML but Kubernetes does not understand YAML. It understands objects which are ghost trucks you know in example. So this one defines schema. This one's define objects that can help you to convert your YAML into Kubernetes understandable constructs. Kubernetes understandable objects. And when you do um cubectl get pods, the YAML that you get is actually converted from the pod object in Kubernetes by using the runtime package. We also have in the API machinery util package and uh this would be looking like it's the same package again but this one's defined in pkg runtime in the API machinery and this one's defined in the util uh as runtime and this one is more like a utility function that helps your operator be stable in case there was a panic which is kind of like a fatal error that your operator got. So instead of completely crashing the process, this lets you log that particular panic and still uh complet still continuing with the with the operator process so it doesn't just completely crash onto you. We then have uh the client go package which is again uh this is the I think the SDK for go for kubernetes and here we are calling the schema or scheme package and this one lets you register your APIs that you have defined the custom resources. It also lets you define the pod services the core constructs of Kubernetes um with your operator or rather think of it this way that it gives your operator the knowledge of the predefined Kubernetes resources like pod deployment uh secret services and also it lets your operator register the EC2 operator um custom resource that we are creating. We also have the controller runtime package and this one right here is the secret source which is responsible to have you or to work with a manager that can help you with clients caches and the leader election. This one, this controller runtime is the one that is responsible that gives you the tools to construct the controllers that can listen on changes on your custom resources and then uh you know they can uh handle the caches, they can handle the clients to talk to the API server uh and eventually um if in case you want to have early election or not uh that also is done by the controller runtime. So if I want to talk about a little bit of the architecture of how this um controller would look like. So we would have the process which is again started from the main.go and this main.go would have a manager. Again you will see this as coming ahead. But here's where a manager is the one that manages two things. one, it has a client and this is used to communicate to the cube API server and it also handles the caches of your requested or um the the the custom resource that was updated. Imagine this, you want to write an operator that reacts on a change uh to the EC2 operator object and that's where the EC2 operator object YAML or the spec will be stored. We're going to talk about the cache much more in the in the future in the video. Not right now. It doesn't make more sense. However, um for me to explain uh the manager, it does have the client which is used to talk to the API server. Then we have the cache. And here's where the interesting thing comes into the picture. This is what we are writing right now. Or rather this green bit. This green bit right here is our user provider logic which is what we are using in the reconcile function. This controller is responsible for reacting on the changes and eventually running the reconiler which is our logic that tells what to do if in case the EC2 operator object was changed or you know um whatever change you made to that this is where it's going to be um this is the logic which is going to be uh running. You can also have in the manager in your controller you can also have a web hook. This is kind of like the similar um validating web hook and mutating web hooks. If in case you want your operator to also uh serve those web hooks, it's possible to do so. Now we also have couple of um we also have couple of packages for the certificate watches. This is the one which is responsible um when you are working with uh let me rather draw it. This will make more sense when you are using things like C or let's say you have um uh admission control admission web hook in your operator you have a mutating web hook. Here you have a mutating web hook and your Kubernetes API server. You register this web hook with the API server and this can then talk to this mutating web hook. The API server will simply ignore or will not talk to your web hooks. You know, I'm not going to explain the mutating web hooks or validating web hooks because this is not a part of this course. Um, it's something there are very good documentations that you can read about. However, when your API server talks to any of the web hooks, whether it is mutating or whether it is validating, uh it has to have a valid certificate. It does not talk over HTTP. You have to have a valid certificate. And a lot of times you would be using the cert manager to issue your certificates to this uh service your your controller that is hosting the mutating web hook. Now if in case the search manager uh again it's used to uh issue certificates for your web hook and every 90 days I think by default it will be rotating your certificates and in this case if your certificate has changed maybe you are storing that certificate into a secret then it is given into the pod. Um however if this certificate is changed you will need to restart your controller. You will need to restart your controller pod. So eventually the new certificate is loaded and the next HTTP request uses the new certificate which is renewed by the search manager. This offers a downtime and to fix this we have theert watcher um package. This one creates a watcher for the change certificates and it reloads them on the fly without you to have to restart your controller package. So you don't have any downtime uh in case you are updating your certificates in case you updated or search manager did an update for your certificates. We also have the health package what lets you uh expose the the you know the livveness probes and the readiness probes that you can use for your operator. This exposes the health and the readiness endpoint probes which you can use in your deployment when you are deploying this operator and you can say uh check at this endpoint every now and then. Uh it's a similar uh it's a standard Kubernetes livess and readiness probe. We also have the zap package which is mostly used for logging. We then have filters package in the metrics package here. And this one let's uh I think this makes sense for me to first talk about the metrics here and then we talk about the filters. See when you are writing your operator with cube builder it doesn't just let you focus on the reconciler. I mean this is what your business logic is. That's what you are uh supposed to be writing. However, with cube builder, your operator which is running in a pod, it by default exposes an endpoint called matrix. And this might be looking sim familiar to you. Um because this is something which we use a lot in Prometheus. When you are writing a Prometheus service monitor or when you are writing a scrape config, you give three things to the Prometheus server. the IP or the service name, you give the port number of the scrape config and then you also define uh the you know the path the scraping path. This same you can use uh with your operator cube builder. When you are building an operator, cube builder exposes the metrics endpoint and this it it exposes couple of Prometheus readable metrics like what is the success rate of your operator? How many times the reconciler has executed? How many times it event it resulted into an error? How many times it resulted into a success? So it's not uh it doesn't give you an idea of how many EC2 instances have you created but rather this is more on the metrics of the operator itself and then if in case you want to maybe you you have a requirement that my operator can create EC2 instances but I also want to know how many it has created successfully. So you know you can also expose your metrics you can instrument your code with Prometheus uh go packages and as soon as you were able to create a VM you know uh on on Amazon we'll look into the code in the future uh in the in the further parts of the video uh you can then increment your uh AWS instance count uh to one because you were able to create just one more um instance and then you can expose this to the metric endpoint. The thing that I'm trying to explain here is it's already done for you by cube builder and by default there is no username or password. It is open to everyone and then you can use Prometheus with a scrape config to scrape this metrics the operator related metrics uh into Prometheus and show that onto Grafana. However um you can also then use this filters uh package. This lets you define some sort of authentication that this metrics endpoint is not publicly. It it should not be publicly accessible. I only want to um I I only want to allow someone who has this username and password. Uh I want to have some sort of authentication on this matrix endpoints. And these are the this filters package provides us these functions where we can use um these authentication gate um gated authentications for our metrics. We then have the web hook. Again, this is the package which is responsible for you to create these validating web hooks, mutating web hooks. There are many many videos available. Uh we also did a live stream on cube simplify of creating your own validating web hook. You can definitely take a look at that. I'll put the link of that in the description. And uh this one helps you declare your validating and mutating web hooks. These are core heart of your operator. You know without these packages it without cube builder using these packages it would be very very difficult for you to build an operator. So cube builder is really good in terms of scaffolding your project. When I say scaffolding it means it is it gives you a very good blueprint. It gives you a lot of boilerplate code which again you can uh refactor but to begin with you only focus on your reconciler logic and that for me it's amazing. Now here's where the repository where my code is going to be in the API v1 and this is where I am uh calling my custom resource definition which I declared. You remember we had API then v1 and then we had the EC2 instance right here. This was our spec of the EC2 instance. That's what we are calling in uh in the the main.go. So I am calling my um my v1 with the name of compute v1 and then I'm also calling my actual controller logic which has the reconiler or this is where my reconiler logic will be in the future. Now coming forward we have couple of variables. This setup log is fairly simple. This sets up a logger for our um you know for our controller and the scheme that you see here. Think of this as a phone book. This is an instantiation of the new scheme function. The scheme is acting as a phone book. It is acting as a registry where you will write all of your objects that you want Kubernetes to know about or rather your operator to know about. And that's what we do here in the function in it. We use the util runtime which is available here. for this runtime package. And here's where we have a must function. So what this does is in case this must function returned an error um in case this must function you know was not able to register if there was a panic the program will stop right here because your operator is completely useless. um yet your operator is completely useless if it doesn't know about the core uh API types like pod, deployment or rather also your own EC2 instance. So we register the default um core u we register the default API types and you can look at that using um cubectl. Let me increase the font a little bit. We can do cube ctl uh API resources here. You see? So think of the phone book which is our scheme. We are adding all of this um to our phone book. So we are telling our operator this is what we have available uh all of this is what we have available in our uh AP in our cubernetes cluster and then we also add our own default u our own custom resources which is what we are calling from the API B1. So essentially we are telling Kubernetes that the scheme that we declared over here it's an empty book and in that empty book using the add to scheme function which is here given uh to us by the client go scheme we add the built-in types so our operator knows what built-in types are available into Kubernetes and also we add our custom type which is the EC2 instance and then our uh registry or the scheme is a complete catalog And that's what our operator would be able to use. Now, now here's the main function. This is where everything starts for any go program. And we are defining a couple of variables. For example, I want to define the metric address um on which IP of my port the metric would be listening to. And once we have defined these variables, we also define some flags from the command line uh when you are running your your you know when when you build this with go build and when you run this binary you can give these uh command lines as metrics address probe address you can define leader election and all that. So we define the IP address on which our metrics should be sobbed. We declare some variables which is the path for our metric certificates because um just like web hooks can be served over a certificate we can declare that our metrics also is declare is you know um accessible over HTTP or it needs a TLS config as well and that's what we can define with these variables what is the path of our certificate what is the name of the certificate and the key we want to use for our metrics. The same goes for our web hook. Now there's a very good fun there's a very good um concept that operators can help you with or rather when you are running distributed systems like HCD or especially when you talk about your cube uh controller manager see that is also a controller what we are writing it has many it's a collection of multiple controllers but this runs as three different pods in your cluster or rather It runs each one on the master in your cluster. The thing is when you are writing a controller uh it is very important of how these controllers are running in parallel and do they all make changes or not. For instance, uh take if I was running two copies of my EC2 controller. So this is one controller and this is another controller and there was an update um which lets me create uh an instance you know uh I did an update I created an object of EC2 instance kind there was update and this update was seen by both of my uh controllers controller number one and controller number two they both are going to go and create me an EC2 instance and this is not What I want I do want high availability but it should be active passive. There should be one leader. There could be multiple replicas for high availability but only one at one time should be running. And this is what leader election u you know um uh is something that you can use and cube builder makes it very easy for you uh that it allows you to uh declare the leader election with a simple boolean. So in that case this is also running. This is also running but this is a leader. So if in case an update statement or an event comes from the API server only this one is seeing it and only one instance is created which is what we want to do. The other one is there but it's not the leader. If the leader is no longer running or or automatically it's going to become the leader and this will be then serving your requests for the EC2 instance custom resource changes. This is what uh leader election means and then you can enable if in case you want to have reader election and you can run your operator into high availability. We define the probe address on which your uh health probes are available. So you remember this this package which is the health where you declare your health's endpoint and the ready endpoint on what port number uh they are exposed by default the port number I think is 8081 here which is the health probe bind address the command line flag and this is the variable that is going to be responsible for it. Do you want to use secure metrics or not? And this this variable secure metrics and metricsert paths uh name and key they are related because you can say I want my metrics to be exposed over h over TLS and if you say that you want them to be over TLS then you can define your metric certificate paths the certificate name and the key otherwise there's no need for that. Uh you can also say if your operator does enable HTTP2 or um HTTP you know it does not enable HTTP2 and then we have a list of functions uh that are R TLS options. I'll make it simpler explanation as we go ahead. So we declare a couple of variables we declare a couple of command line flags. We define some options for our logging that this is development true. when you say development, it actually um gives you a stack trace on warnings as well. Um it doesn't give you any sampling. Uh if you go for production, it only gives you a stack trace on um on errors and it does do a sampling for you. So if in case you are deploying this to production, that's something you should always consider um development as false. Now we set up a logger. We uh we passed all of our flags of the CLI that was given by the user. We um you know we define our options for logging. We create a new logger. Essentially what we're doing in this line here is we are setting up a new logger with our zap options or with our logging options. Now with your TLS when you have this TLS config it's kind of like a list of options that you can do. One option here is if you want to disable uh you know uh if you do not enable HTTP2 here in case you are disabling uh HTTP2 you can append that to your TLS options. So we say in this case uh my um you know I did not enable HTTP2. So for me in the TLS options it would be I disable the HTTP2 and I only enable the version 1.1 of my HTTP because I'm disabling the HTTP2. Now here's where you create some watchers for your certificates. You remember we talked about these certificates for the metrics and there could be certificate for the web hook because you can expose both of them um over over TLS. So the certificate could be for your web hooks. The certificate could be for your metrics. And we have a we have a cert watcher. So essentially what happens is let's say uh this is what I already explained. You have a cert manager. The search manager renews your certificates on the disk. This watcher will be detecting those changes on the certificates. It will load them into the memory in the current pod in the current operator. It does not restart the operator. It does it on its own. There's no there's no downtime. There is zero manual intervention. Otherwise, you'll have to um restart your your operator because your um you know your certificate was updated by the search manager. We define our TLS options which is again a list of functions uh that returns us a TLS config and we um instantiate a new variable. So it's kind of like we are creating an alias and this is the one um by by this time the TLS options is a default TLS options um that we would be using and we declare a new variable and we set that as a value. So we can customize um the TLS configuration for our web hook server uh if in case we want to use a watcher or in case you want to even use a certificates or not. So uh it's easier for us to customize. Now if you really gave a web hook certificate path which is here if you did give a certificate path that means you want your web hook to actually be serving over TLS and that's the that's the thing then if the length of your variable uh is greater than zero we will say initializing web hook certificate watcher and I will be then using the TLS as well and I will be using the certificate. So we define a variable error and here's where we create a new watcher for the certificate uh path and the certificate key. If in case there was an error, you just simply exit one because you wanted a TLS config for your web hook but you couldn't get one. So it makes sense to stop right there. And here we are adding a new option to our web hook TLS uh options. this variable it contains right now till this point it only has one TLS option which is disable HTTP2 that's what we we did here you know uh by this time it only starts with one uh TLS option which is disabling HTTP uh 2 and if you have given a variable um if you have given a webbook certificate path we then append onto this TLS options that we do want to use um another we do want to use a web hook uh certificate and this is the get certificate function from the TLS config that gives us the name or the information of the certificate we want to use for our web hooks and here's where we are creating a new web hook server with these TLS options similar thing happens when you are working with a metric server options so these are the metric server options uh where we define the bind address on which our metric is going to be exposed this bind address is 80081. Do you want to use secure metrics or not? And what are the TLS options? Again, by this time we are just disabling the you know um the HTTP2. Uh we don't have any TLS right now because if you do not do uh if you don't give secure metrics which is as a boolean if you do not give um secured metrics then there would be no TLS options. you only work with HTTP 1.1, you disable HTTP uh you know uh you disable HTTP2 but if in case you did give secure metrics you will be using some sort of authentication um um that your metrics endpoint is not publicly uh it's reachable but not accessible. There is some sort of authentication and authorization and only the authorized users and service accounts can access your metrics. Now u this was the metric service options that we started with. If in case you did want secure metrics you give uh some sort of authentication and then this is the same logic that we did for our web hook certificate path that if in case you do give you know your metric certificate path you create a watcher like we did for our web hook. Uh there's a watcher which is for our metric certificates. Then we append uh the metric certificate option TLS options uh with the certificate uh information. Essentially what this does is if you did give me a certificate path if it's not zero the length of the certificate path is not zero you give me the path of the certificate I'm going to run your metric server with the TLS option that that serves the certificate information. That's essentially what it is doing. So you should not get confused on uh on what this is happening, what this is doing. I just told you. If you do give the parts of your metric certificate, it's just going to expose your metrics endpoint on this certificate that you have given. The same thing happened here. If in case you did give a certificate for your web hook, it's going to expose your validation or mutating web hook over with this certificate information. Uh and here's the one which is quite interesting. This is the from the controller manager from the controller runtime. You see this is the one which I just showed you. This one uh lets us create a manager. Within the manager you can have multiple um controllers. It looks something like this. So here I have in my main.go file um this is my operator. This is the main.go file. In here I have a manager. Oh, wait a second. This is my manager. Let's take it this way. And within my manager, then I will have my controller. And I can have multiple controllers in a manager. If I wanted to uh write something about this, this is my controller. This is my main.go go which is responsible uh for creating a manager using the controller runtime and then the manager is responsible to or it's our responsibility to register our controllers with the manager and that's essentially what we're doing here. So once we declared all the variables, once we gave all the flags, once we defined all of our TLS options, once we have configured if we want to use TLS for our web hooks and metrics and if in case we want to use authentication with our metrics or not. Once all of that is sorted, we start or we use the new manager function that returns us a new manager which is available here. This is our manager. This variable has our manager with all these options. What is the scheme? So our controller knows about all the resources, custom resources or the the core resources available in Kubernetes. What are our metric server options? If in case with the metric server options, do you want secure metrics or not? You know, uh what is the port number for your metrics that you are binding to? What is the IP address for the metrics? What is the endpoint which is usually uh by default/metrics? And then if in case you have given some um certificates the same thing happens for our web hook server is it secure in terms of have you given certificates to that or not. Uh and that is our web hook server. Um we then declare the health probe endpoints which is um which is a probe address that is um where did that go? 8081. This is what your livveness probe and the readiness probes will be looking into the container when they are doing a probe. And here's where the leader election because when you are creating a manager, the manager should know uh are you looking forward to have a leader election and you should definitely do this when you are building an operator that you want to run in multiple replicas in multiple pods. There should be only one which is both of them are running but only one is active at any time. So this is this is absolutely your responsibility um that you can enable the leader election and then if you could not make the manager because new manager returns you the manager and also an error. So if you could not create a manager or you give the error that I was unable to start the manager and you simply exit because if you don't have a manager, you don't have anything. You don't have a controller. So that's that's the over um that's the one that looks on your controllers. If there's no manager, there's no reason to continue. Just just exit right there. And that's why we use the OS package. Now once your manager is created we need to register our controller which is the the custom resource which is uh what we need to do here. So if you were able to create the manager we are using you know um we from the EC2 instance reconiler we define the client and we use the manager.get schema which tells our manager what is the schema of our EC2 instance. Essentially we use a function called setup with manager and this one sets up our EC2 instance custom resource with the manager and which is available here in the EC2 instancecontroller.go file. This is this is the one uh which is where our reconciling logic is and where our reconider logic will be. So it sets up our controller in here in the main.go go. At this point once we started our manager, we set up or we add our um you know we add our um custom resource or we add our controller to our manager. So the manager knows that I have this particular controller. This is what I need to listen on to if any changes are done to this custom resources and this is what the logic is what I need to run with uh with the operator. Now it's also interesting here if in case you were having some certificate watches if this was not nil you add the certificates to your um you know you add the certificate watcher to the manager for your metric server for your web hooks again um we don't we don't use certificates right now and I'm also not using any web hooks for mutating or validating so I'm not going to do anything um any certificates for me it's going to be empty otherwise you will be adding the certificate watcher to your manager. So manager has couple of things. It has controllers. It has another controller. You can have more than one. It will then have the watcher as well for the web hook certificates. It also has a watcher for the metrics certificate and it watches and renews the certificates or reloads the certificate on the fly. So you don't have to restart your your operator from the manager. We also get uh a function called add health check. And this is where u the health check is is being done. Uh we add two health checks or two endpoints. One is a /halth, one is a /ready. And this is what you can use like a fairly simple Kubernetes health check that lets you see if your operator is healthy, if your operator is ready or not. And here from the manager which was written by the new manager function. This manager has a another function called start. And this is the one that starts our manager. It's kind of like you got a car which has an engine which has a you know which has um a mechanism for the airbag. As soon as you turn on the key the whole thing starts. So first thing is your engine starts. It starts sending power to other components. This is a similar analogy when you are starting this particular manager. So the manager starts and then it starts the other controllers inside of this process. It starts the watchers and uh and everything comes in into life. Now of course if you were not able to start the manager or create the manager above here. So either you were not able to create a manager instance or start the manager. We simply just exit because without the manager there's nothing uh that is available. So I think this was the whole main.go file and uh what I wanted to also show you here is we do import quite a lot of packages. We do import quite a lot of go packages around here. One of them is compute v1 in our API v1 directory. And here in the spec, this spec matches to our um where I go config CRD basis the CRD around here. See whatever you give in your um custom resources spec that gets reflected into a resource called custom resource definition. And here's why you're declaring. You're telling Kubernetes this YAML file gets installed into Kubernetes it's a resource that tells Kubernetes about other resources. It's a custom resource that tells Kubernetes about other custom resources. So you tell Kubernetes that I'm telling you about another custom resource who looks something like this. Its version is V1. Its name is EC2 instance. It's a list you know it's the name namespaced scope um object it is under this particular group and here's where the spec for your EC2 uh instance and you can see here the same one to one mapping we have the AMI ID we have the instance type we have the SSH key and we have the storage which is again given uh given here now at any time when you are writing a spec for the API you might want to change something. Imagine you could say I want to give a tag. um or you would say I want to give a department and this is going to be a simple string which is what you can use as tagging you know so when you create an instance you use this department value to add that as a tag to your EC2 instance and whenever it's very important at any time when you make changes to your specification you have to run the make command more precisely ly um you need to do the make manifests because your CRD is not aware that you just make changes to your specification. The CRD is still older. Think of this as now it's outdated compared to the spec where we added a new uh value. Do I have a department here? I don't have that. I can't search for it. Okay. So once you make changes, we do make manifest. And as soon as we do this, you see a new department um spec is now added which is type of a string. We can also say um um maybe I want to add project which is going to be another um tag. Uh and then again I will need to use the make manifest because as soon as I do make manifest you can see on the right side it's going to be added here. You see um the project was again added. So at any time you make changes to your spec, your CRD needs to be updated uh on the disk which is with make manifest and you also need to update the CRD into Kubernetes because see the flow looks something like this. This is you this is the spec and you make changes to the spec. Now, this all is happening on your computer right here. It's all happening. This all is happening on your computer right here. Um, and wait a second. Okay. So, this was a spec that you changed and you changed your CRD on the disk. However, um the CRD doesn't just need to be updated on your computer where where you're you're developing. You then have this Kubernetes cluster which is again um where you need to have a CRD and from where you can then create a custom resource. We talked about it the CRD now and from that you create a custom resource. Now you see you made the change and it's updated here. It's version two of the CRD but you are still using an older one. You're still using a version one. So you can use make manifest in the make file which is given to you by uh the cube builder. You can do make manifest. It updates it on the disk. And if you are pointing to the right Kubernetes cluster using your cube config the environment variable, you can then use make manifest and make install. It is then going to apply the same CRD which was generated by the spec change all the way to your Kubernetes cluster as well. So they're always in sync. You're not thinking I made the changes to my spec, my CRD is updated, but when I try to make changes here for this new change, you know, I want to add a new field called project. It says there's no field called project, but I see it here. It's probably because you did not um update your CRD in the cluster. You only updated that on your disk and that's not going to cut it. So um usually if I ever make changes to my spec, I do make manifest many fifths and I do a make install. So I update this on the disk and I also install this. Make sure you're connected to the right cluster otherwise um if it's a different cluster and the resource the the custom resource does not exist it gets installed there or if it's there it gets updated and there could be some breaking changes that you're introducing. So be very very careful when you're doing it. All right, I think this was the whole uh explanation of the main.go which is probably something you will not use a lot, you will not make changes to but it's it's absolutely important to know all these options what the web hook watcher does. Why do we have so many uh packages involved? Um you know you can expose your metrics uh endpoint securely. When I say securely I mean with an authentication and you can also use TLS or not. This is something optional both of them. The same goes for the web hook endpoints. So it is something which is which is absolutely important to know uh that you can also do the leader election and uh this is the main uh function where your operator starts. So now that you have a very good idea of the main uh go file uh which is the one that starts everything. Let's see how the reconiler works. Let's see the reconciler in action. We will make changes to some custom resources. See how our operator gets those changes and then what can we do on top of that. This is what we will be laying as a foundation of creating our operator that reacts on the changes of the EC2 instance object and then we um we will move on ahead from there. Okay. So whenever you want to write your own custom operator, the first thing you need to ask yourself is what kind of resource are you going to manage? In our case, it is going to be an EC2 instance over on Amazon. We are writing an operator. We building a custom operator that goes to Amazon based on our behalf and it uh you know creates you an EC2 instance. So something would look like this. You're going to have your Kubernetes cluster in which you have your operator running and there is going to be a human a certain someone that gives you a YAML file because we talked to Kubernetes via YAML. The the interesting thing about this YAML is the kind that you have declared it's going to be um you know the API version that you have declared using cubebuilder which is cloud um which is a compute I think which is compute.cloud.com/ver one of this API resource and the kind in this case is E32 instance. Now of course in the end what's happening you give this YAML to let's say the Kubernetes API server because it knows about um the EC2 instance which we will deploy our custom resource definitions to Kubernetes. This resource change maybe you say I want to create a resource of this kind. The controller in here will look on this change. It will get the data from the API server and this is the one responsible to go to Amazon and creating you an EC2 instance. This is the one responsible for making the authentication with EC2. This is the one which is responsible to provide the minimal set of instructions you need to give to Amazon when you want to create an instance. This could be uh the instance you know it could be the instance type that you want to give it which is absolutely required. This could be uh a security group you want to give which I think is absolutely required. Um you can you also would definitely need to give some storage on how much your machine would be needing. Some things could be required some things could be optional. For instance, um tags they are completely optional. You can give that, you cannot give that. It is up to you. So when you are writing an operator, you are writing a custom resource like this, it is on you to have some minimum at least most required uh things that you want to send to Amazon. And this is where when you are designing your spec because when you give you a YAML you will have kind API version then you will have a spec and then you will have a status. So this spec here is actually matching what you give in your YAML for other resources and that is what you will be having. So in this case your YAML would look something like speci ID. This is going to be the name of the key. Uh then SSH key, instance type and instance subnet. We are using these JSON tags so that Kubernetes can unmarshall uh it knows what does this particular thing that you are giving me called an AMI ID. What to do with this particular object with this key and then the request that is coming to the API server. We might want to extend this in terms of let's say uh in here for example storage. So I am giving um I also have an option called tags in my YAML which is going to be a map of string and string and you can also create your custom strct types. For example, if I give storage um and then I can have a custom object here. You see we know what is a string. Go knows what's a string. Go knows what's an integer. Go knows what a boolean is. it doesn't know the embedded type of uh storage config and that's the problem. It says it is undefined. So you can define another strct which is going to be uh type storage config. Oh wait a minute storage config uh and there you can give um you know the size of the and that's what I love about these AI editors. So you can give the volume size, you can give the type of the volume that you want. You know, Amazon have different type of volumes there. And then if in case you want to give your device a name, you don't want that. The only thing I would like is a size. And um it's going to be the type of the in uh the device that I want, which is going to be uh one of the Amazon provided ones. And then you can also have additional uh uh storage which is in here in this case one is a root disk and then you can have additional resour devices and this is where this omit empty comes into the picture. It's very very handy. The same thing could be done for our tags as well. The thing is sometimes these resource these options are you know these things are optional. You can give a YAML for Kubernetes that creates an EC2 instance but you may have tags you may have additional storage. You absolutely need the instance type. You absolutely need the AMI ID that would be wrong. If I do um you know in here if I go and say uh omit empty this is wrong because it has to be a required field in your YAML manifest your resource. So you can choose when you are building your um when you're building your spec as into what things you want. In this case the additional storage is a string or it's a it's a list of storage config. So you can add additional storage configurations. In my case um I I would keep the additional storage just as you know a type and a string. The same thing will happen for your status. So when you do cubectl get status hyphen o yaml what you see is the status dot you will see the phase in which it is you will see the instance ID you will see the public IP. This is probably the information you get back from Amazon. Imagine this if this developer gives you a YAML. Let's say you are building a internal development platform. You want the developers to query uh the resource that they have created. when they do cubectl create-f which has an EC2 instance and then when this guy says cubectl uh get EC2 instance you need to give him some information or you need to give her some sort of information probably the first thing you want the user to know is the state if the instance was failed if it was running if it was pending whatever state it was and then you probably won't want to give them the public IP of this instance. If in case you allow your organization allows for the instance to have a public IP, you will do that. The only place you can get this information is from Amazon. So when you are creating your instance in this case, you want to pull if it's running in certain time. If it's not, you fail the operation. Otherwise, what you do is you get back some information and this information is going to be the state of the instance and then it's going to be a public uh IP. In our case, this is what we care about. There will be many things that can be given back uh as an operation of creating instance. But that's not what we care about. We want to show the user that in their status they can see um the phase which is going to be a string uh the instance ID which is also a string and the public IP which is also a string in our case. Now whenever we build um whenever we u make changes to our API spec I told you that is absolutely important that you run the make command from cube uh from the root of this um cube builder project so that it generates you the custom resource definition. Um what has happened uh wait a second. All right. So when I do um in my API version one EC2 instance types.com actually it's in config CRD the basis and then compute um cloud.com v2 instance this is the actual custom resource definition that you have created when you make changes into your spec like in here when you make changes into your spec what happens is um when you take command cube builder code knows how to write the custom resource definition as a boiler plate and this is where you define the group for your resource. This is where you define the kind for your resource and then you define the version of your resource. So this would tell you that for a for a single kind of resource you can have multiple versions because you see it's uh it's a list of versions that are available. So you can have it to a cloud.computee.com/w1 compute.com/we1 then this schema would apply cloud uh compute.cloud.com/me2 cloud.com/me2 another version of the schema would apply and this is why you probably might have seen that this particular key is only available in a newer version of your YAML there could be some key which is only available in the newer version or uh in the older version it's um it was only available in the older one because in the version two that might have been removed the important thing is um is the spec in here so we have got our properties. We have an AMI ID. We got our instance type. We got our SSH key. And then we got our subnet. And these all are required because we did not get the omit empty. But now because we made some changes into our spec, I absolutely have to regenerate these manifests. And for that I can simply do make um manifests. So what that will do is it will be updating your custom resource definition with a few um more um you know with a few more parameters. For example, one of them is the additional storage. It was not there before but now it is. So then you can have an additional storage and then there's a new option also called storage and there's a new option as well called um tags which is a type of string. So every time uh it also updates the required because some of them do not have omit empty they are absolutely needed. This is how Kubernetes knows that this strip this particular key is not available in the YAML. I have to cry about it. I cannot let the user give me this request because the custom resource definition has marked this particular uh you know string as required. this particular key in YAML as a required but the user has not given me that. Now this is what you will give to your Kubernetes cluster before you can create an EC2 instance before you can do anything w with the operator. The first thing you need to do is you need to give this to Kubernetes because if you do not then when you create a when the developer creates a YAML of kind EC2 instance and then the API version which is cloud.com Kubernetes has no idea what is this um you know what this resource that the user is talking about what is this group called compute.cloud.com cloud.com in version one I don't have an inst a resource called EC2 instance and this is something you can either uh use a cubectl apply uh with this custom resource definition yaml or you can use make install command with the make file that's something what is actually done for you so you see we use customize to build our custom resource definition and then we apply that to um cq cubes apply - f and on the standard input and this is where we now have our custom resource definition first make uninstall and see what happens if I do um cubectl get e2 instance uh dot um oh wait here see if I do k get e2 instances compute.cloud.com cloud.com or if I just do tell me how many ETR instances do I have Kubernetes says I don't have that resource but if I do okay I'm going to deploy you or I'm going to give you a custom resource definition at least don't say I don't know what that resource is if you have that give the user if you don't have that just tell them I do not have that resource but don't just say I don't know what resource are you talking about so this is What you do when you uh give your um you know when you do a make install creates your custom visro definition which if I see here you can see this is uh you can do a CRD on you can do a get on your CRDs and this is the custom resource definition that I have which you actually can uh also see like this and this is the same thing that I just showed you on uh on on on cursor so which is not on cursor distributed in terminals. So that's how one would actually um update or create the custom resource definitions. In my case, uh how would the YAML look like? So if I would probably ask um you know um my my AI that okay take uh take this spec and give me an updated YAML for this resource. It's going to just spit out how the YAML would look like. And see this is additional to what's what's going to happen. Uh I just going to I'm just going to accept that. Here we go. So this is how your YAML would look like. It's going to be a kind of ETO instance. Then you see some specs are there. It tells you um you would have an AMI ID, the SSH key, instance type. Maybe when you give this to your developers, you might want to make it a lot simpler or at least make things like um instance type or VM preset, something like that. if they're more familiar with with those words. Uh maybe you can do uh SSH key. That makes total sense. Um I think this this does make sense. Um it would have been better for um other other examples. But in this case, the YAML is perfectly fine. So I I would say okay, this was what I wanted to show you as a YAML. In our case, um the next thing that you would do is once you have your YAML defined, once you have your everything defined, now we need to look into the reconcile loop. See, by this time, Kubernetes knows that it has some a custom resource called um EC2.cloud.acample doommain.com. In version one, there's a resource called EC2. Now if someone gives it a EC2 instance, what to do on that? If someone gives it a YAML that please create me an EC2 instance, what's going to happen? How would it react to that? And this is where we will be looking into our reconcili. So let's get started and let's see how we will build a reconcili. So our reconstru loop would look something like this. It's under internal controller EC2 instance_controller.go. This is where the magic happens. This is where whenever you make changes to your custom resource, that's the place where it comes to and then this is where the logic you would be giving to operate on the resource that has been changed which is your custom resource. It is in the package controller. It is importing quite a few things. One of them uh is controller runtime. This is absolutely important that handles the runtime of your controller. And also you see it is actually getting our own um uh ECI EC2 instance. This is going to go to github.com in here operator repo API v1 and then it's going to call this as compute v1. Essentially what this is doing is your controller needs access to your spec of the U custom resource. It could also have been very simply done but I could say you know um please go to API v1 because I have that locally available or okay I think it's better because I have it already on GitHub. So what's going to happen is if I show you um if I go to githubhub.com and here is going to be my repository. Let's go to GitHub. I should have just copied otherwise it goes to Golang Populator repo. Here you can see an API v1 and this is where it's looking for the EC2 instance type. This is where our code is. So this is what Kubernetes operator will be using and the controller will be using to map your request to a particular known um data type to a particular known uh spec or status of the custom resource. This is the heart of your um object that you're creating. Um it creates you a reconiler which is used in the client runtime that helps you communicate to the Kubernetes API server. It has a schema object which registers your schema. Couple of cube builder um markers. I think yes it's a marker which is creating you the arbback rules so that you can uh work on the custom resources because when you create an operator it would be running it in its own name space but if a custom resource is created in a different one the operator needs access to see in that name space as well. So this is where the arbback is extremely extremely helpful. Now what can we do with this? This is the reconcile function. This is the one where all of your requests are going to be uh looked into. This is where all of your you know um whenever you do a cubectl get or cubectl apply this is where the changes are going to be looked upon. This function has two return um it returns two things. First it returns the result of the reconciliation and second it returns the error in the in case if there was any if there was no error it will simply return a nil. Now this is the beauty of Kubernetes selfhealing. You know how uh if you create a pod which has a persistent volume claim but that PVC is not bound to a PV yet that pod is going to be in a pending state. It keeps on being in a pending state but as soon as you create a PV as soon as you add it to the persistent volume claim the pod is then automatically started because there was a recue going on for that particular pod. the the reconciliation for the the logic for the pod kept being if the requests are fulfilled if they are not I give you an error and I start the reconciliation again it puts it in the queue to reconcile this is the beauty of selfhealing it will be done eventually once all the conditions are met and you don't have to trigger that reconcile uh you know uh you don't have to trigger another run of the reconcile loop yourself kubernetes sets for you and this is where you will be giving your logic. The first thing that you do the very first thing that you will be doing is you need to operate on that instance. For example, um the basic thing you when you talk to your you know when you say that users can then create an EC2 instance of their type. You want the user to give the name of the instance. You might want them to give the tags of the instance. You might want them to give the storage config. So you need to extract this information. You need to extract this information from the request from the API server request that came to the reconciler loop so that you can use this information to talk to Amazon in in our case because it's a cloud operator. So you need to store or you need to get all these objects that are being given in this YAML by the user in certain variables. So you can iterate on top of that. So this is very important. The user is actually creating a resource of kind EC2 instance. You also need to have a variable of kind EC2 instance. So that Kubernetes you can use the Kubernetes schema to store your actual keys in your variable. It's think like the user is sending a circle. You need a mold that can hold the circle. If the user is sending a triangle, you need a mold that can hold a triangle. If a user is sending an int data, you need a variable of type int. The same thing happens. The user is sending the data of kind EC2 instance. You need a variable that will be of kind EC2 instance. So let's declare that first. The first thing I do is EC2 instance object uh is going uh EC2 instance object is actually here. So I'm using the compute v1. This is the compute v1 and in here I have declared the EC2 instance and there you see this is essentially what we created in our uh types.co. Now we do have a spec then we have a status but essentially this is the root of our Kubernetes um object the EC2 instance will have some metadata it will have some object metadata uh it will have the spec and then the status. So this is what we are calling and creating a variable in our reconciliation object. Then what you can do is because the API server will be sending a request to your controller or rather it's the other way around the controller will listen if there was any changes done on your custom resource and then you can iterate on on top of that. So you create a variable of type e to instance object. I would rather make it simple just to keep it easy to instance. And then we can use the get function. What this get function does is it uses the context which is of your of your request. More importantly, it gets the name space under which this resource was of changed. I'm not saying created. I'm saying it gets the name space of the resource in which the update happened on your custom resource and the actual inflight um YAML the actual context of your YAML is then going to be stored in this particular object in the EC2 instance. So think of this as you take the YAML from the user and you give it to your reconciler. So now it knows the name space in which this object was updated. U the name u you know the the instance type this YAML had the kind um the storage type this this YAML had the number of tags the user wanted that you can now create on top of this. So I was want to say let's say um log uh there was also I think before this there was a logger that we can also use. So here you can see we have a log function and we can say um I want to log all of my request and using log.info I can do that. So I got I create an object of type EC2 instance and this is the EC2 instance instance type. I get my object which is coming from um the inflight request and then I'm saying reconciling EC2 instance and you see I can uh get the name of that particular resource rather than info. Let's just print this for now. So um I would say I want to have an EC2 instance. EC2 instance and then I can say uh print lm I got a um I got a request for an EC2 instance in the name space and then I could say and the EC2 instance is um EC2 instance just just keep it like that uh you can also probably then say fmt.print print ln and you can print the entire spec you want. I don't want to print the entire spec. I would just say I got a request for an EC2 instance in this name space and the instance is instance name. You see these are all the options that you can where I go here is and the instances are here I can say and the easy to inst Oh my god wait let's get let's do that again I want to I want to just uh see the data that has been given to me and I can say uh I got a request for an EC2 instance in the name space And let's keep new uh uh prints. The instance, the EC2 instance name is EC2 instance.name. Uh and I would say then the instance FMT. LM instance type is uh EC2 instance.spec. And you see this is this is the beauty of uh the AI editors again. Now you can see I am able to get all the information which was sent which was you know watched by my reconciler by my operator under E2 instance spec and you can see AMI ID SSH key subnet tag store regation storage this is essentially what you were building in the spec of your custom resource this is a onetoone mapping that is why we created a variable of type EC2 instance and then we got the you know the inflight request that we received from the API server and then I'm saying I got a request for blah blah blah the only thing I do not have see I have the instance ID I have the AMI ID SSH key subnet tag everything I don't have the name for my instance and actually this is the name of the object that I'm giving but maybe I want the user also to give the name of the instance and I would simply say instance name because instance name could be different than the kubernetes object kind uh metadata the yaml that you give they would be different and here I can say um my name would be in the spec uh spec dot instance name it's going to be a capital spec Now the important thing is I just added another uh object in my spec my custom resource definition that is right now in Kubernetes. It has no idea about this new instance name. So I would have to do my magic again. So I would do make generate make manifests and then I would say make install. So that my Kubernetes is now updated that there's a new resource called there's a new uh there's a new key in the strct for the spec which is called instance name and um that's it. Now once you get the data once you you know uh iterate on top of that in my case I'm just printing it right now but as we move forward we will use this data to talk to Kubernetes and then I will uh create myself an instance that is where you would have your actual business logic what we will then do is once you have used the data in my case I'm not changing anything in the object object there was a resource created I got information about this but I'm not updating that resource so then in that case I will be returning a a result which is going to be you know um is if in case you are spending it contains the result of the deconeller reconciler invocation if you go to the controller runtime on go on on the go uh consider this result actually contains two things. Whether to recue this or not and this is a default to false. This is very important. When you exit your reconciler function, you need to tell two things. Whether there was an error in the reconciler function and is there a requirement to rerun this reconciled loop. You only remember we talked about this in the previous part of the video that you only reconcile you only uh you know rerun the reconcile loop if you have updated the API object. We are not doing that right now. So we do not need to u you know send any uh reconcile boolean which is VQ as boolean. By default it is false. So in case in our case we don't have an error and we are also sending a pause for the reconciliation this reconcile loop will not run again. Uh it's kind of like um when you start with the reconcile loop. So this is this is how it looks like. You have the reconcile function and the request came over to this function. you made your change, you made your business logic, whatever you wanted to do. In our case, I'm just printing things. I'm not creating an EC2 instance. I'm not updating my custom resource with the status of uh the EC2 instance creation. I'm just printing this. So, because um here um should be a bit bigger change made. So in my case, did I make a change into the custom resource into the custom resource that request came to me which is in the EC2 instance? And I would say no or you could say a yes. And in in case you have no changes, you would simply return nil for the error and then false for your reconciliation. If you did give a yes, if you made some changes, then you have to return a true for um the reconciliation and then if there was an error, you will return the error. If in case the error was nil, you will return the nil. This part we will talk about uh coming up. But for now, I'm not returning any uh you know, I'm not uh changing anything in the EC2 instance object. So I'm just returning a false. Now this at this point guys uh let me do a l.log and then I would say um let's do here reconcile reconciling EC2 instances the name and I would here say reconstance and this is the name of my instance. Now let's get a YAML and then see how this will be um functioning. Now this is the time we run our operator in Kubernetes. Now we can make a container image. We can you know push that container image to a registry and then get it from there. The good thing about using cube builder is when you have a working development environment and you have a cube config which points to a operate on your cubernetes cluster you can just run the main program locally and then it will be as if like it's running in your kubernetes cluster and I will again uh call my trusted AI to use the spec and give me a dummy yaml So we can create that. Uh uh uh this is the spec. Let me quickly get this and then I would say please undo everything. I don't need that change. Cool. So um let's say Kubernetes. Do I have a folder called Kubernetes? No. Let me do an example. Uh instance instance.yaml. And there is our spec. Before spec we have a API version and then we have a kind and then we have a metadata and then you see we then have spec. The API version is v1. The kind is EC2 instance and the metadata uh wait the API kind is API version v1 but it's compute.cloud.com/v1. E2 instance metadata would be name of Kubernetes object for uh EC2 and there we go this is simple uh what we have and then I would say let's run our operator now we can do go run cmd main go because in in the cmd folder um wait where that goes here in the cmd folder, you have your main program. This is the entry point. In any go code, your entry point is always going to be the main uh go file. This is the one that registers your schema from Kubernetes. This is the one that creates a client so that you can talk to Kubernetes. It registers some uh you know um some booleans, some flags if you will. We will clean this up because we don't need a lot of that. We already have gone through this code. The most important thing that it does is it starts the manager here. Uh enable enable enable enable enable. But I think we did see somewhere that it was starting um the manager. Wait a second. This is the new manager function. Just going to give the manager new. Where was that? uh uh uh uh here. So we're going to have a log of starting manager because we did not work with the web hooks. We don't have any readiness check, livveness check, nothing. So we should just see starting manager and then we should we we will be seeing if we get any request to our controller. So here we will be exporting the cube config. Let me increase that on a little bit. And here I would run my function. If this is a little a bit small for the font, please bear with me. I hope this is this is you know seeable. But uh essentially what I'm doing is I'm running the main function now. So we will be running our operator. Do I have any EC2 instances? Uh no. Do I have them in any name space? Uh no. How does our example look like? So if I do k - f example. Oh wait, I need to go to operator folder here. And then I can do k - f example instance create. Let's do a dry run. See if our yaml was good. And there you can see the yaml was fine. Um then I can just simply say first I run my program in here. And this is how your go code will be running. So you see this is all what cube builder does for you. You do not have to set up your authentication with the API server. You do not have to set up your um you know um how would you run your your controllers? How would you run your multiple operator loops that you have for different API versions? It does that for you. It starts an event source. So it's kind of like the listener for your object in Kubernetes and here it's starting a worker for there's a controller for EC2 instance. This is the group and this is the kind uh which is uh EC2 instance. So it's kind of like you have one controller for one resource. It is a onetoone mapping. You can have multiple instances of that controller and in this case you would do a leader election uh because if one object uh if one you know instance is managing your request for that custom resource others should not do that but in our case we only have one replica but we have one controller park object if I was uh if I was creating um more custom resources let's say right now I have an EC2 instance This is my custom resource. For that I have a controller. My controller here you can see it's called uh also EC2 instance. If I was to create another custom resource which was let's say a storage bucket. Maybe I want the users to be able to create buckets in my Amazon account very easily. There would be another controller uh which is going to be then storage bucket. They could be running in the same manager in the same manager or operator pod. I think this is where you can review uh the part of the video before where we talked about what is in the operator. There's a manager within manager. Then you have multiple controllers but it's a onetoone mapping to the object and um the the controller. Think of this if anything happens to this resource this code will apply. If anything happens to this resource then this particular code would be would be applied. Now is the uh now is the moment of truth. Would it would something happen if I uh simply just say please create me an instance.yamel I should see something in here. That's what I am more concerned about. So let's create that. Um of course it's invalid. I cannot Oh there you go. So it says the kind is invalid. It must be EC2 instance. Of course in the dry run for client side by much sample here my kind was wrong. And then if I do a create again you see there is my request. I know that the instance name is my EC2 instance. This is Kubernetes not knowing about this. This is our operator knows about it. So it started the worker. Cube builder started the worker and this is our code from here till here. This is our code. We get the log which is reconciling instance and you can see this is the code which is um started here reconciling instance name and then we get all of our u program executed. I got a request for an EC2 instance in the name space. You see it gives you the name space default and then the object name as well. um which is a request.namespace. This is telling you the the namespace as well as the name of the object that you have. The EC2 instance name is this is now reading the spec and you can see tags are it's giving you a map of environment dev owner is Alice which is this is what in your YAML looks like uh example and instance. So essentially what the user gave my program our operator our controller most importantly knows about it you see so um my storage would be size 50 and then type GP2 it's actually just printing this as an object map but we can u do that even better for storage let's let's make some changes I want to say storage size is 50 and type is g2. So I could say storage size is um you can say f is size and type is you see storage dot type you can obviously access any sort of object that was there in your spec like this storage dots size because this is how you access yaml so I would say spec dot storage dot size and that's also what's happening spec dots storage dots size and same for the type. Um, you can also do a delete. Now, see this is very very important. This bit executed when we created the resource. When I delete that resource, when I delete that resource, you see my reconciliation loop started again from the very beginning. This is absolutely absolutely important. Whenever you make any changes on your object, the reconiler starts from the very beginning. It does not know whether you created the resource, whether you deleted the resource, whether you um you know um whether you updated some metadata annotation. It has no distinction of what the uh what did you do? It knows about the update that has happened. And this is where it is your duty as someone who is writing the operator, someone who's writing the controller logic that you can make changes. You can run your reconciliation loop many times. But if no change was required, no change is actually made to your object which in this case you can see because the request because the resource was deleted we don't have any EC2 instance name we don't have any instance type nothing but the loop ran completely and here you can it says reconciled EC2 instance blah blah blah something more evident would be when I just uh show you let's say the name of the instance I want to get rid of uh all these things because I want to keep it simple. Uh or rather I would say um I would say fmt. Ln uh got a request simple or update was made to the e uh EC2 instance restores. I'm not saying the name or anything. I'm just saying that there was an update made and this is why I am um called or reconiling that makes no sense. Now I will run this again. I stop my program and this is the beauty of stopping the program when you are building this with cube config uh with the cube builder because it has a graceful shutdown. It doesn't just stop the program abruptly. It is a graceful shutdown and um it it helps you uh cleanly shutting down your manager because I made some logic changes. I'm now starting this again main.go and then I would say um k - f create. So here you can see it says update was made to the EC2 instance resource and this is why I am recon I am reconciling it. That's the main uh logic here. And then I got the instance type which is E3 medium. If I made me make some changes to this EC2 instance let's say I want to add a metadata. I want to add a label here. So I want to say labels and I would say hello colon world. I save and exit. You see I got another line. It's not like I created the object. It's kind of like I only um updated it. So you see I was not making a change as in I was creating that resource. I just edited that and that was only a simple metadata change which was the labels but my code ran again from the very beginning. What if I add some annotations to my object? If I do um here let's go to my annotations and I would say hello again and world. You see the whole reconcile loop runs again. The thing that I'm trying to tell you is whenever you make any changes in your um object in your custom resource, the whole reconciliation loop will run always. What if I maybe remove my label that I had added or remove the annotation? Say that again. You see running it again. Kubernetes does not differentiate whether it was a metadata change, whether it was a spec change. It does not do that. It just simply goes ahead and says okay, you change the resource and this is the update. This is why when you make changes to sync, let's say your instance name or instance type, the reconciler finds this. This is the beauty how a reconciler would work. Whenever you make changes, let's say in your instance type, you make a change from T3 medium to T2 micro, the reconil has no uh state. First of all, it does not remember that before it was T3 medium and now the user has asked for T2 medium. It does not remember the past request. It knows the status right now. I mean it's in the HCD. But in this case, let's say when you are going and when you are saying that uh my my type for the instance was T3 medium before and you change that to T2 medium. This before is stored in HCD. That is correct. But the reconciler loop that will run, it will have no idea that previously the user asked for a T3 medium. They're completely stateless. What the reconciler loop will now do is it gets your request. This is your logic that you would add that allows the user to change the spec for instance type or maybe um you know the user can dynamically change the tags that they want to give. So here in this case if the user has made updates to the type uh key in in the in the YAML of the EC2 uh resource, it is your responsibility that goes to Amazon and sees if the instance of T3 medium was available and if it was you delete that and you create a T2 micro because you can't change the instance type as far as I remember. if you can that's on on Amazon side that's a different story but the reason what I'm telling you is your operator your controller the reconcile loop will not remember the past request it always has to check the current state is T3 medium the desired is T3 medium nothing needs to be done but if the current state in the cluster is T3 medium and the desired is T2 medium it goes to Amazon is okay this needs to go away and this needs to be in action and this is how you do selfhealing or eventually consistent and then you update the object which we will see in the next u sessions. So this is how you will be building an operator that knows how to watch the API server for your custom resource changes that knows how to watch the API server um and update the reconciliation logic in case there was some changes you change the object which we will see um and um yeah that was it. So this will be giving you a very good idea, a very beginner idea. I would not say beginner but it's a good enough idea for you to build your operators and then you run them on Kubernetes. Next thing that we going to be learning is I already have it available. This is going to be how we will be ziting an operator which will be actually creating us an EC2 instance. The next parts of this video are going to be more onto how to use Amazon SDK in Golan to create an EC2 um instance on Amazon because we now know from Kubernetes point of view, from the operator point of view, we know how to write an operator, we know how to write the spec, how to install the custom resource definition and how to react on changes into our custom resource in the operator. Now it's about what do you do with that change. In my case, I'm just printing it. In the actual case of the course of this video, we will be building we will be using these changes and then we will be building them on top of Amazon. We will be creating an EC2 instance. So that is what we will be doing next. Till this point you know how to write your operator. You know you can get requests. you know how you can you know the reconcile loop does it for you. So in the next part we will be using uh the Kubernetes SDK in Golang to create us an easyto instance and then we will see if in case a request was successful we don't need to reconcile again we will talk about finalizes but that's all coming in the video. So let's look at how we can create uh EC2 instances with our operator uh using Golac. Okay, before we can actually get started for the code, there is something which is absolutely important for you to understand. We have been working with the reconciler loop and we talked about that the reconiler is the one that takes your request and runs it through a series of you know your logic and that's where you get your changes for the current state to be equal to the desired state. However, this reconciler is expected to return two values. One of them is the result and the other one is actually an error or it's going to be nil. These two return values are actually required by Kubernetes to know what needs to be done for your current reconciler request. So imagine your reconciler got a request here and you made some changes to your environment. You made some changes to your you know resources that you need to change and then you have to tell Kubernetes whether you want to re rerun the reconciler for the same request or you just want to wait for new requests. Uh wait for new requests. In this case, you did not get an any error. You did not return any error. Based on these values of the result and the error, that is when Kubernetes decides, do I need to rerun your existing request with the reconciler again? And this is how we work with things like selfhealing. If you know about this, you can give this an uh you know, give this the give this a try. Get yourself a pod that is in a pending state because of CPU or because of memory. Ask for resources that are not available in your cluster. The pod is going to be in a pending state. Then go ahead and add a new node that will be able to host that particular pod. And once that node is active and available, this pod gets from pending into the running state. You didn't have to do anything. You didn't have to tell Kubernetes that, hey, I got a new node. Please put my pending pod on this new node. It it didn't work that way. It was self-healing because when the first time the when the first time Kubernetes tries to put your pod to a node, it says, ""Okay, there is no node available. I'm going to put this in a pending state."" Think of this as a recon silent. So, the decision were made that I'm going to put the pod in the pending state. And the controller responsible foruling your pod returns a pending which is actually uh it sends an error that for the request that came to me I was not able to properly process it and there was an error and this is where Kubernetes knows I have to retry again for that request and this is how self-healing works while Kubernetes was retrying and retrying and retrying with an exponential ial back off you happened to add a new node and this is when once you added the new node when the you know when the logic ran again it was no longer pending the reconciliate said okay you asked for eight CPUs and I have node now which is 20 CPU available 20 cores that are available I'm not sending any error rather I'm going to send a nil for an error that I did not get any error and the pod was scheduled and the pod but then eventually went into a running state. This is something that Kubernetes does for you. And as a developer for this reconiler, it is absolutely your responsibility to tell Kubernetes whether your reconil function was okay or did you get any error and would you like uh Kubernetes to actually retry that particular thing. This could happen for EC2 instances. Imagine when you tried to have your reconcile function and you were calling the AWS uh API to create an EC2 instance and you were not able to do that. You had the right credentials, you had the right access for AM for your user that you are using but maybe uh there was some network timeout happen or anything that could stop your request from processing um happened. you would like to retry again, right? Maybe after like 10 seconds or 20 seconds or whatever your time is, you would like to retry. In this case, you can tell Kubernetes that there was an error. My reconciler function is returning an error that please retry that again. And based on the requests and the error values, Kubernetes decides do I need to retry this particular request or do I need to wait for new events or new updates to the custom resource for which this reconciler is listening on. So there's a very simple um condition that your reconciler can actually uh look into or look for and this is also in the priority order. If your reconciler function, if your reconciler is you know um is returning an error. So your error is present you are returning an error. This result is completely ignored. That means whatever you send in the result is completely ignored and you are then using an exponential backoff. A little bit about the result. What are you actually sending in this result is two things. First you are sending do you want to recue or not? Usually it's a it's it's a boolean where you can say I want to recue or not. And second you're sending a time for the recube. If you are sending an error if there is an error present in your reconciler this result is completely ignored and you will always be retrying. Kubernetes says okay the reconciler function or the reconciler is giving me an error that means it could not prop properly process the the request that came in I will retry this and this is where the self-healing uh loop comes into the picture if in case you are uh not sending any error so and this is the second thing if you think okay everything is fine I have processed my request I'm not sending any errors and you do send a custom recue after. And this rec is actually this time rather I should have put this as um wait a second I can probably get a better color here. Um this should be rec after this is the time after which your reconciler should again be uh started and this is like a forever running loop. So imagine this. You create an instance. You create the instance. It's okay. You probably want to check for the instances every 10 seconds or every 20 seconds. Maybe you are doing some sort of drift detection there. And if you were able to look for your instance, everything was fine, that means you are not having any errors with that instance. Um but you re you want to retry that again after 20 seconds and this is what you are sending. You are not sending any error because you did not have any errors. However, you are sending a fixed time. You are telling Kubernetes that there was no error in my request but I want you to rerun this reconiler every 20 seconds. And this is kind of like a forever running loop. It never stops because you don't have any errors but you always want to retry that again and again. You want to rerun this. What could be the reasons for it? I just told you. Maybe you want to do some sort of a drift detection. The third condition could happen if you are not sending any errors and you want to you know recue and your custom timeout is not set which is kind of similar that you have no errors and you also want to recue but you don't have any re uh recue after set that means you are asking Kubernetes that hey my reconciler was okay I want you to retry again, but I'm not telling you in what frequency do you have to try. It's kind of like similar to level two, which in this case you're also not sending any errors, but you are telling how frequent do you want to try. In this case, you're also not sending any errors, but you're not also telling Kubernetes um how frequent do you want to try. You are letting this with Kubernetes and this is going to be the exponential backoff. This is where Kubernetes will say okay the user said there is no error for the reconcile loop the function was running properly fine but they are not asking me to run this in a forever loop I would probably uh I'm going to use an exponential backup so it will run your request and then maybe another time it takes 2 milliseconds the next time it's going to take 4 milliseconds the next time it's going to take uh probably 16 milliseconds or so and this is going to be an exponential back off until I I think the maximum limit is 1,000 milliseconds um until then it stops doing it. And the last condition that you can return for your reconciler is you do not have any errors and you also did not send any rec flags. Probably you just said result result was empty and then you are sending a nil. You are returning a nil. This is where Kubernetes says okay everything was fine. I'm not doing anything. I'll just wait for a new update or I will wait for a new event where the custom resource has been updated. Kind of like for the new requests here. This is absolutely critical for you to understand otherwise you might see your reconiler making changes again and again or you might see your reconciler running again and again because you did not send the right set of values. you did not put the right return values for the recon and for kubernetes to understand what to do now as as I was talking about once this is understood uh I was talking about we will be looking into the go code so let's take a look here and let me get that here so in your screen you can see that I've made some changes to our um our instance spec before this this before u now it was a very simple one. It was just having an instance type, an AMI ID, probably a key pair and a security group. But when you want to make things more robust and when you want to make things more production ready, you have to think from an overall point of view. When you want to create an EC2 instance, there could be many things that you have to give. You definitely have to give the instance type whether you want to use a T2 micro, T3 micro or any other instance family. Then you absolutely have to give an AMI ID which is going to be the the AMI ID you want to use. You have to give the region as well under which your instance should be created. You need to give the availability zone. You have to give the key pair so that you can log into the instance. You need to give the list of security groups around here. the subnets in which your instance could be running and also when you want to provision the machines as soon as they boot up with your changes we usually use Amazon's user data and uh that also is what you can give you can probably give tags as well you can also give some storage you have to give storage and whether you want the instance to have a public IP or not this is kind of like a boolean that you can give now on the right side you can see this omit empty. This is actually that uh a place where you can control what kind of fields in a YAML when you give your EC2 instance spec are optional or what kind of fields are required. For example, tags could be optional. User data is optional but storage is absolutely needed. AMI ID is absolutely needed. Instance type is required. So this omit empty lets people define the only important or the required fields otherwise the other ones could just be skipped. So here you can see I have a storage which is type of a new strct called storage config and here's a new strct called storage config where we define a root volume and then we can also probably give some additional volumes as well. This is an example where you give your root disk as 100 gigs and maybe you want a VM for a database. You can add a bigger disk in the instance and this could be done by the additional volume and both of them are of type volume config and a list of volume config because additional volume itself is a list of additional disks that you can add to your instance. And this is a very simple volume config where you define the size of the disk. You define the type of that disk, the device name which is going to be available in the instance when you mount it or attach that and the encrypted uh boolean if in case you want the device if in case you want the disk to be encrypted or not because Amazon's uh allows you to encrypt your discs in case you want that. So this think about the EC2 instance as a more holistic approach whether you want to give or you want to allow the users to be able to declare their um set of set of data and the metadata. In this case, you are allowing the developers to create an EC2 instance, not just create one, but also you are letting them login with their key pair and you are also allowing them to use their user data that you can, you know, give to Amazon when you are creating the instance that lets it preconfigure before they can even login and the VMs are exactly how they want it to be. So this was a bit of a change in uh our EC2 instance spec to make it more production ready to make it more not from development but actually to production. I also made some changes to the status where when you do cubectl get uh EC2 instance you will see the spec and also you will see the status. So in the status I would like to see the instance ID so it is easier for people to see what is available on Amazon and what your Kubernetes knows about the state of that instance if it is running if it's terminated it is unknown it is stopped all those Amazon EC2 instance states and also a very important thing is going to be the public IP because when I do cubectl get EC2 instance I should have enough that lets me log to this public uh to the to the particular instance and this is a public IP and that's what I want to show when someone does an EC2 uh uh cubectl get EC2 instances and then again we have the standard strct of our EC2 instance which contains the type metadata the object metadata and our spec and status and this is kind of like just when you get a list of instances what's going to happen and this is how Kubernetes knows uh what is a set instance would look like for you. Now I've already made the changes and I told you whenever you make make the changes you have to run the make manifests command and then you have to install that to your Kubernetes cluster. So my Kubernetes cluster already has this custom resource definition. If I do cubectl get EC2 or CRD which is EC2 instances.computee.cloud.com cloud.com- oyl and let's look at this you can see the name is easyto instance the list kind the plural the singular it is a namespace scope resource and there you can see I have got couple of things such as the kind and there's my spec I have got the AMI ID the associate boolean um a public IP or not it's a boolean the availability zone I want to run my instances on and things like my security group which is type of an array because you can give multiple security groups and then I've got my storage configuration where I give one root volume and I have got additional volumes which is type of an object which is then again uh globally it's a type of an array so you can give multiple additional volumes but you can only have one root device you can only have one root um clock device now once we have uh defined our spec properly. There is going to be now some uh code that actually uses this and creates an EC2 instance. So let's see that. Um once I have my instance type, I can actually go to my U E2 controller and this is where everything starts. This is where you will be seeing the reconcile loop. We saw this before. We use the reconiler to actually see uh what happens when I get a request and this is what your to-do list is my logic starts and I have created a logger for this context that is aware of the context and you can use l.info which is going to just print stuff when you are running your operator. So it makes it more verbose and you can see what is going to be uh what's going to happen or what is happening with your controller. It prints out a function uh it prints out uh an info message that the reconsidered loop has started and this is the name space under which you got a request. So Rick RQ is the request that comes to your consiler and then you send a result back to Kubernetes. So it came from this name space and the name of the request was uh request.name. That's the name of the object that uh we are uh we are working with. Then there are some comments which I was building this. I put some comments for us to be easily understanding this again. But you know what we are doing? We are creating a variable of type EC2 instance so that we can marshall the object which is coming to us in this reconciled loop by Kubernetes into a variable and then we can easily iterate over on top of that. we get the object uh into uh our EC2 instance variable from this name space and if you could not get the object and this is absolutely very important. See, you may have any problems uh for getting the object. Maybe you have a wrong YAML. Maybe you probably were supposed to give a string, but you give a boolean to one of the keys. Or you probably did cubectl delete the object. That's correct. Even if you delete the object, it is an update to the custom resource. Then again this reconciler is going to be started and you have to check if the error that you got when you are trying to get the object was is not found. This is one of the errors from Kubernetes. Kubernetes has a package called errors. And let me show you here and here you can see it has all these errors defined for you. So it makes it easier for you to declare what was the error in my case. See, sometimes when you create an object, it gives you the object already exists. It's an error. But you can actually see what kind of error it was because if I was just say if I was just saying get me the object and if error is not equal to nil, I would just say okay there was an error please try again. But the user will never know what the error was. In this case, I can say, ""Hey, you know what? I was trying to get your object. I was trying to get it into my variable, but I got an error while trying to get it and the error was is not found."" And that is where it returns a true if the condition was that I could not find the object. This is probably when you are deleting the object. Um, it again runs the reconciler. It looks something like this. you have uh the object here. Whatever change you make on this, whatever change you make on this, the reconciler will be running again. So the change could be you added an annotation. That's an update. Then the reconiler the change could be you added a label on top of that to the object. Again the reconil would be running. It is your responsibility to write this reconiler in a way that if it surely should not be changing anything if the change that you did to the object doesn't require a change it should not be changing the actual resources. For example, your object could be EC2 instance. Maybe on this Kubernetes object, you want it to give a label. That doesn't mean you have to change something external to the Amazon instance. That doesn't that should not happen. So this is something you have to code in your reconciler. Even when you say cubectl delete, when you say delete, the object is deleted. there was a change on the object and then another uh run of the reconciler would happen. So you have to check that when you were trying to get your object you could not get that and there was an error and the error was actually is not found you will simply say um the object does not exist or uh there's no need to reconcile because the object was deleted and then we just return an empty result and a nil. Remember this is one of the return types that you have to say. What you're telling Kubernetes is everything is fine. There was no error from my side because the object does not exist in in our case and please wait for the new requests which are coming to the deconidered. This one request that came in is all good. If you could not get the error for any other reason then it is not found. Maybe uh you did not have write um arbback to get that to get the object in that name space. For whatever reason you could not get the object of the request, you will then say u you will send an error. And you see here the moment you send an error you are telling Kubernetes please retry this object. Please retry running this reconiler please retry the operation of the whole reconciler loop. And this is where the self-healing would work. Probably you had some problems where you could not get um you could not get the you know the object but you try again and it if it works then you're happy because you don't you don't have any errors anymore otherwise you send an error again and this is going to go ahead with a exponential backoff. So it's like when the first request comes in, it was an error but it was not an is not found error. You send uh you return an error. Okay, it goes back to the reconciler again, runs to the reconciler, there was another um there was an error again which was returned goes back to the reconiler and this happens with an exponential backoff. This is where return values of the reconiler function are absolutely critical. Absolutely critical. Now the next thing is um whenever you delete an object you set a deletion time stamp and I will talk to uh you about this in in the future. We are not going to talk about this right now. This is when you delete the object. We will first learn how to create one and then we will delete one. And this also is about deletion. This is the logic of deletion. We'll talk about that later and I'll tell you why this is here. This is the logic of checking if the instance is already there because you want to be at the potent. You don't want to create the same instance with the same instance ID if it is already there. And this is also the logic which I probably would talk to you about later. This also is a logic which I will probably talk to you about later. Um so here is where we start in our loop. The first thing you do is you start your reconider. You create an object. You try to get the object into your EC2 instance variable the custom variable type that you have created. And then you say okay I'm starting completely new. I have no ID. I have no instances on my Amazon and I'm going to create a new instance. The first thing you do when you create an instance is or when you create an object, it's a very good idea to add a finalizer. You might have seen this in Kubernetes. You when you do cubectl get hyphen or YAML, you might see this finalizer in the metadata texture of your object. What this finalizer actually does is is very it's very interesting. So let's say when you created this object in Kubernetes which was an EC2 instance and there you added a finalizer. Finalizer is nothing but it's a list of key value pairs that you can add. Let's say I add my finalizer as hello colon word. Then this object was created and your reconciler actually went to Amazon and give you a new uh AWS instance. All right, everything is happy. You got the instance. Now the the thing that happens with finalizer is when you say I want to delete this object when you say I don't need this instance anymore I want to delete this particular object you can delete this from Kubernetes however that's not the only thing where you need to delete it from you also have to delete this from Amazon so how do you tell Kubernetes that why I am deleting this from an external resource from an external platform. Do not delete this object from Kubernetes. Only when this instance is completely gone from here can only you delete this particular object. That is the role of a finalizer. Finalizers will hold the deletion will hold the deletion of the object in Kubernetes until the actual cleanup has happened. And once you have you know um once you have deleted the object you then remove the finalizer and then Kubernetes will allow you to delete this particular uh EC2 instance Kubernetes object that we have created as a good practice when you create the object in Kubernetes that's where you should add the finalizer and this is extremely important this finalizer is being added to your Kubernetes object. object which is EC2 instance and this also is an update and this is also going to rerun the reconciler loop any update to the object whether you are adding a label whether you are adding a metadata whether you're adding anything really whether you are updating the status of the object in your code that will recon uh that will start a new reconsidered loop so this is where you have to be very careful of a depoency uh in your code and you see what happens. We we print a message and we say I'm about to add a finalizer and I use this um this append function because it's just a key value pair I'm adding in my EC2 instance finalizer because I've already mastered this using the R.get. My EC2 instance has actually the YAML of the request that was given to me and I'm creating uh a key in here called finalizer and I'm appending uh my finalizers here in uh called EC2 instance uh EC2 instance.comput.example.com. I'll show you how it looks. It just creates a new key under your object in your object and then it just um adds this uh this as a as as a list uh there as an entry in the list because you can have multiple finalizers uh in your object. Once you have declared that I want to add a finalizer, the actual way of updating your object is going to be R.update reconciler.update. There are a couple of functions we get with the reconciler. Get lets us get the object of the uh you know get gets uh the get function lets us get the yaml of the object into our variable and then you can also use r.update. This lets you update the object that the reconciler is working on right now. So in this case I'm updating my EC2 instance where I'm actually creating and adding some finalizers and we will see this when you create the instance it will actually give you the finalizers when you uh as soon as you create the the instance and because you made an update on the object it will start a reconciler again. Not right now though this is very important. See it's very important to uh to remember when you have the reconciler let's say when you have the reconciler it starts let's say here you made an update to the object maybe you updated the annotations this is where you will start another reconciler but not right now you will move ahead in your code and you probably make another update in this case you updated the labels of your uh reconciler of your object. Kubernetes also records this as a second time it has to run. Then you do a return and you do a nil. What's going to happen is Kubernetes will run this reconiler twice because you made updates to the object twice. It's kind of like it remembers that this is where an update was. I have to rerun the reconciler. This is what an update was. I have to rerun the reconciler. It is a golden rule of reconcilers that any update to the custom resource whether it was done by you with cubectl commands or whether your reconciler is doing it will start another reconciler loop. It will not stop u you know the current reconciler. It's not like it got an update. it will directly go from here. It's not like that. It will finish the proper execution and then based on how many updates did you make, this is where the reconired loop is going to run again. And this is your responsibility to make sure that um you know when you run this again, this update does not happen. This label does not happen because they're already there. And then you will say, okay, I did not make any changes throughout my reconciler on this object. um I I need to do nothing. I don't need to start the reconider again for this particular uh custom resource. If you make a new custom resource then again yes the reconciler will be started and the loop will keep on running. Extremely extremely important to know about the return types of the reconciler. Now once I have uh updated you know um my object I'm telling Kubernetes please add this finalizer to my object and if I got an error that said failed to add finalizer actually if I got any error um I'm printing an error that says hey I was not able to add the finalizer and I say please recue and I'm sending an error. So this is where you are returning another uh return type. And you see whenever you get an error whether you are trying to get the object, whether you're trying to delete the object, whether you're trying to update the object, you want to retry again. And this is where you will send uh you will return an error. And whenever you return an error, this whole result is completely ignored. It's absolutely absolutely important to understand this. when you are returning or when your reconciler you see here this reconciler function is returning two values one is the result the second one is an error if you return an error the result is completely forgotten kubernetes says you know what the reconciler had an error I'm going to retry that again with an exponential backoff and this is where this is how the self-healing works in Kubernetes Now once you have added the finalizer and this printing an info message for info log which says finalizer was added this updates a new reconider loop execution but the current will continue and this is where we create an EC2 instance I'm just sending a log uh I'm just printing a log continuing with the EC2 instance in the current reconciler and this is the beauty what we were waiting for this is what's happening when we want to write an operator that talks to uh that that creates mere Kubernetes cluster. Guys, this is where it all comes down to. We have our spec of the custom resource. We have the, you know, the logic that listens on the update of our custom resource. We have the logic to get the manifest or think of this as this way to get the YAML of what the user has given in my EC2 instance. Now I need to create an EC2 instance. This is absolutely important. Now it's going to be so much fun. Now when you want to create an Amazon instance, you know what you need. You absolutely need an Amazon account and you need to have the credentials. You need to give the credentials to your operator or to your controller so it can go on your behalf and work on Amazon. And this is exactly what we will be doing. Before we can actually go ahead and create an instance, we need to figure out the authentication. Then we will use a client that we created using this authentication and we will give it this particular YAML and we're going to ask it to go ahead and give me an instance on Amazon. And this is exactly what's happening now in this create EC2 instance function. So I've created an EC2 instance. I've created a function which is called create EC2 instance and I pass the users requested YAML. I pass the user manifest the EC2 instance and let's see what this function actually does. This function which is the create EC2 instance. It is accepting a value of type EC2 instance which is the whole YAML from the user and it is returning two things. First, it returns another um it is returning a variable. It's returning a type of created instance info. See, when you create an EC2 instance, you get a lot of output. You get a lot of data. But we don't want all of that. We only want to um you know when a user creates an instance when a user creates this instance probably they care about the state whether the state is running or not. They care about um you know uh created or not true or false. They also care about the public IP that was it you know um created of was it there or not? What do I have a public IP or not? And for this information I have created a new strct which looks something like this created instance info and this helps me to send back the data from my create instance function. What I'm sending back is I can send an instance ID which is important so people can know what instance ids are there on Amazon uh using cubectl get I'm also sending a public IP I can send a private IP a public DNS private DNS the state that is all I can send from my function and this is the um this is the return type and my function which is going to create me the EC2 instance it is returning two things first is the strct which is the information of the created instance and second is an error um which is I probably could tell the user that I tried to create the instance but there was an error maybe the authentication was a was a problem maybe you don't have enough quota in your region in your account I I want to send I want to uh send them something so they are aware of what has really happened why the request failed to create the instance so we create a logger which is create AC2 instance. This is good. You can have your logs with custom uh log name and this is easier for you to know which file which function created this particular log entry when you do cubectl logs in your operator. Um so I'm putting up a info which I'm saying I'm starting the EC2 instance creation. This is going to be the AMI ID. I'm going to use what the user has given from the spec. This is the instance type and this is the region under which I'm going to create my instance. So the first thing I have to do is to create an EC2 instance client guys. Now it has nothing to do with Kubernetes. It is completely how you create an Amazon instance in Goland. It has nothing to do with Kubernetes because you already have the instance YAML or rather the instance info because EC2 instance is the whole instance that should be created. Now you uh are doing the generic things on how to create instances on Amazon. The first thing you do is you create the EC2 instance client with this AWS client function. What this is essentially doing is it's reading the AWS access key and the secret key from your OS environment variable. You see because you need to give some sort of authentication on how you would talk to Amazon. I'm using the AWS key and the access key and then I'm using the config from Amazon to load the default config with these credentials that I have given. Um and then if I could not create the config, I return an error. Otherwise, if you have the access key and the secret key, um you are able to create a config and then you are returning a new config. Think of this as this function is just returning an EC2 client. And this is absolutely important. You would use this client to talk to Amazon. So you use your access key and your secret key to create an EC2 client. And then this EC2 client is here. Till this you have the authentication to Amazon. Till this 24 you have the authentication to Amazon. Now you need to say hey Amazon please create me an instance with this key in this subnet. This is the minimum count. This is the maximum number of instances I want of of such. Um this is going to be the instance type that I want you to create. And this is going to be the image AMI ID I want to use. These are the input instance parameters. You are creating an instance. Amazon expects you to give certain instance inputs and these are um one of them. There are many other instance inputs that you can give. If I show you, you can tell Amazon what is the maximum count of instances you want, what is the minimum count of instances you need, any u block device mappings you have, you probably can say what is the capacity reservation specifications, the CPU options you can give, is it a dry run or not, is it a EBS optimizer or not. Um, so there are many many different uh options you can give when you are creating Amazon. This is just something when you create Amazon instance, this is the information you give. You can also give security group IDs if in case you want the security group to be used for creating this instance. I'm just keeping it very simple so that we know what's really happening. We are creating our query to create an EC2 instance with these inputs. And once I have my input declared, I'm using my client EC2 client which I created above. And there's a function called run instances. And this is the function from go uh SDK of Kubernet of Amazon that launches the specified number of instances using the AMI for which you have the permissions for and this is absolutely the one that has creating the instance for you. So, so far you created the client, you created the instance uh input and now you have created your actual instance here. Now, for whatever reason, this run instance is going to be returning uh two things. First, it returns the actual output. See, I told you when you create the instance, you get a lot of output. So, this is what's going to be returned. So if you look into this EC2 instance dot uh EC2 run instance output. If I look that on Goland uh here you can see I'm going to look for the run instance output. Run instance output here. And this is what is being returned to you. You are returned. Where did that go? Whoa. Whoa. Whoa. I think I was a bit quick there. Let's wait. Uh run instance output here. So you see you are returned the act the the the growth you returned the instances that were created and this is the metadata. There's something wrong with my browser. Wait. Essentially what you are given is the what you are given is uh where did I go return instance output uh there's a type so you get the instances and this is where you have the instance metadata what is the primary uh what is the private IP what is the instance um uh ID that you that was created for you what region it was running in what zone it was running So think of that as a metadata of your instance when you created that and that's what we are um saving in the result. If in case there was any error because this run instance does return an error as well you will say I failed to create the EC2 instance and then you return the error back to the main program and you say this was the actual error because of which I could not create the instance. There could be many reasons why you could not make one. Um perhaps you did not have the permissions in that region. Perhaps you did not have quotas in that region. Perhaps you used a wrong AMI ID which doesn't exist. Um could be a typo or anything. You just uh are returning this to the user. It's a good thing to return them the reason why uh it failed. So that's what you're checking. If the instances returned is zero, you will just say um uh there were no instances returned for me. And till here if we have no error, we have an instance for ourselves. And this is what has happened so far. our code. We had the client and then we use the run instance function to actually create an instance and it gave us some output back. Till this time this output contains things like uh the state that was you know when the instance was created at that time what was the region the metadata the private IP the private DNS um DNS name by this time there is no public IP however there's one thing important second when you make an API call to Amazon with this run instance function. What you essentially got back in the output is the state of the instance at the time when AWS received that request. It might not be running. You know how when you create an EC2 instance, it goes into uh pending, creating, initializing, then it eventually goes into the running state. At this time you have an instance created for you but it might not be in the running state. It might take some time for the instance to be in the running state. And this is what you want. You want to wait until the VM is running. So when you uh run when you execute the run instance function it creates the instance and gives you back the metadata. What it does not have however is the public IP and the state whether the state is running or not. So it's kind of like you say hey Amazon create me the instance. Amazon says cool I will give you one here's some metadata but you don't get the public IP and you might not be in a running state when you created this actual instance but this is what you say if I got back in my result because you see this one instances it gives you back the list of instances and you are checking if um if there was you know uh an actual instance where I could I did not have any errors and there was an instance created you will just send an info that says okay I was able to create the instance successfully and this is the instance ID that was returned to me you store the result of uh you store the output that was given to you and uh there you can access things like instance ID uh private IP public IP because it's all returned for uh returned by Amazon to you and now we wait for the instance to be running. See, it's a good idea that you created the instance, but it's not like imagine this uh there's this developer and he goes to you and says, ""Can you give me an EC2 instance?"" And you go to Amazon and you say, ""Please create me the instance."" And you get back the p the private IP in your company. you guys are using a bastion host which is available and using this bastion we can talk to the uh VM which has a private IP because you might not have a public IP you might have disabled the public IPs. So essentially what happened the guy asked for a VM you said hey Amazon create me a VM and you got the private IP and you gave it to to him. You never waited to see if the instance was actually in the running state or not. Maybe the instance was created but it never reached running. Maybe there were some problems in the region of Amazon or maybe the instance malfunctioned. Whatever could have happened, you you were not waiting for the instance to be running. You gave it to him and he logs in to the bastion only to find out that this instance is not running. So he cannot use it or she cannot use it. And that's where the problem is. As an operator, it is your responsibility that you create that particular resource. You create the instance and you wait for it to be in a certain state that you want. In our case, it is running. So what I could have done is I could have had a for loop. I could have had a for loop that keeps polling Amazon. Hey, is this instance now in running? Is it now in running? Is it now in running every maybe 5 seconds? That's also doable. Think of this as um a while through and I would say think it like this. So I'm using a while loop check the instance and that's it. This is kind of like my function. It keeps running. I am giving it an input to describe me the instance and I'm describing the instance with this function and I get some responses back. If the state name is running, think of this as a pseudo code. um then you break otherwise you keep running. So you keep running every every 5 seconds or 10 seconds. That is a doable option but it's not a good idea. It's not a good idea because Amazon gives you these waiters that can do this for you more gracefully. A waiter is nothing but it's a construct from the Golang from the go package of Amazon that waits for a certain time um for a certain state to be reached of the instance. In my case, there's a new instance running waiter. If I go to that and if I show you or probably even here, um there is going to be a new instance uh running waiter. Now what this does what this does is it defines a waiter for instance one. This one actually has the logic to wait for the instance to be in the ready state. It does the polling more efficiently compared to me having this writing in my logic uh in my code. So you can define a waiter which is going to wait for the instance to be reaching the running state. You can also give the maximum time for which you want to wait because it's not like if Amazon takes forever for your instance to be created. Um you just say um the you know the checking loop will keep running forever. You have to give some feedback to the user and typically you can give the run max time which is going to be time dot minute and three. So you're giving three minutes that you want to wait for the instance to be reaching the running state. This could be depending on your uh requirement you can you can increase this or decrease this. Every request that you make this waiter will be exponentially uh backing off. So it's it's kind of like it starts from like every 10 seconds and then uh it increases this time out up to your uh given time. So it does it a lot better. You create a waiter and then you use the wait function to ask it to wait on this instance ID. So you are telling this waiter that please wait for this instance for this maximum time for it to be reaching the running state. And by this time if it was not reached the running state there would be an error. And if the error was not equal to nil at this time you will just say failed to wait for instance to be running and the instance could not reach the running state in 3 minutes. Now this is important. You do say the maximum time for your instance or that you want to wait is 3 minutes. However, if the instance has reached the running state in the first 30 seconds, then the loop will stop. It's not like you're going to wait for 3 minutes dedicated even if the instance reached the running state before. It's not going to happen like that. This is why the waiters are quite interesting. They have the logic for it. So, you don't have to deal with with that. This only comes from uh experience when you are using the SDK. These are the things that you can also Google. How do I make my code more efficient? How do I use waiting? And you will get that. Now by this time when the instance has been created, we get the remaining uh information back. We got the state because we were waiting for it to be running and we only break our loop when the running state is there within 3 minutes. Of course, now by this time, Amazon has also associated your instance a public IP. 3 minutes are good enough for Amazon to give your instance a public IP and there then um what you can do so actually um okay this was a bit wrong by this time we are just waiting for the instance to be running we don't have the public IP yet this is where I probably skipped ahead when you are using a veator you were only waiting for this instance to be running and once the instance is running, Amazon will give you a public IP. So you have it running but you don't have the public IP yet because it was not given to you in the output when you created the instance. This is where you will use another Amazon function describe instance. Now you say okay I created the instance I waited for that to be in the running state. Now I'm describing this particular instance. Now I'm going to get my public IP as well. Of course, given if you have uh the public IP allowed in your Amazon account and this is where another request happens to Amazon. So we waited for it to be in the running state within 3 minutes. And then I'm saying calling Amazon describe instance API to give the instance details. I tell Amazon that I want to describe an instance whose ID is what I got when you created this instance for me and uh I want to store this result which is of the describe instance um function and I want to store this into a describe result variable. If I could not describe the instance again this is a very simple go check you will say I failed to describe the instance whatever reason you are having you just give the instance or if you could describe it your result is going to be in the describe result which is again a type of describe instance output. Now when you describe the instance you get a strct back from Amazon. You get some data in in a in a specific strct which we can see it here. You do get some output with describe instances. Wait for that. And this is the describe instance output. What you get is you get the information about your reservations which is the instances on Amazon. And within these reservations, you have the instance information. So if you look at the reservation there, you have the instances that were described for you. So you can call the reservation because I know I only created one instance. So it's only going to have one element in the list because the reservation is a list of reservations. And for these instances which is only one the public DNS name I uh is going to be describing like this. So I print the public IP and then I say the state is state dot name. Again this is returned by the instance str of the golab because if I show you here if I go to instance it will have public DNS. Let's look for that. There you go. The instance strct is returning a public DNS. It is returning me a public IP address and also it returns me the state. Um here the state is of type instance state where we also have an instance state name. So it's kind of like they have created packages for all the other one and here you see you have the name. So you have asked for I want to describe my instance and the input is this instance ID. I store the result. I store all the reservations that was returned to me by Amazon and all the instances inside of it. I know I only have one instance. So I can call it with zero index and tell me the public DNS name and tell me the state of the of the VM. Now here's interesting thing. By now you have all the information you need for your EC2 instance. You've got the private IP, you've got the public IP, you've got the instance state, you've got the name of the instance that was created, the key name that was using and now you can actually um u so so by this you have uh all the resources that you need for your Amazon VM uh to be to to be given to the developer who has asked for this instance. The next thing that you can do is you can get the information uh about the instance. But this way this thing is not needed because we already um uh we already requested the the instance information. That's not what we are doing. However, this is what we are returning back. This is extremely important. See when you have all the information about your instance that you have created, we want this to be returned back to the actual controller. So that this is where we have the instance information. This function is returning me um a type which is created instance info. And I just showed you the created instance info here. Where did that go in my API? This one. So here's a strct uh which is created instance info and that is what my function should be returning. This create EC2 instance should be returning and this is what I'm preparing now. So I got all my instance information in a variable called instance from the described result and then I'm preparing my return type because you know this function is returning two things. First is an error if there was any and second is the created instance info which contains the public IP the private IP uh which contains a public DNS private DNS the state and the instance ID and this is what I've prepared now so once this uh is done we will simply say uh I have created my EC2 instance and there I'm returning my um my return types because I did not have any errors when creat creating this instance, I'm returning a nil and I'm returning the information of my instance which was created. What might be interesting is uh this function called dreer string uh dreer string. What this does is it is actually just dreferencing my pointer. The reason why is we are dreferencing this pointer is because when you talk to Amazon SDK, it is returning you things like public IP address uh which might not be available at that time. So when you're returning this, it might be a uh it is a pointer type but you are returning a nil pointer and that's a problem. So this is this is important that we are able to distinguish between whether it was an empty string or whether it was a nil value. If it was indeed a nil value and you were trying to dreference a nil pointer, that's going to be a problem. And this is essentially why we waited for so long for the instance to actually have a public IP. This dreference function just dreferences my string to return a string which I can give back to my main function. And by this time and by this time I have an EC2 instance that was created for me. Now the create EC2 instance function doesn't just create me an instance on Amazon. It it does return me two different values as well. One is an error which is a good idea that your function does return an error if there was any or it returns a nil so that you can use that error in the further steps. For example, we are using it here to say if there was an error, we want to put that error as a log output of our reconil. So when people are looking at the logs of our application which is a reconciler in this case they will know why there was an error which you know which stopped you from creating an EC2 instance and then you can use this error to be sent as a reconcilers's return value because you remember reconciler uh here it is expected to return two things first is the result of the reconciling function which is the current reconciler and then if there was any error with that reconciler. Now it depends how you are creating uh you know um your your reconciler. Maybe you want to retry after waiting some time you want to retry creating that that easy to instance. And this is why uh you can return an error within the return function. What you're essentially telling Kubernetes is I tried to do an an operation which was in my case was to create an EC2 instance and I could not do that. Whatever the problem was, I want you to take some time and retry that process again. Retry that function again. And this is where Kubernetes will say, okay, I'm going to retry running that reconciler loop. So, I'm going to retry to create that EC2 instance for you. And this is kind of like being done in an exponential backoff. It tries, it fails, it waits a little time, it tries again. If it fails again, it waits a bit longer. And this is how the Kubernetes will be doing its uh exponential backoff with your request. So you're not getting rate limited uh by you know um by Amazon that you keep trying and asking for an EC2 instance every 2 minutes or 3 minutes or whatever your uh reconcile is it waits during the time and it uh it's an exponential backoff. Now once you have the EC2 instance once you were able to create the EC2 instance I'm returning the info as well which is if you remember it's a strct that we created uh probably somewhere around here. This is the strct that we created and this is the information I want from my create EC2 instance function because this is something I want to give to my users when they do a cubectl Jet EC2 instance. They should know the instance ID and most importantly they should be knowing the the public IP of the instance so they can always log and they can start working there. You also would probably want to give them the state of that instance. How is it right now on Amazon it is running? Is it stopped? Is it terminated? Um or if any other state that is you want to update them as well. So we do a very small log. We are saying that okay I was able to create the instance and now I will update my status. If you remember every EC2 object that we create, every EC2 instance that we create has a spec and it also has a status field. This is much like with any other um this is much like with any other Kubernetes object which is where you have the flexibility to tell what status should be. In our case, we are telling the instance ID. In our case, we are telling the public IP. In our case, we are telling the state of this instance whether it is running, whether it is stopped, whether it is you know um terminated or all the other um states that Amazon instances can have and that is where we are appending the actual object. This EC2 instance here if you remember we actually create a variable for it and we got the object from the request that came into our reconciler. So the user asks uh to do something on the EC2 instance custom resource. We got that YAML. Think of you get the YAML from the user with the R.get method and store this into a variable. Now this EC2 instance uh has a spec which you are reading and you are using that information to work on. So this is where the user is giving uh the instance type they want to use. The user is giving what the object is for the storage, what user data they want. that is a spec. We usually use the spec to do our operation. We use the spec of the EC2 instance to create ourselves an instance that the user is asking for. And then the status is for us as a Kubernetes developer to tell what happened with this particular object. And that's where um the AC2 instance status is where we can tell that what is the instance ID, what is the state, public IP, private, public DNS, private DNS. This is all actually is what we have defined in here. You can see our instance, our EC2 instance also has a status truck because the way our actual EC2 instance looks like is it has the metadata for the object and the type and then it has a spec and then it has a status and this is what we will be updating now because we already did an operation. Maybe it failed, maybe it was successful. If it failed, we handled it. We ask Kubernetes to rerun the compiler but if it was successful you want to update on the status and that's what we are doing. So the status or my instance ID the state I'm actually getting it from uh this function. So the instance ID is given to me from this function under the create instance and created instance info variable uh which is having an instance ID and then we are setting up the state the public IP all on the right side of these uh substitutions is given to me by this function and I'm updating the status of my custom resource which was which was you know uh picked up by the reconciler. Now you have associated the output of the function to the status of this uh to the status of this um EC2 instance variable. It doesn't just update it. However, you need to use a function called r.status.update because you actually want to update on the status. If you see here the reconiler has got couple of functions. One because r is the type of e2 instance reconciler. It has couple of functions. First one is the r.get. This lets you get the actual object which is coming to the reconciler. In our case, think of this as it lets you get the gaml of the EC2 instance object which the user has created. Then the other one is also um r.update. In this case you are doing an update on the EC2 instance. We will be uh using this. We use this in case of adding the finalizer. you are adding um on the object that there's an update which is the EC2 instance and the finalizer. You can then also update the spec which is using the R dot um you know using the status function and you want to update on this status of the object. You are not updating on the metadata in here. You are not updating on the spec. You are only updating the status. And that's why we tell our reconciler that we want to work with the status of our object and essentially we want to update the status with this information that we have just added here. So to to sum up that again you use the creates create EC2 instance function we get some information from that and then we are updating our object status with this information. If you were able to update that everything is fine. But if there was indeed an error when you were trying to update the status of this object just say um I could not update you know I could not update the object and then you return an error which will try to reconcile it again and we'll try to re-update your um your your status and if everything is fine we reach the end of our loop and then we just say it's all done nothing needs to be done keep looking for the object updates and this reconciler is all ended for However, if you remember, I did tell you couple of things. If you remember any update that you do, any update to the object, in our case, the object that the reconciler is looking for is a EC2 instance. If you update this or a user updates this with cubectl edit, it does not matter. If at all there is any update to this object, there's going to be another run of the reconiler. It's absolutely important for us to understand. So the way your reconciler is working right now is first thing it gets the object. Second, it tries to create an instance. It then updates the finalizer on the object. Then if the instance creation was okay, it goes ahead and it updates the status of the custom resource. And then we reach the end of the loop. The problem however is you updated the object here. So there was a change on the object here at this place because when you update or even if you want to add a finalizer, maybe you add a label to your object, maybe you add an annotation to the object, it does not matter. Kubernetes does not differentiate what kind of change you did on the object. It says, okay, the reconciler looks for an for an update. You did an update here also when you were able to create the instance you got some in instant information back and that is where the status was updated. So here was also an update. The way our reconil works is it starts from here. It sees that okay right now the instance that the user is looking for it is not it is not there because it's a new instance. Then what happens is it categorizes this. It's a new instance because this object when I say object it's the EC2 instance object. It does not exist in my Kubernetes CD. It's a new object. Then it says it's a new object. It creates you an instance and then you add the finalizer. And this is where you update the status. When you have updated the status, you actually give the instance ID in the status and you can only give the instance ID if you have an instance ID and you will only have the instance ID when the object was created when the instance was created for you. So think of this as what's what's happening here. you caught the instance and then you are updating your uh status of uh with an instance ID and this will be triggering a new reconciler event because I I'm telling you again and again every time you make an update to the EC2 instance object it does not matter whether you update the label whether you update things in the metadata whether you update things in the spec or you update things in the status and the the reconciler will start again and this is where it is your responsibility to make sure your reconciler is ident because what's going to happen then when you reach the end of the loop it's not just going to wait for uh it's not just going to wait for new EC EC2 instance object Kubernetes remembers it when it is running through your reconiler it marks that okay this particular operation which was updating the finalizer this was an update so I will rerun the reconiler it doesn't just stop the execution current one the current will go ahead we think of this as a handler in anible if you know about that it says this particular step asked me to update my object which is the EC2 instance I will run the reconciler once again and then it goes on the fourth step and here as well you update the the status of the custom resource. The same thing happens here. It says okay this operation as well is updating my custom resource. I will rerun the reconil again. So in this case when your first execution happens you will start again on your reconiler to be to be running and this will happen two times because you have updated the object here and here two times when you get a kind of like you know when you update the object the current execution does not stop. It's not like you reach three and you start again. It doesn't happen that way. You will run through the entire reconciler. You will keep noting which operations were updating the custom resource and then for how many times that was updated the reconil would be running and now this is your responsibility to make it at potent. Imagine guys, you created this instance, you got the instance ID and when you run it again, you create one more instance and you update the object, you update the custom resource, then again you will get a new, you will create a new instance, update the finalizer, create this update the status, and then you will go again and you will keep creating instances. And this is kind of like a forever loop. And the reason why it's a forever loop is because the reconiler has no state. It does not remember that the last request is where I created the instance. It doesn't have any remembrance of what was happening with this in the end. So it is your responsibility that once I have executed through my reconiler when I updated the object here let's say and I also updated the object here uh for this object when the request will go again to my reconiler I should be checking the state of the you know what is my current state you have to check the current state and you have to check if This is meeting the desired state. In our case for the EC2 instance, we have to check that. See here we update the status. Here we update the status and we give there is an instance ID. When you make a new EC2 instance object, this will not have an instance ID because it is brand new. But once you run this through the reconiler, you create an instance, you update the finalizer and then you update the custom resource object with the status and there's an instance ID. It is then you can use in your reconilider. You can check if the if the request that's coming in to me for this status of this object is there an instance ID. If there is an instance ID, I already have run uh I've already worked with this with this instance before. I do not need to create a new instance is not needed because this one already has an instance ID. Then you work on that instance. See if that is running that is stopped. You know, you do the drift detection. But at least the new instance doesn't need to be created. And this is essentially what's happening in our loop once we make couple of updates. In this case, we are updating the spec and also we are updating our finalizer. Here we reach there. So we will say um okay I I'm done with the reconciler. I would have waited for a new object but because in the reconciler I did update my status I'm going to go on the very top of my reconciler and run it again. So it's going to start from the very beginning again two times because your reconciler is updating the object twice. Absolutely important. Without this you will be creating a reconciler that keeps on working uh and it doesn't really stop or it doesn't really know what uh what to do. So to understand this a bit better, let's see how your reconsider can go into a loop and do the things again and again and again and how you can stop this. Um and how do we stop that in our controller? So this is kind of like the request that you give. Imagine this is your EC2 instance request that you are giving. You give things such as your kind. Oh, wait a second. So you give your kind here. You define your object's metadata and what you are also defining is the spec. That's what you want the instance to be created as. And right now there would be a status but this one is actually empty because you are creating this in object in Kubernetes for the first time. It will not have any status. it will only have a status once the reconciler has run through its logic and that is the one which will be updating the status. So you feed your uh your object you know you feed your object uh information when you do a cubectl create hyphen f on this object that is then sent to the reconciler and the reconciler logic says I will be creating an instance now this one imagine what's happening here it goes to Amazon and then it gets information it creates you a VM the VM is running here it gets the public IP the state of this instance which is we are more concerned about it should be running that's why we have a waiter that waits until this VM is running and this is the information we get back from Amazon it's a very simple description of what we are doing in our code once we get this and once we are able to create this instance what we then do is we update this particular object in our case we are updating Updating this for the finalizer. So we update the object to add the finalizer here and then we update the status of that particular object. Eventually once the object will be exiting once the object will be exiting your reconiler this whole thing here is kind of like the reconciler. This is your reconciler logic. Um it makes sense for me to increase the thickness so you will see this. This is the actual reconiler that's happening. Reconiler let me increase the size of that a little bit there. So the reconil creates an instance it updates the object and this is the output of your reconiler apart from the Amazon VM that has been created on Amazon. you get your spec back and also one interesting thing is this bit. Now your object has a status because we updated on the status. It will also have a finalizer which I have not added there because I want to keep it simple. But we have a status now. And because you updated the object here, this is very important. Because you updated the object, you will be passing the same object. You will be then passing the same object to the reconiler. And then what's going to happen is you will be creating the instance. And then what's going to happen? You will be updating the object. And then what's going to happen? This is how you will be reaching a forever running loop which is then going to be problematic because the reconciler has no idea that it has created the object already. It has created the VM on Amazon already. It it doesn't have a correlation between what it did and what to be done because it has no state. So essentially what you would be looking for is once you have made changes to your object which is in our case you have this status added there I will change my logic a little bit. What I would say that okay create instance if the object dot status dot uh instance id is blank which is then if it's empty then it's a new object that the user have created when I'm saying object I mean this particular yaml if this does not have a status or at least it doesn't have a status and an instance ID that means it has never run through me. the reconciler has never created an EC2 instance for that and that is when you should be creating a new EC2 instance otherwise if it is not blank then just skip directly do not make any changes whatsoever on the object because as soon as you make a change on the object it starts again and your reconciler would be doing any change that you make in your reconciler to the object of the Kubernetes object it will trigger a new uh execution you will trigger a new loop and that's exactly I have this sort of a demotency done by this particular function. See we already do uh we already get the object which is coming to a reconciler using the r.get get we get the YAML the spec and the status of my EC2 instance in this variable and then I'm checking if the status if the status field is populated and the instance ID is not Z not not blank you remember now right it will be blank if it was a new instance if if it was a new object and for a new object I need to create a new instance but if the object is not new it will have a status it will have an instance ID and then I'm saying if the instance ID is not empty we simply say requested object already exists in Kubernetes not creating a new instance because I've already created an instance for the same instance ID on Amazon that's why you have the instance ID because I was able to create that and there's no need then simply I would just be returning a nil and I'll wait for a new update on this object which is EC2 instance nothing to be done. This bit makes our code adempted. Now here you can be a little bit more cheffy if you want. You can be a little bit more you know uh complicating things where you can introduce a drift detection. The thing is imagine you did create the EC2 instance um which is on Amazon. So here is AWS. You did create the instance and it was in a running state. You get that back and you update your status of the object here. And when somebody will do cubectl get EC2 instance, they will see the instance ID, they will see the public IP and then they will see the status as well which is running that matches your Amazon instance. The problem is if you go outside and you stop your instance, if you stop your instance, it does not update the status of your object because Kubernetes does not know what you did to your instance outside. It just doesn't know that. So it could be your your you know it could be your uh feature in the program in in your software where you can say if this instance ID is not empty that means on Amazon I do have an instance it might be running it might be stopped it might be in some other state I don't know I have it then I will go to Amazon and then check if it is indeed running or not so this goes to the reconciler you will say you know what the instance ID is not empty so I will not make a new instance but I will just go to Amazon and see if this instance with this instance ID what is the state of that so you go there you find it is stopped you then update your state from running to actually stop so you can have a drift detection if the instance was stopped there you also update your status here. This is kind of like a Shepy thing you can do. In my case, I'm keeping it simple because I'm saying the instance was already been operated on. It is there on Amazon um and uh it's also in Kubernetes. I will not do anything. I will not create a new instance. But you can have a drift detection as well where you take the instance ID, describe the instance, get the state, update the state. You know, we we know how to update the state. we did it here and this is going to be your own little drift detection and I think that's going to be interesting to to build. So if you have followed the course till here um I would really encourage you guys to add this functionality as well which I'm leaving deliberately because I don't want to make it too much complicated and if the instance ID was empty uh was not empty in our case we already have an object and then I'm not creating new instance see my program will just back off from here it will not create new instances and this is what this is actually what um what's going to So this is now I'm going to show you a bit of a demo of how all this look like to here. This is where we will actually deploy this um to our Kubernetes environment. And then let's try to create and see this in action. I already have this running. But what you can do is first thing you can do is u make u many fit and you know about this we already did this when we were building the API. This creates the custom resource definition and then you can do a make install. It installs or creates those custom resource definitions in your Kubernetes object in your Kubernetes cluster. I already have that because if I do k explain ec2 instances you take it it knows about my instance which is in the compute.cloud.com and this is the group this is the version and this is what my fields can be for the EC2 instance. I can also do cubectl get EC2 to instance and you can see it's it doesn't say I don't know what this object is because I did a make manifest uh manifest and you can do these things together make install it creates you the CRD it installs it on the cluster and then so far what I what we have built we can run our um reconider using go run cm MB main.go. I'm in the root of my uh project. And now you can see this is what my reconciler is now running. If you go through the logs a little bit, it starts the manager. The manager manages multiple controllers and then you can have more than one worker for a controller. And this is what you see here. We start the manager. We start the controllers and then there are a couple of workers. I only have one uh but you can you can read about multiple workers and you can you know uh create more than one if you have much workloads but for us one worker would be enough. Now to actually create an EC2 instance on Amazon, let me just quickly open up my AWS console and show you how would this look like. And right now I don't think I have any instances which is running. That makes sense because I didn't make any instances. Um let it load. Hello. Hello. Let me try that again. Okay. Uh, wait a second. Now, let me try that again. All right. Probably it was my tail scale that was behaving a bit weird. But as you can see right now I do not have any running instances. Now what we can do is I have an object which looks like the spec that you would like. Here you can see I want to create an EC2 instance. This is the name. This is the name space. And I've given my uh T3 medium my AMI ID that I want to use. It's in the region for Amazon Linux 2. The region is central one. the availability zone. I already have this key pair. I already have the security group. I already have this subnet in my Amazon account because you know you need these things before you can make a BM. And this is how my instance is going to be created. Now because we already have our controller running, as soon as I say please create me an EC2 instance, as soon as I say please create me this instance, you will see your logs acting up. Let me just do it here so you will see that better. Now let me do a cubectl create there. You see automatically as soon as you gave a create instruction your reconcil loop started and this is the logic that we have given from the very beginning that's the log we are seeing reconcile loop started and this is where you get the object uh you create a variable of of that type you get the object and then you see if the instance ID is there or not if it has a deletion termination timestamp nothing is there because it's a new object object and then you will see creating new instance adding the finalizer the finalizer would be added all that we went through will be happening now so let's go through the logs a little bit you can see the reconstru loop was started and then I see the log of it's creating a new instance and then you add the finalizer it's interesting to see this in the object so if you do k get ec2 instance you see this is the eventual uh output you're going to get and there was an instance created on Amazon for me from my Kubernetes in from my Kubernetes cluster that is exactly what we were working through we are able to create an EC2 instance from our Kubernetes environment using the controller that you just have written and you're able to get the information the state is running the public IP is the same public IP that you see here 35.159.299 299 uh 220 and alert 7. It's the same here and it is the same instance ID. Now this time I got an instance ID because the VM was created. But what I'm more interested to show you is um this thing which is going to be the finalizer. You see we get the log on the left side. It says about to add finalizer. You can see here about to add finalizer. And this is what the update um that we did to our object. If you see this here, this would make sense. You add the finalizer which is ec2 instances.computcloud.com and then you do an actual update on the object. And that's the result of this update function. Then once the object was created, once the instance, let's go forward and see the logs. How did they they go ahead. So we say we are creating a new instance. We are about to add a finalizer. We add the finalizer and it says this update will trigger a new reconcile loop but the current will continue. As I've already told you we do an update, we register this so that we will come back and restart the loop again. But we keep continuing. We don't break the existing exe execution right there. So you add a finalizer to your Kubernetes object and then we continue with the EC2 instance creation in the current reconciler and that's where we actually create the instance. Once you create uh once you make a call to Amazon to create the instance you can see here we call the run instance API. The EC2 instance creation was completely successful and this is now where for the first time we get the instance ID. you will only get the instance ID if the instance was created, right? When the instance was uh was running. So we we then make another call to Amazon to get the public IP cuz you don't just get the public IP as soon as you create the VM. It takes a little time for the the public IP to be populated. And we just say I'm calling the Amazon describe API to get the instance details. And this is where you just print. This is just like some debug that I was doing with this with this code. And here you can see we get the private IP 172.31.25.250 which is the same uh in here. If I check that here you can see uh 172.3125250 that is my output. My domain name is uh this is the public domain name. This is the instance IP, the region and you know the metadata that was given to me by the describe instance API. You can see the name of the key that you were using when you were creating this instance. And here you now have the public IP. Very important. Till this we have made update to our reconciler um one time you know we have made the changes to the reconciler uh once which is updating the finalizer. Now we update the status as well. Now we update the status and um what actually happens is now you will be updating things in here. This is the status update. So we do the status update. We update the instance ID, the private DNS, the public IP, the private the public DNS and the state of that which is running. Out of these five things, we are only showing uh four. The public IP, the state, the instance type and the instance ID. Now, it is absolutely important to remember we made the changes to our object. The reconil will be starting again. And there you can see after you made the changes to your um to your status the reconciler was started again. However, this time we saw the requested object already exists in Kubernetes and not creating a new instance. This is where we use or we introduce the item potency in our Kubernetes um in our controller. Um it is missing one log however which is a bit misleading that you might think because we made updates to our object twice this should be running twice and that is absolutely correct. I think it's missing a log. So let's try that again. Now what might look like there's a missing log entry and it could give you an indication that I said whenever you update an object in Kubernetes as many times the reconil loop will be running those many times. So we updated our custom resource once at the finalizer and then we update our uh custom resource once with the status the two times the reconciler load should be running. But we only see the log entry here once which says uh this is the reconciled loop now started and requested object already exists. We should be seeing this twice cuz that's what I've been telling you because we updated the resource twice. But we don't do we don't see that here and I think it's going to be a lot more clear when we see the internals of how the operator is knowing that there was a change and what really happens internally. So let's say you are working on this Kubernetes uh resource which is our custom resource and you do a update or you add this resource whatever you do you have triggered a change on this custom resource. Now any change that you do to any resource in Kubernetes the first one that knows about this or gets to know about that is your API server. This is where you have your authentication. This is where you have your authorization. And once you have gone through the authentication, Once you have gone through the authorization, you know, and also the admission um controllers, the the web hooks, this will be persisting your change into HCD. This is the time where you have made a commit to the ETCD and this is your source of truth. This is your single source of truths and that's where you have added your desired state. Okay, that's where you have added your desired state. Now, as soon as the API server makes an update to this HCD, this API server kind of not really broadcasts, but you can think of that it tells everyone that hey guys, there was an update to this custom resource which is kind um which is you know of kind uh EC2 instance and you know there are Many many controllers in Kubernetes responsible for different resources. For example, one is a pod controller which is responsible for changes into the pod. One is a deployment controller which is responsible for changes in the deployment. One is a service controller which is responsible for the service. What they do? They are only reacting on the resources which they have been programmed for. In our case, we do have a controller which is only listening on the EC2 instance uh type of the resource. So the API server tells everyone. Think of this as broadcast but it is not really broadcasting. It sends an event to anyone who is watching this custom resource. The pod controller watches the pods. The deployment controller watches the deployment. In our case, this EC2 instance controller is watching the custom resource of kind EC2 instance. So this update is listened by this guy or it's watched by this controller. Think of this as your controller subscribes to the API server and it is telling the API server whenever there is a change of uh in this kind EC2 instance tell me the API server registers it that okay there's this guy who is watching and listening for it and then the API server will be telling um once it triggers an update like this once it triggers an event then the EC2 instance uh controller who's watching this update this this event gets to be notified about that. Now if we zoom in in this op in this controller a little bit let me uh create what's really happening in this controller. So this will make more uh sense because I just said the EC2 instance controller gets it. But really what happens here is that this particular controller, this particular controller who's watching, you know, who's watching the API server, the one responsible for watching uh or doing the watch is called an informer. Think of informer as a piece of software that opens a long running or a very long uh living um you know stream to the API server and it always catches these updates that hey guys okay there was an update I am now notified about it. This is the part of your controller. Your controller has an informer which helps you to open a watch to the custom resource that you are interested about. Now as soon as there was from the API server uh it says there is an update the API server sends this update as well as the object and the watcher consumes it. The watcher subscribe to it. And this is how this informer when I say watcher it is the informer. This informer gets the you know the actual update event and then it gets the actual object which is the yaml. This object is kind of like the yaml of our kubernetes resource. Now this informer has couple of uh things to do. The first thing it does is it stores the object. It stores this object which is given with this update event into a cache. This cache is managed in the controller itself. You don't have to do that. Cube builder already has bootstrapped these things for you uh using the contain uh using the controller runtime. There are packages that manages caches for you. And this cache is where you are storing your whole object that was given to you with this event of an update. The first thing that the informer does is always adds it to the cache. Then the informer has couple of something called handlers or we call them event handlers or we call them resource event handler. These are think of this as functions that would be running uh when you make a new add operation, when you do an on update operation, maybe you did an update to your resource or when you do a delete. These functions they are not doing anything except all three. The only thing that they do is when the informer has stored this object into the cache these handler the informer will be triggering this handler and based on what you have done they add this object's key into this working queue into this work this is the infamous work Q that we have been talking about now what's really added in the working queue is not the whole object of your um your of your um custom resource. It's not the whole object. The thing that is added in this working queue uh the thing that is added in this working Q is the name space and within that it is the name of your object which is the name of the EC2 uh instance kind and then you have the metadata. So metadata name that's what's added. It does not add the entire object the spec the status they are not added there. Now once you have added once your handler has added this in the queue then in your controller you have workers or we here we have the reconcile uh loop that we have been working with and there's a worker running this reconcile loop. This worker keeps on looking in this working queue. As soon as there is a key in this working queue which has been added by the resource handler in this case, the worker runs the reconcile logic. And this is eventually how your um Kubernetes you know how your controller how your operator gets to know about that there was a change and so that I have to run my worker. Now during our reconciler we did update our spec uh we did update our uh object two times. One was for the finalizer. So look at look at what's going to happen once you update this for the finalizer. The same thing begins from here. It's kind of like the same process because whether you update your custom resource, whether you do this or an application does that, the API server has no differentiation of who made the change to this custom resource. All it knows there was a change. That's all I care about. Now, the first time when we added the finalizer and we updated the the object, the same thing happened. This update was sent to the API server. Then it was stored in the HCD and then um you know your controller was called because it has the same thing. The informer was there. This informer then triggered a handler and this handler added this particular key in the working queue. And let's give this the name space as default default slash uh EC2 that's the name of my um kind metadata.name name. This is the name of my object in Kubernetes. That happened and while you were running your recon, while you were running your loop, the second thing that you did is you updated, you know, the status for the status. We updated things like the instance ID. We updated the instance ID. Then we updated the public IP. Wherever at any time there's an update, the same thing will happen. The only difference is this working Q is kind of uh single for uh for one controller. There's one working Q. It's shared for that controller. So the hander adds it here. And the same thing will be happening here. So let's say you update the status. Now this update to the custom resource is seen by the API server adds it to the HCB. Then the informers are watching uh they first thing they update the cache locally. They update update the cache locally and uh then the handler is called. Now what was happening this this is where um the one single log line explanation is coming. See your handler is responsible to add the name space and the name of that object in the working cube. But what happened was when this handler wanted to add it, it said I want to add an object which is in the default name space and the name of my object is EC2. This working Q is smart in a way. It says an object with the same name in the same name space that you are trying to add is already in the queue. So I'm going to do something called the dduplication. And this is such a power powerful mechanism because if you did not have that you will be kind of running a reconciler storm. You know, every time you make changes to your object, imagine this while running your reconciler, maybe you made changes to your object 10 times or 20 times, right? You made changes to the object 10 and 20 times. You do not want to run the reconiler 10 or 20 times. Just one run because you already have that the same key is available in the working queue. it's going to use um the spec of that object and be done with it. This dduplication is already handled by this working Q package in Golang which is again we are using this weather controller runtime. We don't see this but this is uh eventually what's happening in the background. So we do not add it again or rather you can say you add it but then it is getting dduplicated. Now once you finish your reconciler see this is what happened you update the finalizer you then create the EC2 instance if you haven't forgotten the flow this is what is happening in our reconil logic we update the finalizer then we add the EC2 instance uh on Amazon now once we added this EC2 instance we update the status And here once we have updated this tariff we are done. The worker is creep the worker now here is free. Now the only thing the worker is ever responsible for is looking at this particular working queue as soon as it gets free. When I say it gets free, I mean the reconcile loop has run successfully, you know, and now the worker is looking for any other changes or it is looking if there is a key in the working queue. These are the two reasons why the reconcile loop would be started or why the worker will run the reconciler again. For the worker to be running this reconciler, there are two uh ways it could do that. First, there was a change made by the user to the custom resource which is what I told you the whole process just now where you make a change to a custom resource the API server and then the worker sees the working queue here's a working queue it starts that or second the worker will be running the reconilers the worker is going to run the reconiler in case you have some changes made uh by the user to the custom resource or your req interval uh is done and then it's time to uh retry that again if in case there was a change uh to the custom resource if not it doesn't do anything it should not do anything it should just simply um be uh exiting the the reconciled loop the third thing third reason why it could run after it is finished is there is an object in the working in the work queue. It's kind of like imagine um you know you are moving bricks from point A. This is point A. You have a brick here and you have a work to move this to point B. This is you. So either you if there is no break let's say if there is no break you wait you wait and every 10 seconds every 10 seconds you see if there is a break there's no break okay I don't do anything this is uh the req interval every x seconds or minutes you are watching but there's nothing there so you don't do anything second one is you know your manager or your owner or whoever that is. This guy places a break in here. He does it on himself and then he tells you I have added a break. Now you get active. You put this brick uh in your hands and you move it to the point B. This one's when somebody has made changes um you know uh when somebody has made changes to the custom resource manually. The third reason why you might move with brick. The third reason why you might move this brick is imagine that um this is point A and this is point B. There was a break. You were watching this here. This is important. You are watching this now. So there was a break. You are watching this. You move this brick there. And while this brick is in the you know think of this as it's transporting here another one comes up another one was added now as soon as you finish moving this brick from point A to point B your work is done you immediately look at here what there's another one you move it again and this is kind of like there was already an object pending while you were finishing your work someone put another brick and As soon as you are done with moving that that break in your hand, you look back, there's another break, you move that. That's the working cube that we are talking about. As soon as there's an object, u your worker will be starting again. And this is why we only see one log entry because when the worker finished creating the instance, when the worker finished updating the status, it's it was done. it was exiting and then it saw that there was a new object in this working queue. In this case, the third um you know in this case the third stage is uh applicable to us. So the worker saw hey I was making uh you know because there was already a key added to this working queue I started working on that but something already added another key in this working queue. So as soon as I'm done with the in you know with the current run of this instance creation I look there and then I run that again. I run the reconcile once more because there is a key in my working cube. And this is why you only see that once because we already have this item potency that if you know for our status dot uh instance ID is not equal to null that means we already have worked on this. This is the adap potency you have to add in your operators so that it doesn't keep running in a forever running loop. Um, we talked about this already, but this is where you only see that once because there was only one key added even though you had two updates, even though you made changes to your custom resource twice, the dduplication happened in the working queue and there was only one object and as soon as the worker is taking this, you know, it reads the object from there. The working queue gets empty because the worker is working on the only available key. It reconciles it, sees the ident potency kicks in and then there's nothing. There's no object in the working queue. The worker is now waiting. The worker is happy. Now the worker will be triggering the reconciler if there was a change made by the user or when your reconcile interval is re and uh you know you have configured your reconciler to to watch for or to ask the API server is there a change to my um object every uh reconcile interval which is I think we have it here. So somewhere around um you know um I think we don't we do not configure a rec uh I think we did not configure a DEQ interval. So this is also a very small piece of information. When you write your reconiler I'm not sure if I've explained this already in in there or not. I think I did but let's talk about that again. For the reconciler you can define a recq after which is if I show that again here in this result uh we can send two things. Uh one is the recue. Do you want the operator to recue after the worker is done working? You know you want to try that again for a new object and what duration after. So think of this as you might want your controller in a in easy words you might want your controller every uh 1 minute to run again even if there is no object in the working queue. You want it to be running every 1 minutes. And this is how you can configure um you know um when you are returning the result you could say vq after um time do 1 second. So this is kind of like making this as a chron job. Now you can create your operators as a chron job or you can make them behave as a chron job by using the dq after you say I'm not returning an error so there's no retry needed. However, after this time, um, you know, rec should happen. So that's that's like making this as a chron job. Imagine you want to, uh, delete the pods which are container creating error state. So you want to scan through all the pods, delete the ones which are in container creating state as a cleanup and you want to rerun this process after um, x amount of seconds or or minutes. So this is kind of like a chron job but your operator can do that as well with this req after. So now that you have a very good idea of how the internals of your operator are working where is the uh working queue managed again the working queue is managed in the operator itself in the memory of the of the controller. Uh the informer is part of the controller. The handlers are part of the controller. This cache is also a part of the controller and this working Q is also the part of the controller. The worker is part of the controller and again the reconcile loop it's all what's making up the controller and uh yeah that's that's something important. Now with this cache you might think why is this cache uh you know created here? What what is the reason of this cache? Think of this as when API server makes a change, it sends the update that I have made a change to the custom resource and it also sends the actual object to uh which is the whole YAML of of of the object of the custom resource. It sends an update event. It sends an update event and the object custom resource or the whole custom resource object. Let's put it. It sends a custom resource object and this event is seen by the uh controller. This event is seen by the controller. Now what's happening once this controller sees this event? the first thing uh or informer inside this controller is the one that watches it. It adds this object into its cache or I should rather wait I will do it like this. This whole object is actually added by the informer in the cache of the controller. And then you know what happens? There's then the event handler. This adds the key in the working Q. And then there is a worker that keeps looking at this working queue. As soon as it finds that there's a key in the working queue uh which is added by the event handler you know the event handler adds this key which is the name and the name space of the object now the whole YAML not the whole object but just the name and the name space the worker starts it now if the worker needs to access the spec you know that's what we are doing in here um let's say if I say create instance and that's what I am or rather if I begin I'm saying it here. Um, see, I'm checking if this object has a deletion timestamp. I'm reading that object. I'm seeing if that object status has an instance ID or not, I'm reading that. So whenever you want to read that object, you know, whenever the worker logic or the whenever the reconciler logic wants to read the object, it does not read it from the HCD. It doesn't go to API server then reads it from the HCD. It does not do that. it reads it from this cache which is which is faster in orders of magnitude you know um compared to when you were going to the API server and reading it and this cache is always updated as soon as this update event or add event you know or delete event comes in the first thing the informer does is refreshes the old copy and updates the new one so that you're recon compenser will always be getting the latest state from uh from the cache of your custom resource object and this is how it's reading the object. The worker reads the spec, the worker reads the status, whatever you do with this object, it's being done from this cache and this makes it really really fast. You don't have to go out of your process or rather the controller to the API server then to the HCD then read the the spec or the status. It's right there whenever you need it. So I think this was a very good idea of uh just explaining why we only saw one line you know one run of the insider not two. I think this was the right time for me to explain this and I hope this is clear to you guys how the API server um up sends an update. How the informer is looking for it. How the watcher and the informer which is one of the same similar things is looking at that. How the handler adds it to the key and then the worker reads the working queue uh and then you know runs the deconsider logic. Now that you know all this, this will be giving me a good idea to tell you how Kubernetes handles the object deletion. We talked about how does it create one. We talked about how do we work with the caches? We work with the informers, the handlers, you know, the working queue, how the reconciler uh does that, how the dduplication works. So now let's talk about how Kubernetes will handle the deletion of objects. This is where the deletion timestamp will be coming into the picture. When you have an object and when you say cubectl delete the object, the reconciler does not know whether you have asked for deleting the object, whether you are updating the object, whether you are updating the finalizer, whether you are updating the status of that object. It absolutely has no idea about that. And this is where how would you differentiate that the particular update was to delete that object. How does Kubernetes know the user is asking for deleting this object? It knows about it by adding something called a timestamp or um essentially it's a deletion time stamp. See if you look at my object now um if you look at my object right now it's less it does not have a deletion time stamp what it has is a creation time stamp what it does not have is a deletion time stamp but I can run this on a loop let me show you that uh cubectl get dogs and And you can see it here. I probably I can just show you the the metadata. So you can see here this is my current object right now. This is my current object that I have created. And you can see my reconciler is happy. It knows about this object that it was already created. It is on Amazon. And you can also uh the cubectpl get EC2 instances and you know the public IP and everything that's that's okay but when you want to delete this the way Kubernetes knows that this was a deletion operation is by adding a deletion timestamp and this is what our program can actually look for. So you can see here this is what the program uh could could look into. Um here that's where you can check if it was a deletion request or not. So as soon as your reconciler is started it doesn't know the reason why it has started. Was it an update? Was it a delete object you know delete um operation on the object? And this is where you can use this deletion timestamp. If the deletion time stamp is zero, then it's not a deletion request. However, if the deletion timestamp is not zero, that means you can say the user has actually requested a deletion, the instance is now being deleted. And then you can call Amazon to delete your instance, clean up properly. And once only you have done the deletion then you can remove your finalizer. And this is how finalizers are used when you are deleting an object. As soon as you give a request to delete something finalizer will hold the deletion uh of that Kubernetes object until the actual resource has been terminated. And if you were not able to remove the finalizer you do not delete that object from Kubernetes. you know, you just say try again. You keep on doing it and once the instance was gone, then eventually you let the you let the finalizers be removed and you know the object will actually be gone. Now this is what you will see in the logs right now. As soon as I will do uh let me go to the directory and as soon as I will do delete on this EC2 instance. I hope you can see this in in the you know in the size of the font. What I essentially want you to look at uh to look at is as soon as we give a delete request to Kubernetes there will be a new deletion timestamp and because it is an update to our object the reconciler will be started then the reconciler will know oh it has a deletion timestamp so basically I need to delete my instance and then delete it from Kubernetes. So if I do cubectl uh delete, see what happens. It got a deletion timestamp automatically. My object was updated and then my Kubernetes reconcile started again and it saw oh wait it has a deletion time stamp and you see my object is not deleted yet. It's waiting. It did not give me the response back. It's waiting because I have a finalizer that is holding the deletion. So you see it says has update deletion timestamp instance is being deleted. Then we call the delete EC2 instance function which I'll just show you the instance termination was initiated and it is waiting for the instance to be terminated which you go here. You will be seeing the instance is no longer running because it is right now shutting down. This is the instance that we just created. It's right now shutting down and I'm waiting for my object is waiting until it goes into the terminated state as the other instances are in terminated. We want for the instance to be properly terminated and once the instance is now you can see it's terminated. See what happens. We're waiting for the instance to be terminated and let's just give it a little time to actually uh the maximum time it's going to wait is for 5 minutes for deletion. This will be then updating and removing the finalizers for me. Um, and eventually that's going to be then cleaned up. And that's what happened. I only was able to delete my object once the instance was terminated. You see here waiting for instance to be terminated. it was waiting and then once it returned the terminated state we say EC2 instance successfully terminated and again uh because you deleted this object now it is another update on the reconil. So you can see the reconiler has started again. So see what happens again is this is very interesting any update that you make it starts. So what happened is what happened is you have an object you do a deletion on this object. This object gets a deletion timestamp and this is seen as an update by the reconiler. The reconciler runs, it sees, okay, it does have a deletion timestamp. So, I need to talk to Amazon to delete this particular instance. I need to delete this particular instance. And once it is deleted, you know, once it's deleted, what I will do is I will remove the finalizer and then let the object deletion be done. And here's where you're making an update again. Now, see what happens. Um, if I go back to our code, how it works. You can see we delete the EC2 instance. This is a very simple delete EC2 instance function. All it does is it says deleting the instance and then it runs the terminate instance function for terminating the actual instance. It waited it waits for it to be terminated. This is quite similar on how we were doing a running waiter. we have a terminated waiter and we wait for the waiter to actually return uh terminated and then we say instance was terminated just fine. If we were not able to terminate that if there was an error we try again but in our case we did not have any error that means we were able to terminate the EC2 instance that means this cleanup has happened now I can remove my finalizer and this is again an update. This will again start the reconciler. Absolutely uh important. Any update that you make, it's going to start the reconciler. So you remove the finalizer using the control uh using the controller runtime uh using the controller utils and you say please remove this finalizer and then you update the object and there you see as soon as you update the object Kubernetes says cool I will go back and run the reconciler again and at this point the instance is terminated and the finalizer is removed. We go back to the beginning of our reconciler here and that's what you see now in the logs when we were able to terminate the instance. Uh because we updated the finalizer at this location, the reconstruction loop started again for a particular um object. But you see here uh it's quite interesting between the time when you remove the finalizer between the time when you remove the finalizer when you were removing or you removed the finalizer which is registered as an update to the object. Uh, and a new run of the reconciler. New run of the reconiler. Your object in Kubernetes, your object in Kubernetes is actually gone. It's deleted. Think about this that the custom resource for which you remove the finalizer it's um it's UID because every resource in Kubernetes has a UID think of this as a it has a UID was a B c then what happened you removed the finalizer and then um you updated the object um you updated the sorry let's start that again so between the time when you remove the finalizer which is triggered as a which is registered as an update your object was actually deleted but the reconciler says because you updated I'm going to start the reconciler again from the beginning and I will start it for an object whose UID is ABC it's a it's not a new object because you updated an object which existed and then it was not existing anymore but the reconciler does not know that it is deleted. It just says I'll start the reconciler again for the same object. And this is where you have to tell the reconciler that even if you are starting again if you try to get the object and you get an error you will get an error because there is no object that exists with that ID that you are trying. But if the error is uh it's is not found you know when you do cubectl get uh pod xyz you you get the pod is not found it's kind of like that error simply just say okay it's it was a cleanup I will not do anything and I'll just wait for the new uh request and that's what's happening in our logs it started again you updated the finalizer the object was deleted the update was registered the reconcil was started from the for the same reconciler object you know but it is gone now and that's where we are handling it we say otherwise the reconciler would have said um not found not found not found we want it to be handled saying the instance was deleted that you know um you are trying for if you cannot find this object it's okay because that's what you get not found that was the error that is not found and that is the entire end to end functionality of our controller. This by this time you know how to create resources on Amazon. Your controller is able to ignore or have a then potency. So it doesn't keep running the loop again and again and again. It can handle the termination of instances. It can handle the finalizer as well. And only when you remove the finalizer, it will clean up or it will let Kubernetes clean up the objects and subtly ignore if the object you know for which there was an update that was uh registered and the rerun of the reconciler happens but the object is gone. Meanwhile, we handle it that if you try to get the object and the error that you get while getting the object is uh is not found. Kubernetes has a package called errors which is where all the errors are defined. You see um is already exist or uh is not found. Sometimes you create an object and you get Kubernetes says this already exists and you can use these uh error types in your program. I'm using one which is called is not found and this is what it says. is not found returns true if the specified error was created by new not found. So you are trying to get an object but you get an error because it doesn't exist and the error is is not found. So that's what we are doing. If it's not found simply say okay the instance is deleted no need to reconcile and I'll wait for new objects to be coming up to me and that is what you will see this loop now will only run when a new object of EC2 instance type is created or updated let's try that again I will do it that please give me a new instance because I don't have this object in Kubernetes it's going to be creating a new VM form. So let's do that and you will see this in action again. Create and you see on the right side the reconcile is started because there was an update which is create on our instance and this is the same thing happening. We see that this object is new. So we create the instance again. We add the finalizer and then the finalizer was added and then I call the Amazon API to get me an instance. I describe the instance, get the public IP and I update my status. You go back to Amazon and you will now see that there is an instance which is running. There we go. This is the instance which is now uh you know it's in the running state and we had a waiter for it to wait for the running state uh to to uh for the instance to be reaching the running state. And this is the same public ID and this is the same uh thing that we see in here 3647 52 and the same instance ID. Now um I will actually um I wanted to show you something which is again that's what's happening here. We do get the object uh we do get the object and as soon as we update the status the recon starts again and it says the requested object is already existing because you have an instance ID. I will not do anything. Try the deletion again just to see it's working. I do a deletion. This time I will get the deletion termination timestamp and we will clean up the Amazon instance. Then we remove the finalizer, remove the object and handle the next run of the reconciler by saying the object was deleted. It's okay. Nothing is required to be done. So you see it got started again. It found the deletion timestamp. That means the instance is being deleted. See, you will not have the deletion timestamp if you do not delete an object. I showed you in the previous examples. It was only having creation time stamp. But as soon as you add a deletion timestamp, that is a cue uh that you are trying to delete an object and that's what we're looking for. If your object has a deletion timestamp, that means it should be uh terminating. And there you can see my instance state is changing from running to shutting down, which is what we saw before as well. It will wait for some time for that to be terminated because we're using a waiter. Golang uh the Go SDK for for Amazon. lets you wait using these waiters. Instead of you waiting for two seconds, then pulling again, then pulling again, there's a waiter that lets you do it very very easily. Or you can also do uh kind of like long polling if Amazon supports it. I'm not sure, but you can do a periodic polling that you wait for 5 seconds up to 5 minutes and then you pull if the instance state is terminated and blah blah blah. But you see here my object is holding the deletion or Kubernetes is holding the deletion of my object. I'm waiting I'm waiting for the finalizer to be removed and the finalizer will only be removed when the object is is cleaned up because of our logic in here. We wait for the object to be deleted. If the object was properly deleted then only I remove the finalizer. Otherwise, I just send an error and I go back to the beginning and try that again. Try the deletion again. And because I would be able to terminate my instance, it's waiting a little bit longer. And these are um this is the beauty of using a waiter. It's not like you are waiting for dedicated 5 minutes. If uh using a waiter with with the Golang SDK of Amazon, if the instance is terminated even before, you don't wait for the entire 5 minutes. it's more efficient waiting for the resources and then you can see now it is terminated successfully. Uh so you remove the finalizer that triggers an that registers an update your object is deleted that registered update start the reconider loop again and then you say cleaned up no need to reconcile. This is the beauty of Kubernetes operators. What this shows you is that you are able to manage your Amazon instances right from your um Kubernetes environment and this was entirely that this course was about you can make this more chessy as I said you can have bit of a drift detection or in in my case I didn't have it because I want you guys to you know build it in your own program where you can say if the instance is stopped on Amazon It updates my status of my uh of the Kubernetes object from running to stop it clean the public IP that all but this is a very very good example of uh using Kubernetes as a platform to manage other platforms. You can use Kubernetes not just as a destination platform for your applications but you can use Kubernetes as a platform to manage your resources on any other platform which is which is the beauty of Kubernetes expending it or extending that using the operators is what you can do. So this was the entire demo. This was the entire code that I want you guys to try again. And now let's see how you can package this properly with Helm and how you can actually run this inside of Kubernetes because right now this one is running on my local computer. It is using the cube config environment variable to actually connect to my to my Kubernetes. But let's package this using helm and then see how you can ship it and you can run this inside of kubernetes and uh let's let's get started there. So now that we have seen our application our controller is running end to end and it's able to create the uh you know the ecto instances. Um the way it is running right now is there's my computer and I'm running um the go I've installed go there and I'm running it with go run. Here's where the reconciler is running connecting to the Kubernetes cluster using this cube config environment variable. There are a couple of environment variables as well which is the AWS access key and the secret uh key which is used to connect to Amazon and eventually create an instance because I need to authenticate to Amazon and uh from there uh we are able to get ourselves an instance and this gets reflected in my Kubernetes cluster. The thing however is you're not going to be running this application here. Essentially an operator runs inside your Kubernetes cluster. It's running here as a pod as a generic pod that has access to uh the credentials needed to talk to your Amazon environment. This pod is running in Kubernetes with a service account. Now it's quite important uh how arbback plays a role whenever you are writing an operator. Imagine this is a name space called um uh name a give it any name space and you have another nameace under which your operator is running. Let's say this this nameace is actually called bidding. you have a team whose name is bidding and they do bidding for for clients and um they are using your custom resources which is the EC2 instance. So they create a object of EC2 uh EC2 instance. Now to let your operator know that in this bidding name space there has been a change in the EC2 instance object because your operator listens um on these uh on these objects and changes you need to run you don't you actually need to give the access to the operator pod which is running with a Kubernetes service account. So you need to give the service account access in this name space to be able to list get you know the the basic Kubernetes arbback. You need the service account to have access to these name spaces for the object called EC2 instance. And you would be needing to give this service account access to both read of this in of this object and also to write for that object because you need to update the status of this EC2 instance. So both of them are needed and this is how your operator will be able to manage this namespace or at least manage the object EC2 instance in this name space and be able to go to Amazon create uh an instance there and eventually update this EC2 instance status giving them giving the billing theme the public IP of the instance which was just created for them for that to be running inside the uh inside the name space for this operator to actually be running in the Kubernetes cluster. We need to build a Docker image and this is no-brainer. You saw this coming miles ago. We need to build this image and you will be pushing this image to a repository and from there you will be creating a deployment in Kubernetes. you will be creating a deployment in Kubernetes that uses this image. Um and then you will be deploying this pod which is the operator pod. You also need the credentials here. You need the AWS access key and you also need the secret key. Now you can also create you will also then need to create a secret in Kubernetes reference the secret in this particular deployment and then roll out that pod. So eventually the pod has the logic for creating our instances and managing them on Amazon and then it will also have the right authentication um artifacts needed to talk to uh to to Amazon. For this building of your image, there is a make file available from um cube a make file available from cube builder which is very very simple. So let's let's see how this works. This is a make file in the project which is from cube builder. And first thing you will need to change here is the URL of your um image where you want to uh be able to push the image where you want docker to tag this image and eventually push it for you. For me it's my uh Docker Hub repository and I think I'm keeping this public so if anybody wants to use that um they can and this is uh the Docker image and it has lots of targets available. You have this make file and you already used it for creating your manifest when you update changes in the API spec you had to regenerate the manifests you know creating the custom resource definitions. uh you have some uh you have some targets for testing your application environment. You also have some llinters available. And here's where things gets interesting. You can just say go run what we have been doing. Go run uh cmd main.go. We can also say make run. So it's kind of like an alias. It generates the manifests the the boilerplate code. It formats your go code and it also runs another um bet. What is it? uh it it runs the go with against your code. What we are looking for is the docker build. It has a target called docker build. Essentially what it just does is it runs your container tool. For me in for my case it's docker. So it will be running docker build hyphen tag and it's giving me the image tag which is what I have declared above here and essentially it builds me a docker image with that particular tag and then I can do a docker push to push my image to a registry. Now this goes without saying that your kubernetes cluster will need access to this container registry because without that they will not be able to pull the images. You also can build images from multiple architects. Right now, I'm only building for ARM because I'm running this on Mac and my Kubernetes cluster is also running on Mac. So, it's all ARM for me. But you might be building this on your Mac and you might want to run this operator for an AMD machine. Uh you can use a Docker buildex target to build it for different platforms and then generate a single manifest and be able to deploy it there. This make file makes it very very simple for you to be able to build your images for your platform that you are running with or for crossplatform as well. So let's do that. I will do make docker build. And what this does is it's building me the docker image from my main branch or this repository which is the EC2 operator. Now this is where it takes a little bit time. You see it's building it for Linux but it's building it for ARM 64 architecture and this is where we are building our source code into an executable binary which is going to be called as manager. So let's wait for that to finish and once this will be done you can see the Golang version we are using is 1.23. If you want to see the docker image it is very uh minimal. You are using the Golang 1.23 as your builder. You copy your go mod. You copy your go sum. Set up a working directory. Uh copy your APIs internal your main.go and eventually you go ahead and build your manager because this is the one which is running your controller your controller manager from the disase images. You just execute this manager binary which you have built with go with a user 6532. So it's a nonroot user which is a good thing. you you always almost want to run your container images with a nonroot user um for security reasons and once your image is build I can simply say make docker push and this is going to push my image to the registry I've already pushed couple of few layers because when I was trying with this course I have it and now this one is pushing your container registry your image to the registry if I want to see this uh let's Go to dockerhub. Can I see that here? Of course. So hub.docker.com and there will be the image. Search for my username and there will be couple of images I have. What was the name of the image? EC2 kubernetes operator. Here is so this one is where I have uh the tag which is only latest. You can have a CI/CD pipeline if you are storing your code in GitHub. You can use GitHub actions to always update your images in case your API spec is changed. In case your main main go files changed or you know your internal folder which contains the actual controller logic has uh been updated. You can trigger a new build and then you can trigger a new deployment. With this thing uh aside we have our image. However, these artifacts still are needed. We need a deployment. We need this secret for this deployment to work. We need a name space. So, building this image wasn't as big as a problem because you need to have quite a few of resources here. You also need Kubernetes artifacts for the rolebased access control. you need to give the service account running this pod access for EC2 uh EC2 instance resources on the cluster level because it should be able to work in any name space um at least for this object only. So this rpack is also required. So you should be getting where we are going with this. We need something to be able to ship this application for other customers and that's what we will be using Helm for. You can create a Helm chart which will be you know one of one of the things that I wanted to do with this course is a Helm chart which shows an end toend delivery of this application. Helm you can do that yourself. We have the Helm init command. It makes it very simple for me to create a Helm chart. Then you will update your deployment to set the environment variables from the secret. You will create the secret. It's simple. However, there are two ways in which you can do uh uh in which cube builder can actually help you. The first one is you can do u make uh build installer. There's a target called build installer. And what it does is it reads your um make file. It reads your make file. It reads the image that you have. It then generates a file called dis/install.yamel. And if I show you this file um what this looks like, it's a new file. And you see it has all of these artifacts which is needed for your application to be uh deployed in Kubernetes. So it creates a name space called EC2 operatory system. Then it has uh the the custom resource definition which is our EC2 instance. Then it has the service account um which is going to be running our actual pod. Then you have a couple of roles. You have some cluster roles. You have some cluster role bindings. Um at least it lets you be able to create update delete in this API group for this resources. Cube builder really helps you to be able to bootstrap your um deployment strategy. So with a make file with this target you can create a single deployable unit. And here's the important thing. It gives you a service as well as it gives you a deployment here. This one you see it's using our image that we just has uh that we just had pushed. It has couple of uh livveness probes. It has some uh readiness probes as well. In our program, we did not create an endpoint at /halth. We do not have a livveness probe. We don't have a readiness probe because I wanted to keep it simple focusing on the operator. So, you will probably be removing them. So, uh getting rid of you know the liveless probe. It's here. And then you will be getting rid of the readiness probe. when you are actually deploying this for production, these things are really really good to have. So you can check the health of your uh you know you can check the health of your um operator. Now for this run container because you also need some environment variables. So you will see uh you can say environment and you can see see here uh you can have these access key environment variable and secret key environment variable and it's coming from a secret called AWS credentials and then in the end you can actually append uh here and you can create API version secret there you go some random data is being spilled but that's okay this will give you a complete deployable um YAML file which you can just do a cubectl apply hyphen f and be able to deploy this application. However, there's no version control on this file as we would be able to do these things on a helm chart. Somebody who wants to deploy this controller, they need to know the very layer the lower level details of where to create the secret, where to update my AWS access key and secret key, where do I update my uh controller parameters in the deployment. So that is still a problem and that's where cube builder helps you uh instead of giving this big file which is a single deployable unit. You can use cube builder edit command. What it does is you can tell cube builder that I want to use the plug-in helm/v1 alpha. Essentially this gives you a helm chart created to deploy your operator. And if I do that, you can see here generating Helm chart to distribute the project and you don't have any web hooks created which we discussed in the beginning. So it doesn't do that. However, it gives you all these rolebased access controls. It gives you all the templates for your um deployment uh for your service account for your services everything in the desk chart folder. And this is how your Helm chart would look like. If I want to show you this one is created here. So the name of your chart is EC2 operator which is also the name of your project. Um and once you have this you can look at the templates where the event with the actual uh resources Kubernetes constructs are created. You have them for CRD which is the actual custom resource definition. And of course we want to be able to deploy that uh the assert manager for the issue. So what you see in the template folder is couple of uh resources created which is the cubernetes construct. We have the CRD which is essentially what we want to be able to deploy. We also have search manager which is going to give us an issuer certificate. Um and this is part of web hooks in case we were using any we want to use manager for that. Here's where the interesting thing is. This is the deployment of your manager. This is where you will be using a values file to define your um to define the values for your um for your resources. You also can see if you have metrics available for the service. If you have metrics available uh from your operator in case you are sharing them for Kubernetes, you can create some network policies. You can have uh service monitors. And then here's where a plethora of arbback rules which are created for you. So this makes it very very simple for you to ship your uh project without you doing a helm in it and actually creating all these resources by yourself letting you easily control the behavior of your operator by the single Helm chart with the values file. So the values file which is shipped with the helm chart that we just built with cube builder. It controls the deployment of my controller manager that I'm shipping up with my operator. So this defines how many replicas do I want of my controller manager, where's the image coming from. And this is something that you pushed with uh with docker. So if I do docker push, this is where the actual image was was pushed. So let's take that and uh let me change that here. Now there are a couple of arguments available to your controller manager. Uh I don't need the push so that can be taken out. There's a couple of arguments available. Uh the first one is leader elect. This is something you would be using in case you have a leader election where you run multiple you know replicas of of your controller. In our case we are just running one. So it doesn't make much sense for us. And there are two arguments which is metric bind address and the health probe binding address which we will talk about in a little bit. This is a standard concept of Kubernetes where we define how much limits for my CPU and my memory the application would be needing. And here's where uh it is really really interesting. So usually when you are building an application, it is your responsibility to run an HTTP server inside of it. If in case you want to use the HTTP get type of livveness probe or the readiness probe and it is your responsibility to create a endpoint for example in this case it is the health endpoint and so is the ready endpoint as well. Usually you write the application, you make these uh you make these API endpoints available and then you tell Kubernetes that check my application on this port number and on this part and see if you get a response which is 200. If you do that within this uh period seconds and the delay after the initial delay um my application is is live, my application is ready. Otherwise do what you need to do in case when an application uh fails its readiness probe or the livveness probe which is either you stop sending traffic to it or you kill the container and redeploy that. We did not create any sort of API for the health and the readiness right now and that is the beauty of this operator framework that we are using which is cube builder. These API endpoints are already available to you in uh you know uh in your controller. So you can make use of them right out of the box using the readiness probe and the livveness probe. And this is where I'm configuring that my health probe binding address is any address in the container and it's on the port number 8081 on which my health and the ready probes are running. It's also important to understand that you get some metrics out of it uh out of the controller manager already when you are using um when you're using cube builder to write your own operators. You don't have to implement the logic of how would I export some metrics of my application. It is automatically done for you by cube builder. Of course, it's a very limited set of metrics which we will see. We will explore what kind of metrics there are available. But it makes total sense from the controller's point of view if it's working properly or not. Uh how many times it has reconciled, how many times it has failed, how many times the reconcile loop is successful. All of that is right out of the box for you to use in your uh operator. Then there are a couple of uh security related contexts that we don't want to run our container as a root user and the service account name we want to use. And um um and here's where things gets interesting. See when you are working with Kubernetes you usually in the same cluster let's say this is our Kubernetes cluster in this cluster you create a name space for your operator and then there are uh you know this is a customer name spaces so here let's say I give this as EC2 operator this is where my operator usually would be living which is running as a pod and here is where I will be creating um my object which is EC2 instance. Now if if a developer is creating this if a developer is creating this object in their name space the pod or my controller I should say should be able to react on this change because that's the object that my operator should be listening into rather if there was any number of name spaces anywhere in the cluster if the EC2 uh EC2 to instance is actually created or deleted or updated. My operator should be able to see that change and this is why my operator pod is running with a service account. I will need to give access to this service account that this service account has a role uh and a role binding or a cluster cluster role binding which allows the service account to list, get, update, patch, watch, delete the changes happening on this particular Kubernetes object. That is absolutely important. Otherwise, you will only be able to create your instances in this ob uh in the same namespace. But that's not what we want to do. A pattern for Kubernetes is you create your operator in a different namespace in a dedicated name space and you let users use that operator in their own um name uh in their own name spaces by creating the object on which this operator would be listening onto. And that's what we are doing here. We want to enable all the rolebased access control needed which is again coming from the templates and are back here. All these rolebased access control roles, role binding, cluster role and the cluster role bindings that are required for my operator which is running with this service account to be able to list, get, patch, update all those Kubernetes related uh constructs that I can do on an API endpoint and I want it to be allowed. Otherwise it would be you who have to figure out what roles I need to give to my operator. What role bindings I need to give to my operator. What what you know on the cluster level I need to do for allowing it access on the name spaces on the EC2 operator resources. So this this helm chart from cube builder really makes it simpler for you. You can also control if you want to uh enable the custom resources. So this helm chart does not just deploy the controller, it also deploys the custom resource definitions for you. And here's where you can say enable true that yes, I want to deploy the custom resource definition as well. And I want to keep them in case someone does a helm in uninstall for my chart. See, you will be using this Helm chart to um you know u to deploy this operator. You would be using this Helm chart to deploy this operator. Now you might decide that I want to uninstall but what to do with that CRD? Would you like that CRD to be here available? So somebody could also deploy an operator maybe manually you know creating a deployment at least your cluster would understand the custom resource definition or you also want to clean this up. This is the flag where you can just use uh it will be keeping the custom resource definition or it will be deleting the custom resource definition. There's also matrix available as I said the the operator that you have written with cube builder it comes with pre given metrics available. We will explore these metrics and you can say that you want these metrics to be exposed or to be uh to to be uh accessed from uh from outside the pod and for that what it does is it creates you a service in the name space. So if I go to my templates and if I show you here metrics and here's what it's doing. If values matrix is enabled all it does is it creates you a service type of resource in Kubernetes and the target port uh the port on the service is 8443 and the target port is also 8443. However in our case the metric port is listening on 8080. So I will change that here to 8080. So this will create me a Kubernetes service type which will be listening on port 8443 and forwarding it to my pod at port number AL. This is used by um the Prometheus service monitor. Again my cluster does not have Prometheus installed but if it it would be installed. Uh this enabling uh of the Prometheus key will be creating a service monitor which then uses this particular service to see if the pod is running or not and to scrape the metrics from it uh just to show you uh in in Prometheus and then you can have a dashboard available on that using uh that Prometheus as a data source in your graphana. Pretty straightforward stuff. And here's where we have access to uh controlling in in case we want the search manager injection to our web hooks or not. Right now we are not working with any web hooks. So I'm just going to keep that as disabled. And I'm also not using any network policies. So I would be disabling that as well. You probably want to allow this if in case you want to have metrics and those metrics should only be created by Prometheus running in a certain name space and you can use metrop policies to control um that behavior. Now once you have this we can deploy this uh helm chart but it's missing one thing. See your board is responsible to go to Amazon and then create resources on top of that which is which happens to be an EC2 instance. You need access uh to Amazon. In that case you need the authentication. Now when we were doing this locally which I explained to you when we were doing this thing locally I had my Amazon environment variables already exported but right now my pod does not have them. The code reads them from uh from environment variables but I also need to set the them in my pod. So you have to set some environment variables for the AWS access key and secret key so that you can authenticate to Amazon. And that's what I had already done in my shell. If I show you uh env for AWS and you can see these are my access key ID and this is my secret key which again by the time you are watching this I already would have disabled them because there's no way I want those keys to be um uh to be exported publicly. Now once you see this what you can do there are two options for us to pass these environment variables into our controller. The controller is created by a deployment which happens to be uh here. So this is the deployment which is responsible for deploying our controller which is using the image that we have given which runs the manager command that's being set by the docker file when we build this container image. And here's where we can define some environment variables. Um, you can create a secret. So you could do something like this. So I would say AWS secret.gamel. And this is going to be kind of secret. And you can see you can create a secret called AWS credentials in that name space. And then type is opaque. And you have your AWS access key and the secret key. Of course you will put them as plain text. And then you can refer that secret. For example, in here uh I can say access key ID and secret key ID. That's one way of doing that. And that's probably a better way because you have your sensitive data in a secret. You use that secret into the deployment and then uh you deploy the application. And eventually it's going to get the secrets uh from uh it's going to get the sensitive data which happens to be the access key ID and the secret access key from your secret and then the code will be running fine. That's one way of doing it. Another way of doing that is which I'm going to do and that's a little bit um that's that's quite wrong. We should not do that but for the demo I'm just doing it. This deployment that is created by the helmchart, it reads the environment variables from values.controller manager.container.env. So I can actually set some environment variables like this. This is my AWS access key ID and this is my AWS access secret key. Of course, this is something you would be creating a secret for and then referencing which I just showed you. But for me to keep it simple, I'm just um um showing you there's another way of doing it which is a bad way. But um you have been warned. So be very careful about uh controlling your access key ids or the secret keys. You should never never put them in plain text in your code or in your Helm charts or in your values file. You should never do that. Probably in this case when you are you know on a journey to build an operator, you already know about the external secrets operator. uh uh project and that's what you would be using to read these secrets from a secrets manager like vault like Google secrets manager or they have integration with other things as well. Now once we have this once we have our controller we define the number of replicas I want we define the environment variables we have the right image repository we define the liveness probe and the readiness probe and everything else it's time to deploy this Helen chart to our cluster and for that I can just go to this chart because that's where it is created this is my values file I can say um let me just see if I have any errors in my the file somewhere and end probably not. It looks good to me. This is the range again. This is the end of the if condition. Looks good. So, let's do that now. Helm uh install of I have no EC2 instances. I already have the custom resource definition because I was trying this Helm chart. But let's delete that as well. uh delete the custom resource definition. So that is deleted. My my Kubernetes resource does not understand. It could not find the requested resources. And uh let's do that now. So Helm install give me uh install my EC2 operator Helm chart. That's the name of my Helm chart. I want you to create the name space. The namespace named is EC2 operator which is already existing but if it's there it doesn't do anything. So there's no problem. It's kind of like in a temp potent field and here's the values file and dot would be my helm chart that I have just created. Now uh let me look at the pods and you can see the pod is running now which is using our image. If I describe that pod here, we can see we have the livveness probe, we have the readiness probe, we have the environment variables as well. And this is running uh we can do k get pods. It is running on this particular node. Now I'm using k3d and I have got some nodes available. these uh there's one control plane, one is master and then there are uh two worker nodes that I have. All of them are actually docker containers. If you remember when we were setting up the environment for our uh you know the development environment, we are using k3d and if I do docker pfs gp for agent zero, you can see k3d ec2 operator agent zero. That is the same name of my uh worker on which this pod is running. What I want to show you is if I exec uh into uh docker exec husband it uh sh if I exec into this container um I can get the IP of my pod and I can say port 8080 / health. It doesn't have cur but if I do wget um that also fails saying on this IP port 8080 there's no health. Let me check what was the endpoint for my health checks for the health probe. It was 8081. That's correct. So I need to look on port 8081 / health to find out if my pod is healthy or not. And uh it says health already file exists. So let me do a little cleanup. Uh health ready metrics cuz I was trying this before. So it was already there. Let's start from the beginning. I want to see inside my controller on this pod 8081 is there a health endpoint and you see health is saved. If I do cat health it's okay. So this tells me my health probe my um you know the livveness probe is working fine because I have on this port number I have uh this this API endpoint and it returns me a value of okay. The same thing goes on if I use the ready endpoint. Maybe let me increase the font a little bit. And here you can see I also have get ready is saved. And if I do cat on ready, that's also okay. So on both of these endpoints for my livveness probe and the readiness probe on this port number on both of these endpoints, I get an okay. That means these endpoints were created by the controller runtime for us. uh so that we can do a health uh health check. It's also interesting that we have uh some metrics as well on port number 8080. So if I do uh wget on port number 8080 on the metric endpoint matrix endpoint you see there was something available on this endpoint as well. And if I do less on metrics, these are all the Prometheus matrix which are already built in and exposed by the application which you have built. This is not what you have done. This is already given by the um by the controller. And here's where you can see um the controller runtime total. How many times the controller runtime has reconciled and resulted into an error. how many times it reconciled and result into a recess, a rec. And all of this is what you can use this information, you know, you can use these informations and how many errors total you have had so far to show a dashboard um of how your operator is doing. And with these metrics if in case you see that the errors are going up you can make changes to your operator you can make changes to your code eventually to be at a better stage than the previous one because you have metrics you have insights of how this is this is going on. You can even do um your own code instrumentation for a bit of metrics. For example, you can tell how many EC2 instances have been created by this particular operator. Um, having Prometheus in your code and then exporting those metrics in in a way that Prometheus understands it and can scrape it is a different topic al together. But if in case you know that it would be nice for you to have this instrumented in your code and then you can export this information of how many EC2 instances were created um deleted so you can know how how how much people are using your um your particular um your operator. So it looks like my pod is running. It looks like my pod has got the right health endpoints. my part has got the right um health and the readiness probes and also I've got the metrics available. But now I want to create an instance because that's what it should be doing. It's okay. Everything is is happy. But is it really doing what it's supposed to do? So let's do that. And I'm going to look on the logs of my EC2 operator. You can see it's starting the workers. It's all healthy. It's waiting for my resources. And now what I'm going to do is I'm going to do the same thing what I did when we were running this out of the cluster which was on our computer. And I'm going to create an EC2 instance which looks something like this. This is my EC2 instance. I give the instance type, the AMI ID, the region that I'm using, the availability zone, the key pair. This is something we already had used before. But I want now that operator now it's running in my Kubernetes cluster to create me this instance because eventually that is where you will be running this uh inside of the Kubernetes cluster. So if I do a create this is the moment of truth what we have been working towards so far and right now for me to keep it simple you can see uh AWS console you can see I do not have any instances running so there's nothing running uh let's do that and I will do a create now as soon as I did that this output should be looking familiar to you. This is where um we got a request. It says the request was new. So I'm creating a new instance and then we go to Amazon and we add a finalizer. We go to Amazon and we wait for the instance to be running and you can see the instance is already created. If I do EC2 instances uh in the default name space and here you can see can I ping this instance? Of course. And uh that was the the beauty of my operator. I got an EC2 instance which I can access right with the public IP from my computer right from the public IP of my in of my you know of my comfort just to get cubectl get instances get the IP and log and I start working there. The important thing is this object EC2 instance is in the default name space and my operator is actually running in the EC2 operator name space. So these are different and this is what I was talking about here. This name space is default and here's where the object was created and this is the pod which is running in my EC2 operator name space went to Amazon because here's where the instance available. You can see it's running now. And once it's done, my EC2 instance operator went to this object and it updated the status. So my operator needs access to not just read the the object but also to write to that object so that it can update the status such as it can give you the state of this instance uh the public IP and the instance ID that was created on Amazon. And that is what we have been looking forward to so that we can um you know uh we can go ahead and create our instances or manage our Amazon environment to clean it up. So I can show you it's actually also deleting resources. Let me do a little bit of um you know a buffer. So we start from when we delete our instance what happens as soon as would I do a delete my reconcile loop starts because there's an update to the object and I see it has a deletion timestamp instance has been deleted so we print that we are now deleting the EC2 instance and then we use Amazon API to delete to send a terminate request to our instance and then we wait for the instance to be terminated and that's essentially what's happening in here. So you see it's not running anymore. It is now terminated. This was the instance which was terminated. And as soon as it got terminated, my waiter said, ""Okay, it's all fine. It's not terminated."" And eventually I was able to delete my EC2 instance object in the Kubernetes cluster. It was pending. it did not delete it until the actual resource on Amazon was was cleaned and then the finalizer was removed from my object and then essentially the object was actually deleted. So this is how you will be building an application. You will test this locally. You will build it into a container image and then you will ship this to your different clusters that you want to deploy using a Helm chart. Essentially what you were able to do locally is now all happening in your Kubernetes clusters because the operator should be running in your Kubernetes cluster. So this is what I wanted to show you guys an end to end starting from bootstrapping the project then going ahead building the project testing it and eventually making it work and then we deploy that with Kubernetes and essentially run this in the cluster with all the proper role based access control with all the proper line probes the redness probes and also the matrix which is which is which is nice to have to see how your controller is really doing and when everything is good there's no need to reconcile all is happy. So I think this makes a this makes a lot of sense uh to write your own operators and I want you guys to try this out and see how this works and let me know if you have any questions and I'll be happy to help and let's move on ahead. All right, so the code for building this operator for the cloud that manages your EC2 instances is now coming to an end. And I have to admit it's quite a lot. But trust me, what you have just done with this course is that you have actually understood one of the most advanced concepts in Kubernetes which is the reconciler which is how to write applications which are self-healing which is how to write an operator. While you know the basics now, while you know a very good understanding of how to work with operators, there's no limit to it. Think of this course as a logistic that makes you, you know, that enables you to go ahead and build cool stuff that runs on top of Kubernetes. Not just using um container images to run on Kubernetes, but rather software that runs on Kubernetes and manages your other infrastructure, which is what we did with Amazon. You might be using Azure. Try to make the developers life easier by managing resources um using Kubernetes on Azure. Maybe you are doing this on GCP. The sky is the limit for you. Now, now you not just know how to use Kubernetes, but you also know how to write applications which are native to Kubernetes that manages your different other environments. So you know how to work with the reconcile loops. You know how do you design the API endpoints. You know what is the controller logic is you know what the integration looks like when you're working with cloud. And you are very much applicable now or you already are probably working as a platform engineer. You might be a site reliability engineer. You might be a DevOps engineer but now you know how to expand and expand on Kubernetes. Cube Builder as a project we have looked into good detail in this course. We have got we have really you know struck the nerve of using it to create a production ready bootstrap plus bootstrap operator but I would not say it is right now production ready. When you build an operator you run it fails people complain about it you refine that and eventually it becomes a production ready. But you have the tools now to go to that journey on yourself. Next try to build your own operator. Try to extend this particular operator to have the metrics available. Try to write your own operator that manages the SP buckets. Now for people try to write your own operator that manages EFS um file systems on on Amazon or maybe anything else. No, it doesn't have to be limited to the cloud. So go ahead, have fun, have, you know, have fun building uh building new operators and have fun building new tools that runs on top of Kubernetes. And if you have any questions, let me know.","This comprehensive course transforms the understanding of Kubernetes, shifting the perspective from a mere container orchestration platform to a powerful **Software Development Kit (SDK)**. Taught by an expert with extensive production experience, this 6+ hour deep dive empowers developers, DevOps, and Platform Engineers to build bespoke extensions for Kubernetes: **Custom Operators** and **Controllers**.

The core project involves building a real-world operator in **Go** using the **Kubebuilder** framework, capable of managing external infrastructurespecifically, provisioning, updating, and deleting **AWS EC2 instances** directly from Kubernetes manifests.

---

## Key Takeaways and Core Concepts

### 1. The Controller Pattern: The Engine of Kubernetes

The course provides an in-depth explanation of the **Controller**the fundamental software component driving Kubernetes' automation.

*   **Forever Running Loop:** A controller is essentially a **forever running loop** that continuously **observes the current state** of a resource and compares it against the **desired state** (defined in the resource **Spec**).
*   **Reconcile Loop:** The central function of the operator is the **Reconcile Loop**. This logic calculates the **drift** between the current and desired states and executes the necessary business logic (e.g., calling the AWS SDK to create a VM).
*   **Idempotency is Critical:** A major emphasis is placed on writing **Idempotent** controllers. This means the controller must be able to run hundreds of times without causing unintended side effects or creating unnecessary resources, acting only when a change is required.
*   **Self-Healing Mechanism:** The controller utilizes the return values of the reconcile function (Result and Error) to implement **self-healing**. If an operation fails, the controller returns an error, prompting Kubernetes to automatically **re-queue** the request with an **exponential backoff**, ensuring eventual consistency.

### 2. Extending Kubernetes Vocabulary

To manage external resources like EC2, Kubernetes must first be taught about them through custom definitions.

*   **Custom Resource Definitions (CRDs):** These are used to expand Kubernetes' vocabulary, declaring a new resource type (e.g., `EC2Instance`).
*   **Custom Resources (CRs):** These are user-created YAML files that instantiate a **CRD**. The course teaches how to meticulously define the resource **Spec** (the configuration inputs, like AMI ID, Instance Type, Key Pair) and the resource **Status** (the external feedback, like the instance ID and **Public IP**).
*   **Finalizers for Cleanup:** Proper resource deletion is managed using **Finalizers**. When an object is deleted, the Finalizer prevents the Kubernetes object from being removed until the controller successfully completes all necessary external cleanup (e.g., terminating the AWS EC2 instance). The controller checks for a **Deletion Timestamp** to initiate the cleanup process.

### 3. Advanced Operator Architecture

The course explores the internal workings of the controller process, highlighting the components that ensure speed and resiliency:

*   **Informers and Caches:** **Informers** maintain a long-running watch connection to the API server. When a change occurs, the",2026-01-16T01:55:41.961321
freeCodeCamp.org,Why you should prioritize networking if you&#39;re in tech,9tyZL5KIiGU,Most of my career I missed networking you know I think I I started doing it pretty late once I got into open source I think because I was always a very shy person so I never tried interacting not creating much not doing much of networking I think that's the one piece of advice I always give to everybody who is in tech like the world is very small I mean tech is even smaller than what you think and you will always if you if you network enough you will always get opportunities to work with maybe some amazing developers in the in the community so Don't dep prioritize,"**Unlock the Power of Networking in Tech**

As a professional in the tech industry, it's essential to prioritize **networking** to unlock new opportunities and collaborations. The speaker, who initially underestimated the importance of networking due to shyness, now emphasizes its significance in the tech world. The key takeaway is that **networking** can lead to incredible opportunities to work with talented developers and like-minded individuals in the community.

**Why Networking Matters in Tech**

The tech industry is a **small and interconnected** world, where relationships and connections can make a significant difference in one's career. By **networking**, professionals can:

* Access new job opportunities and collaborations
* Learn from experienced developers and thought leaders
* Stay updated on the latest trends and technologies
* Build a strong professional network that can lead to new projects and partnerships

**Overcoming Barriers to Networking**

For those who are **introverted** or **shy**, it's essential to remember that **networking** is a skill that can be developed over time. Starting small, attending industry events, and engaging with online communities can help build confidence and establish meaningful connections.

**Key Takeaways**

* **Networking** is crucial for success in the tech industry
* The tech world is **small and interconnected**, making relationships and connections vital
* **Don't deprioritize** networking, as it can lead to incredible opportunities and collaborations
* **Introverts** and **shy** individuals can still develop their networking skills with practice and patience

**Create Your Own Networking Opportunities**

Don't wait for opportunities to come to you - create your own by:

* Attending industry events and conferences
* Joining online communities and forums
* Reaching out to professionals in your desired field
* Participating in **open-source** projects and collaborations

By prioritizing **networking** and taking proactive steps to build your professional network, you can unlock new opportunities, collaborations, and career growth in the tech industry.

**Social Media Post Ideas**

* ""Unlock the power of networking in tech and discover new opportunities! #networking #techindustry #careeradvice""
* ""Did you know that the tech world is smaller than you think? Prioritize networking to connect with talented developers and thought leaders! #networking #techcommunity #collaboration""
* ""Don't let shyness hold you back! Develop your networking skills and build meaningful connections in the tech industry. #networking #introverts #careerdevelopment""",2026-01-16T01:56:00.202270
LangChain,Choosing the Right Multi-Agent Architecture,fqvbxkgU6vE,"Hey folks, it's Sydney from Lingchain. I'm super excited to chat with you today about how to choose a multi-agent architecture. First, I would actually like to caution you. You might not actually need a multi-agent pattern for your system. Many agentic tasks are actually best handled by a single agent with well-designed tools. That being said, when your tasks are increasingly complex, multi-agent might be the way to go. So, let's dive into chatting about our scoring criteria for different architectures. So we have four criteria here. The first is distributed development which is exactly what it sounds like. Can different teams maintain different components or agents independently based on their specialties? The second is parallelization. Can you execute multiple agents at the same time? The third is multihop conversational support. So does the architecture support calling multiple sub aents in series with the context from previous calls? And the final criteria is direct user interaction. So the question here is can sub aents converse directly with a user. So our first pattern that we're going to look at is sub aents often also called the supervisor pattern. And in this pattern a main supervisor agent coordinates sub aents as tools. All of the message routing passes through the main agent which decides how and when to invoke each sub aent. Now we can talk about how this architecture kind of scores along those different criteria lines. So we actually give distributed development here a five out of five. The sub aents architecture is great when these different sub aents manages tools are are managed across teams. The sub aents architecture also does quite well with parallelization. Agents support parallel tool calling and thus you can invoke your sub aents in parallel. Multihop conversations are also easy to organize here. This just requires multiple cycles of the model and tool calling loop. Finally, direct user interaction is definitely where the sub aents architecture falls short. With lang chains architecture, you can technically achieve direct user interaction via interrupts in your sub aents, but there's no easy way for users to interact directly with sub aents. Our next architecture is the handoffs pattern. In the handoffs pattern, agents can use tool calling to hand off control to other agents. And you can see that in the diagram here. The user request is sent to the entry point agent, which is agent A. And then all of ages A, B, and C have the power to hand off to one another and then also to generate a final response. Distributed development is one of the weak points of the handoffs architecture. It's a bit difficult to develop agents independently that need capabilities to hand off to each other. Parallelization is also not a specialty of the handoffs architecture, but the handoffs architecture is particularly great for multihop conversations and direct user interaction. In fact, this is probably the best architecture to choose if you are looking to have those two features. Our third architecture which is sort of a quasi multi-agent architecture is support for skills. So skills are specialized prompts and knowledge that are loaded on demand. A single agent stays in control while loading context from skills as needed. This is a practice called progressive disclosure and it's becoming increasingly more popular as a context management strategy. The skills architecture scores quite high on the distributed development front. Different teams can manage different skills based on their specialties. We score a three out of five here on the parallelization front. Even though you can load multiple skills in parallel and call multiple skills in parallel, it's kind of a two-step process. And so that's why we don't give it that five out of five. You can certainly have multiple calls in series to the sub aents aka skills here. So multihop scores at five out of five. And then direct user interaction is also a five out of five. Again, we really just have that one core agent that the user can easily interact directly with. Our final architecture is the router architecture. In this one, a routing step classifies input and directs it to one or more specialized agents and then results are synthesized into a combined response. And both the router and synthesizer steps here can be agentic, but they also don't have to be. They could be more deterministic. We score distributed development for this case at a three out of five just because there's no standard protocol for the agents unlike in the sub aents protocol or the skill protocol where you can use tools or skills as standard protocols. It's a little bit harder to standardize here. So certainly possible to do distributed development but not quite as easy. We do though score parallelization at a five out of five here. The router can invoke multiple sub aents in parallel or just one at a time. Multihop we've scored at a zero out of five here. The main point of support for multihop is that you can have multiple invocations of an agent in series which is not super feasible with this architecture. You can have stateful routers but it's much more difficult to manage. We actually recommend and you can check out our new docs on this if you do want a stateful router that you just wrap your router in a tool. Finally, we score direct interaction with the user at a three out of five. As you can see, there's router and synthesizer steps on either side of the agent invocation. So, this isn't as direct as the interaction that we see in some of our other architectures. Putting this all together, we're sort of using the fivestar scale here. Here is a table summarizing all of our results. And you can also check out our newly rolled out multi-agent docs to learn more about these comparisons. Again, I would say that probably the most important thing here is to start simple. So start with a single agent and build up from there as your problem gets more complex. Thanks, folks.","**Choosing the Right Multi-Agent Architecture: A Comprehensive Guide**

When it comes to designing complex systems, **multi-agent architectures** can be a powerful tool. However, it's essential to determine whether a **multi-agent pattern** is truly necessary for your system. In many cases, a single agent with well-designed **tools** can handle tasks more efficiently. But as tasks become increasingly complex, **multi-agent architectures** can provide a scalable solution.

To evaluate different architectures, four key **scoring criteria** are used:

1. **Distributed Development**: Can different teams maintain different components or agents independently?
2. **Parallelization**: Can multiple agents be executed at the same time?
3. **Multihop Conversational Support**: Does the architecture support calling multiple sub-agents in series with context from previous calls?
4. **Direct User Interaction**: Can sub-agents converse directly with a user?

Four **multi-agent architectures** are examined:

1. **Sub-Agents Architecture** (also known as the **Supervisor Pattern**): A main supervisor agent coordinates sub-agents as tools. This architecture excels in **distributed development** and **parallelization** but falls short in **direct user interaction**.
2. **Handoffs Pattern**: Agents can use **tool calling** to hand off control to other agents. This architecture is ideal for **multihop conversations** and **direct user interaction** but struggles with **distributed development** and **parallelization**.
3. **Skills Architecture**: A single agent stays in control while loading context from **skills** (specialized prompts and knowledge) as needed. This architecture scores high in **distributed development**, **multihop conversations**, and **direct user interaction** but has limitations in **parallelization**.
4. **Router Architecture**: A routing step classifies input and directs it to one or more specialized agents, with results synthesized into a combined response. This architecture excels in **parallelization** but has challenges with **multihop conversations** and **direct user interaction**.

When choosing a **multi-agent architecture**, it's crucial to consider the specific needs of your system. **Start simple** with a single agent and build up to more complex architectures as needed. By evaluating these four architectures and their **scoring criteria**, you can make an informed decision and design a system that meets your requirements.

**Key Takeaways:**

* **Multi-agent architectures** can be powerful tools for complex systems, but they may not always be necessary.
* **Distributed development**, **parallelization**, **multihop conversational support**, and **direct user interaction** are essential criteria for evaluating **multi-agent architectures**.
* Each architecture has its strengths and weaknesses, and the right choice depends on the specific needs of your system.
* **Start simple** and build up to more complex architectures as needed.

**Create interesting social media posts:**

* ""Did you know that **multi-agent architectures** can be used to design complex systems? Learn how to choose the right architecture for your system! #MultiAgentArchitectures #SystemDesign""
* ""What are the key criteria for evaluating **multi-agent architectures**? Find out how to make an informed decision for your system! #MultiAgentArchitectures #SystemEvaluation""
* ""Need help choosing the right **multi-agent architecture** for your system? Start simple and build up to more complex architectures as needed! #MultiAgentArchitectures #SystemDesignTips""",2026-01-16T01:58:07.878043
LangChain,"Build Better Agent UX: Streaming Progress, Status, and File Ops with LangChain",3daSUNpWErQ,"In our last video, we implemented a basic tool calling agent and streamed the results live into our React application. Now, in most cases, a tool call only takes a reasonable amount of time. Often, you fetch something from the API, write or fetch to the file system or integrated into another service. But what if your tool call actually runs for a decent amount of time? For example, you trigger a sub agent that does a whole lot of work for you until it responds. A really responsive UI should give a user constant updates to ensure they know something is happening in the background. Ideally giving them some sense of expectation to when this operation is about to finish with chain. This can be managed via custom stream events. Uh let's dive into to see how we can implement and add custom events to our agent and how we can render it into our front end. Let's check it out. Now in our sandbox example, we are working with a data analysis agent here that will help us to parse through multiple files, analyze the content and give us some trends about its content. So in a normal application, you may show the human message followed by some sort of loading indicator that shows that something is happening in the background. And once all the two calls have been done, you show the final assistant message with the result of the analysis. Now, as you can see here, it may take some time until all the files have been processed by the LLM and a trend has been generated. We can make the app now more responsive by rendering immediate tool events right into the front end while the tool is being executed. So let me go into the code and reenable some sections and rerun the example again. We will now see that as soon as the tool is being executed, we are streaming live updates from the tool call into the UI which will make the overall application much more responsive. Let's look into the code first. Let's have a look into the agent. The agent is fairly simple. We again define a model. We have a tool to analyze the data and we have a tool to process files. The implementation of these tools is not important. What's important is how we send updates to the UI and we do this with the config.writer function. The config.writer function is part of the tool runtime and allows us to just send arbitrary events to the front end. It is part of the second argument of your tool function and provides you a way to send arbitrary data bs to the UI. You can see here that we're iterating through different stages and for every stage we send a progress update that satisfies a certain interface and then we just have an arbitrary wait time of 500 milliseconds until we go through the next step and finally send a final status report. Now what's important here are two things. For one, we are giving every of our custom events a type that will help us to later identify that event in our UI and render specific cards for these events in the front end. What we also send along is the tool call. This will help us especially the tool call ID will help us to render the progress event to the specific progress or data analysis tool call. Now in our front end we get access to these custom events through the oncustom event handler which is part of the use stream hook that we have. Again we access the custom streaming agent which is defined in our agents and we register the on custom event where we essentially just collect the data and put them into dedicated maps that we then access during the rendering time of our component. Now when we receive the data, it's usually typed as an unknown object. We now have to have these or implement these helper functions to help properly type these objects so we can put them into the right map. And we do this by having these helper functions where we validate that the data blob is an object, it's not null, and that the type that we set along is the right type. And then whenever this function returns to true, we can tell Typescript to label the data object with the dedicated interface. So for is progress data, we label the object as progress data object and for the other functions the same. Now when we render the component again we will stream through all the messages and whenever we come across an AI message that contains a specific amount of tool calls we're mapping over these tool calls and connect every tool call with a specific component that renders a specific custom event of that tool call and at the end we render the message bubble and render the the custom tool update cards right after it. So the way this now looks like is that we have an assistant message with loading indicator and as soon as the tool call is being executed, we can render something in the UI while the tool is working on different types of data. We can even show multiple updates for different or multiple tool calls at the same time. Now custom events are a great way to render immediate feedback in your application when a tool call may take a second or two longer than desired. Check out the example below to see the whole application, how we deployed it with langraph dev server and how we identify these custom events and render them in the front end. You can also see everything in our front end docs where we document how you can register your custom event handler as well as detect these custom events in your UI. Thank you for watching and see you in the next","**Building a Responsive UI with LangChain: Streaming Progress, Status, and File Operations**

In this video, we explore how to create a more **responsive UI** by streaming **custom events** from a **tool call** in real-time, using **LangChain**. This approach enables developers to provide users with constant updates, setting expectations for when an operation will finish. We dive into the implementation of **custom stream events** and how to render them in the front-end, making the application more engaging and interactive.

**Key Takeaways:**

1. **Custom Events**: LangChain allows developers to send **arbitrary events** to the front-end using the **config.writer function**, which is part of the **tool runtime**. These events can be used to provide updates on the progress of a **tool call**.
2. **Streaming Progress**: By streaming **progress updates**, developers can create a more **responsive UI** that keeps users informed about the status of an operation. This is particularly useful when a **tool call** takes a significant amount of time to complete.
3. **Front-end Rendering**: The **onCustomEvent** handler, part of the **useStream** hook, is used to collect and render **custom events** in the front-end. Developers can use **helper functions** to properly type these events and render them in the UI.
4. **LangChain Integration**: LangChain provides a seamless way to integrate **custom events** into the front-end, allowing developers to create a more **interactive** and **engaging** user experience.

**Important Concepts:**

* **Tool Call**: A **tool call** is an operation that is executed by a **tool**, which can take a significant amount of time to complete.
* **Custom Events**: **Custom events** are used to provide updates on the progress of a **tool call**, allowing developers to create a more **responsive UI**.
* **Config.Writer Function**: The **config.writer function** is part of the **tool runtime** and allows developers to send **arbitrary events** to the front-end.
* **OnCustomEvent Handler**: The **onCustomEvent** handler is part of the **useStream** hook and is used to collect and render **custom events** in the front-end.

**Social Media Post Ideas:**

* ""Create a more **responsive UI** with **LangChain**! Learn how to stream **custom events** from a **tool call** in real-time and provide users with constant updates. #LangChain #UI #UX""
* ""Take your **UI** to the next level with **custom events**! Discover how to use **LangChain** to stream **progress updates** and create a more **interactive** user experience. #LangChain #CustomEvents #UI""
* ""Want to know the secret to creating a more **engaging** user experience? It's all about **custom events**! Learn how to use **LangChain** to stream **custom events** and create a more **responsive UI**. #LangChain #CustomEvents #UX""",2026-01-16T01:58:13.398454
Microsoft,What happens when AI enters healthcare? | On Second Thought,WKrGCu398ss,"I think there is a legitimate fear
that you don't want to be alone in your health care journey entirely. I think a big concern
people have is empathy, judgment. Where is the human in the future of care? I think framing the question of will it be an eye
or doctor is just the false dichotomy. It will obviously be both. If there was one thing that you wish people knew or understood about AI
and health care, what would it be? AI is transforming health care
and these changes are already in play. It's helping to predict diseases. It's helping to read scans and helping doctors make faster
and more precise decisions. And there's a lot of excitement about
the dawn of personalized health care. But at the same time,
there's some very real concern about the role of the physician and what happens to empathy and judgment
in this new world. So today, I'm here to talk with Jonathan
Carlson, VP of Health Futures at Microsoft. And he is on the cutting edge of this
AI research and he's helping to redesign what care and what health
may look like in the future. So as a futurist, I was really intrigued to hear your title,
VP of Health Futures at Microsoft. What does that mean? What do you do? So at a high level,
I lead our research team that focuses on health and life sciences. And we've been at this for 20 years
or so thinking about what is the future. How do we use computing to really advance
human health? In the last decade or so,
this is really shifted into how do we really use the emerging
technologies of artificial intelligence, both from an application perspective, but what are also
some of the fundamental learnings that we need to really push on
from a technology perspective? And how do you see the physician's role in collaboration
with or evolving alongside AI? Because on the one hand, AI,
it doesn't actually have tacit knowledge. It's not in the field, it doesn't actually
have patients, but it has textbooks. The physician has patients
and has read textbooks. So do you see a lot of overlap and
synergies, or where do you see the tool working with or doing its own thing
alongside physicians? This is going to evolve. But if you think about
what is so hard about medicine, you just practicing textbook medicine,
how many textbooks are there in the world? How many specialties are there? Why are there specialty? There are specialties because no one human
can understand all medicine. It's just not possible. Even medicine
as we understand it today. Which of course is going to be different
than how we understand it tomorrow. So already physicians use search engines. They use, you know, Google search, but they also use medical specific
search engines to stay up to date. One of the
I think, really immediate use cases of AI from a clinical perspective
is just its ability to help physicians diagnosed make sense of the particular
case in front of me and then match it to what we already understand
about medicine. And we're starting to see early
academic studies showing that AI is very, very good at this. How do we integrate that into practice? Is still a pretty open question,
but the ability of models and AIs so working with physicians to start
integrating the different parts of the information
and being able to even just triage and say, no this really is important,
you do need to go to the urgent care right now, and being able to uplevel those frontline physicians,
I think, will be super important. I think the most important thing is that
we are really getting to better patient outcomes. Objectively, these systems are very good
at things like medical diagnosis, and they're not perfect by any means,
but I think we can see how that will start integrating
directly into direct patient care. And how do we think about the data? I mean, whether that's somebody
asking an AI system a general question or even just AI being
an ambient technology in the room, how do we find that line? because we're going to need more data to do personalized medicine,
and we can talk about that in a second, but where do we draw that line around
how much we should share with the system? Because once you give a generative
AI system data, you can't get that back. Yeah. And I think one of the important shifts that we're starting to see that
I think we really need to push on is not ask who owns the data
and is the patient giving consent, but how is the data
being used as a public good, and how are we respecting
the dignity of that data and giving patients choice
to be part of that process? And how do we stop thinking about who owns the data,
but who is the trustees of that data? Again, how do we think about this
from a public good? Balancing with with personal choice
and personal dignity. One of the things that I'm really excited and encouraged to see is that we're seeing patient groups becoming very involved
with this discussion, and we're seeing bioethicists becoming very involved
with these discussions and regulators. And I think it really
is a societal discussion. And I guess with that question,
who also gets to benefit from the data because it can go into a public
good data set. But to make sure that you were also the beneficiary of those answers
and all of those analytics. That's right. If you could explain the potential
and the possibilities for the personalized area of health care, what that could look like
and what we could be excited for it. Let me first ask,
why do we want personalized medicine? To me, the fundamental problem of medicine
is twofold. One, we just don't understand biology. And so there's a bunch of noise there. And the other is
that we're really diverse. All of us are different. And yet medicine
has to operate off of averages. Again, none of us are average. Every one of us has our idiosyncrasies, right? And so the goal of personalized medicine
is to do the right thing for you, to do the right thing for me. Some of the great breakthroughs
in cancer care immunotherapies only work, at best, about 30 or 40% of the time. And we have no idea which 30 or 40% of the people. What we really need to do is get to the point
where we can actually start predicting who's going to respond to things. But I think we often think about, well,
how was I going to predict this is some magic
instead of learn from something? And I think the most important thing
is to think about how can we use AI to structure
unstructured information, right? We're already digitizing our encounters
in the clinic. So in some sense, we already have the information
about what's happening in medicine encoded digitally. The problem is it's a mess. It's
total chaos. It's things that my physician has written
down. It's PDFs. Its faxes that have been digitized,
but AI is actually really good at taking unstructured stream of thought writing instruction into a table. Once you do that, well,
we have decades of understanding about how to use statistics to find populations
that will respond or not to respond to treatment. We can start using that to identify
biomarkers to say, hey, this person is 30% likely to respond,
this person is 70% likely to respond. And then we can start using
that to find subpopulations to get more and more precise care,
not just say, okay, this person can respond,
but these people are not responding. What is it about the biology? And now we have a discovery
program to go on and find medicine. If a patient needed to get something
diagnosed. Do you have any studies of what
this could look like in the future, in a world
where AI agents are more prevalent? Yeah, the whole idea of agents is
fascinating is becoming kind of a buzzword. But another way to think about it is
how do we just decompose a problem and ask instances of
AI to play different roles? So two different examples.
One really simple one. If you ask a model
to get to a diagnosis, but also think about costs at the same time,
it'll kind of get itself confused. But if you say, you know what, I'm going to ask five different instances
of a model to role play, Theyll actually get to a much more reasoned,
sort of multipurpose answer when care is working at its best cancer care,
for example, you will see your oncologist. And once you go through the textbook
medicine, you'll run into the endpoint and say, well,
I don't know what to do next. What do we do? Well, they'll bring your case
to a multidisciplinary tumor board. And that's literally a room
full of a dozen specialists, you know, radiologists, pathologists, geneticists,
and people think about clinical trials. There'll be this whole effort to say, okay, lets pull in
everything we know about this patient for for clinical trials,
let's look for the latest research. You can actually decompose
that in an agentic way in terms of having different models,
a model that understands pathology images and model
that understands clinical trials. And you can design systems
and that can actually go off and pull together the information needed
to inform the tumor board. You can turn that
into an interactive experience where they can interact
with the physicians. Right now that's been used to prepare
for tumor boards. I think in the future, we have an opportunity
to bring this technology again to resource limited settings where you don't have
access to 12 specialists, and you can start giving some of this
information to the frontline care workers. So essentially, you're seeing Microsoft
set up a study where they put a team of AI agents
that one would look at be like a cost center
in a hospital or in an insurance room. The other would be a specialist
in this field and the other
a specialist in something else. And the AI agents themselves deliberated on the best course of action
for the patient. That's right. Yeah. And what was the outcome of the study where you were able to pair it
against physicians? Yeah. In that particular case
it ends up with better diagnostic outcomes as measured by did you get to
the correct answer and how much better. So the important thing here is
these were diagnostic challenges that were published in New England
Journal. So humans are almost by design bad at this.
These are really really hard problems. But the answers in
some sense are in a textbook. And so the models are very good. Whats more interesting is as they do that
they can make better utilization of the medical system because you can also
measure well, how did you get to that answer? Was it just asking questions,
did you order an MRI? Did you order a really expensive biopsy? The most important thing is
it gets to the right answer. But sometimes you can get there
through a less resource intensive way, and so you can by getting the model
to think about it holistically, you can actually get to a better answer,
in the better use resource, whether that's dollars
or whether that's time. Now I want to be clear, the goal for us
as Microsoft is not necessarily to build these systems
as a final product, necessarily. It's more to show what's possible and to build these tools in such a way
that others can build on top of those. How do you empower somebody
to get a really good model, and then plug that into a system
where others can then consume that? And you mentioned that before to truly to try to understand
what the role of the physician is in three years and five years,
and it probably means judgment gonna get really hard. They're asking supercomputers
really complex questions. But shouldn't physicians
be training for this now? Which means AI should be incorporated
into medical schools. So when you're graduating,
you're ready to use this tool. Is that happening? I mean, it's spotty, I would say. But I think this is a bigger question. You studied this yourself of that of how do we actually incorporate
AI into all aspects of education? The dumb way to do it
is to use it to cheat. That doesn't help anybody,
but there's ways that you can use AI just from testing mechanisms,
from tutoring, etc., etc.. I think the more interesting question is,
how do you start training physicians to use the AI in their practice,
and how do you do it in such a way that they still learn
the fundamentals of medicine? Develop a taste, if you will,
for how medicine should be done. I think that boundary is going
to be really interesting to figure out. How do we do that well. I think a big concern people have is empathy, judgment. Where is the human in the future of care? And you describe it as
if we can bring more care to the patient so they don't even have to go seek it. That's actually a win for everyone. But I think there is a legitimate fear that you don't want to be alone
in your health care journey entirely. So where do you think that evolution
happens? It's hard to predict the future,
but I think we as humans need human interaction,
need human touch, need human judgment. And that's not just a text box
saying, here's what the right answer is, but it's somebody that can sit next to me
and cry with me as somebody that can really help me walk through that journey
and get to the answer that I need. I think we see this
very clearly with the role that nurses play in medicine right now. It's not just about do the thing,
but do the thing with you and help you along your journey. And so I think that human element
will always be critical. I think framing the question of will
it be an AI or doctor is just the false dichotomy.
It will obviously be both. I agree, I agree,
and as you had mentioned, different people will be doing
different aspects of the care. So we say we do want human
empathy and human touch point, but who is doing that care in a world
where I could help with diagnostics, maybe that means you interact
with the nurse more or a different type of care giver, and that is the true
impact of a system wide change. We don't recognize the world that we live
in, and that's also possible. Yeah. We also want our human
to have all the tools available that they have to make
the right decisions. Hey. If there was one thing that you wish people knew or understood
about AI in health care, what would it be? If you reflect on the history of medicine,
it is replete with inventions of new tools that fundamentally change
how medicine is practiced. I think back to the invention
of the microscope in the 1800s. It wasn't just now there's a new tool
that doctors could use. There's a whole new discipline,
a whole new specialty, a whole new training mechanism
to actually learn how to use this. There's other examples obstetrics is a great example
where when the ultrasound was invented, there were lots of concerns about
what's the safety of this new technology for the mother, for the unborn baby,
should we use it or should we not? But now if you go to, to obstetrician and they say,
I don't believe in that stuff, I'm just going to use my fingers
and trust me on this one. I think that would be medical malpractice. These things evolve over time. I think what's different now is rather than having a very specialized tool
that will lead to more specialists, my hope is that as a general tool,
we can actually diffuse the specialty back down
into the frontline of medical care, because the reality that medicine is more complicated
than any one person can understand. Love that, So that's a really fascinating way to end. Lots to think about thank you so much. It has been a pleasure. Thank you.","**The Future of Healthcare: How AI is Revolutionizing the Industry**

The integration of **Artificial Intelligence (AI)** in healthcare is transforming the way medical care is delivered, making it more efficient, personalized, and effective. According to Jonathan Carlson, VP of Health Futures at Microsoft, **AI is not a replacement for human healthcare professionals**, but rather a tool that can assist them in making more accurate diagnoses and providing better patient care.

One of the primary concerns about **AI in healthcare** is the potential loss of **empathy and human judgment**. However, Carlson emphasizes that **AI and human healthcare professionals will work together** to provide the best possible care for patients. **AI will help physicians** by analyzing vast amounts of medical data, identifying patterns, and providing insights that can inform treatment decisions.

**Personalized medicine** is another area where **AI is making a significant impact**. By analyzing individual patient data, **AI can help identify the most effective treatment options** and predict patient outcomes. This approach has the potential to revolutionize the way diseases are treated, particularly in areas such as cancer care, where **immunotherapies** have shown promising results.

To achieve this, **AI needs access to large amounts of high-quality data**, which raises concerns about **data ownership and patient consent**. Carlson suggests that instead of focusing on who owns the data, we should be thinking about **how data can be used as a public good**, while still respecting patient dignity and choice.

The use of **AI agents** is also being explored in healthcare, where **multiple AI models can work together** to analyze patient data, diagnose conditions, and develop treatment plans. This approach has shown promising results in **diagnostic challenges**, where **AI has outperformed human physicians** in certain cases.

**Incorporating AI into medical education** is crucial to ensure that future healthcare professionals are equipped to work with **AI systems**. This includes teaching physicians how to use **AI tools** to inform their practice, while still developing the fundamental skills and knowledge required to provide high-quality care.

Ultimately, the integration of **AI in healthcare** has the potential to **improve patient outcomes**, **reduce costs**, and **enhance the overall quality of care**. As Carlson notes, **AI is not a replacement for human healthcare professionals**, but rather a tool that can **augment and support their work**, leading to better health outcomes for all.

**Key Takeaways:**

1. **AI is transforming healthcare** by providing more accurate diagnoses, personalized treatment options, and improved patient outcomes.
2. **AI and human healthcare professionals will work together** to provide the best possible care for patients.
3. **Personalized medicine** has the potential to revolutionize disease treatment, particularly in areas such as cancer care.
4. **Data ownership and patient consent** are critical concerns that need to be addressed to ensure the responsible use of **AI in healthcare**.
5. **Incorporating AI into medical education** is essential to prepare future healthcare professionals to work with **AI systems**.

**Social Media Post Ideas:**

* ""The future of healthcare is here! **AI is revolutionizing the way medical care is delivered**, making it more efficient, personalized, and effective. #AIinHealthcare #HealthcareInnovation""
* ""Did you know that **AI can help physicians diagnose diseases more accurately**? Learn more about the potential of **AI in healthcare** and how it's changing the game for patients and healthcare professionals alike. #AIinHealthcare #MedicalInnovation""
* ""What if **AI could help you get the right treatment for your disease**? **Personalized medicine** is a reality, and it's changing the way we approach healthcare. #AIinHealthcare #PersonalizedMedicine""",2026-01-16T02:01:23.395083
Google Cloud Tech,Running a multi-agent AI architecture,0J_fz6RlqVg,"Okay, so we have our front-end application, but it doesn't want to manage a complex multi-step AI workflow. It just wants to ask a question and get an answer. Think of our front end like a homeowner who wants a new kitchen. They don't want to manage the plumber, the electrician, and the carpenter individually. So, they hire a general contractor instead. That's what our orchestrator agent will be. It's the general contractor, but for our AI squad. It hides all of the wild internal complexity from your users. In the orchestrator's code, we connect to a remote specialist using remote ATA agent. We just give it a URL and that's it. It's that simple. It adds the subcontractor to the team. Now, for the logic, we want a research to judge loop. ADK makes this trivial with loop agent. But how does it know when to stop? To accomplish this, we need a shared state. When the judge runs, we save its feedback to the session state. Then our escalator checker, a tiny custom agent, peaks at that state. If it sees status pass, it pulls the emergency break by yielding an escalate equals true event. It's like the building inspector finally signing off on all the permits. But we don't want our users staring at a blank screen. Obviously, we want to implement streaming so that they get updates every step of the way. In our server, we tap into the ADK event stream. And as events fly by, we check who sent them. And if it's the researcher, we tell the front end researcher is working, for example. But let's run it all locally. I've got a script that spins up all four microservices on my laptop. It's a full distributed system on one machine. All of the code is in the description below for you to follow along as well. The front end connects to just one port, but it gets live updates from the whole squad. This is the user experience we want. So to recap, the orchestrator is our general contractor. It manages the workflow using ADK patterns and shared state. It uses streaming to keep the homeowner, which is the front end, happy while the work gets done. So, it works on my machine, but that's not good enough. Next, we'll take the whole squad into production. Bye for now. [Music]","**Unlocking the Power of Multi-Agent AI Architecture**

Imagine having a **front-end application** that can seamlessly interact with a complex **AI workflow** without getting bogged down in the intricacies of managing multiple **AI agents**. This is where the **orchestrator agent** comes in - a **general contractor** for your **AI squad** that simplifies the process and hides the internal complexity from users.

The **orchestrator agent** plays a crucial role in managing the **multi-step AI workflow** by connecting to **remote specialist agents** using **remote ATA agents**. With just a URL, you can add a new **subcontractor** to the team, making it easy to scale and adapt to changing requirements. The **orchestrator** also utilizes **ADK patterns** and **shared state** to facilitate a **research-to-judge loop**, ensuring that the **AI workflow** is efficient and effective.

But how does it know when to stop? This is where the **escalator checker** comes in - a tiny custom **agent** that monitors the **shared state** and triggers an **escalate equals true event** when the **judge** has completed its task. This **event-driven** approach enables the **orchestrator** to stream updates to the **front-end application**, keeping users informed every step of the way.

**Key Takeaways:**

1. **Orchestrator agent**: The **general contractor** that manages the **AI workflow** and hides internal complexity from users.
2. **Remote ATA agents**: Enables easy integration with **remote specialist agents** using just a URL.
3. **ADK patterns**: Facilitates a **research-to-judge loop** and ensures efficient **AI workflow** management.
4. **Shared state**: Allows the **orchestrator** to monitor progress and trigger events as needed.
5. **Streaming**: Enables real-time updates to the **front-end application**, enhancing the user experience.

**Getting Started:**

To replicate this **multi-agent AI architecture** locally, you can use a script to spin up all four **microservices** on your laptop, creating a full **distributed system** on one machine. The code is available in the description below, allowing you to follow along and experiment with this powerful **AI framework**.

**What's Next:**

The next step is to take this **AI squad** into production, ensuring that the **orchestrator agent** and **remote specialist agents** work seamlessly together to deliver a streamlined and efficient **AI workflow**. Stay tuned for more updates on this exciting project!

**Social Media Post Ideas:**

* ""Discover the power of **multi-agent AI architecture** and how it can simplify your **AI workflow**! #AI #MachineLearning""
* ""Meet the **orchestrator agent**, the **general contractor** for your **AI squad**! #AI #Automation""
* ""Learn how to create a **distributed system** on one machine and take your **AI workflow** to the next level! #AI #DevOps""",2026-01-16T02:05:31.650003
IBM Technology,What are State Space Models? Redefining AI &amp; Machine Learning with Data,HbZD0XoN5fc,"Imagine AI that thinks faster, remembers more, and handles massive amounts of data in real time. That's what these state space models or SSMs bring to the table. They let AI track hidden patterns over time, and they turn them into actionable insights. Now, when we combine this with cutting edge architectures and optimized hardware. Now these innovations, they're making AI a lot faster and much more efficient. SSMs they are neural building blocks they act as memory layers by learning how information evolves over time to make them powerful tools for processing sequential type data. Now, ultimately, what these systems are, they do three things. The first one is they remember what has happened right in the past. Now the second part is they actually update the memory over time as these patterns change. And the third item is they make a prediction. And when you combine this together this is what the SSM helps us to achieve here. Now first let's dive into SSMs and figure out what they really are. Now these state space models, they represent systems using two key components. Now the first one is called the state equation. Now this models how a hidden state evolves over time. And if we get down into it the x sub t is a state vector at time t, while the matrix A defines how that system changes. Now the use of T, it's what we call the control or bias inputs. Well, B is the matrix that also changes how the inputs affect that state. Now finally the w it processes this noise that can impact even creativity of this system. But in essence this state equation it says how the world evolves internally. But now let's link this to generative AI. So let's take text for example. So we first create what's called an embedding. Now this embedding is an input into the model to update its hidden state or evolving memory. Now the optional input u sub t could be the prompt embedding. Then. Interestingly, this w sub t can be what's called that random variability that enables the model to be very creative. Now the second part of the equation this is called the observation piece. Now this maps the hidden state to the observed output. Um, and let's get into it just a little bit. And let's look at the y sub t. This is the observed output at time t while the C matrix. It maps these hidden states to observations. Now the matrix D it's fed through this system. And it affects and impacts the input to output. While the v sub t it introduces this measurement error. Now this equation also says how the model sees the world. Now linking this to gen AI the y sub t is really that next token and that sequential data. And then from this state equation. The x sub t is used to determine what token to create next. While the v sub t, it'll give it that creativity that we might be looking for. Now to learn matrices C and D, they influence how the output text is then created. Now when we put both of these together, this enables us to model these systems with these hidden dynamics. Now these types of models were first used throughout the field of robotics. And if we take for example a common filters, this helps robots to estimate its true position even when there's readings that are very, very noisy. But now if we look at today, these SSMs are being used within AI and machine learning. They've been adapted to handle the sequential data such as speech, text or any time series data. SSMs have become one of the most important mathematical frameworks in the evolution of generative AI. So what's not really open for debate is that GPUs are expensive. We have to do this model. Training and inference both require a lot of these GPUs and traditional transformers. They're just really inefficient during inference time. Now the largest bottleneck, right, is GPU memory bandwidth, which is very prevalent for the transformer based architecture. So for example, an 8 billion model, it can barely fit onto an A100 GPU that has about 80 gigs worth of memory. Now the kV cache alone, it takes about 64 gigs in size and add in another eight gigs of the model weights. And we're about at 90% memory capacity and then try to move that around. That's when we hit this bandwidth problem. Now the interesting part is that GPUs can only be at half util rate or at use. While maybe we're waiting for this, this data to be moved in and out of memory. Now this is that data movement problem. Now we can think of this as there could be a 15 times increase in compute capacity over time, right? But at the same time there's a much smaller amount of improvement in this memory and bandwidth. It might just be a 3X now. The towels when this is a problem is when the GPU use is really low, but the memory bandwidth is nearly full. Now, performance barely improves when we begin to add more cores. We can also try to increase the batch size, but this helps to only reuse the acid memory and we still need to move in and out these bits and bytes. But this is where SSMs really can come into the picture to help us out. So first of all we can use what are called these long sequences, right? This is where we can use the SSMs to help us process the sequential data that comes in over time. Like we mentioned, text, sound or any other type. Right. And then we also have these transformers. Right. Whenever we want to compare them. All right. And what's interesting is that Transformers take big O to the N squared versus the SNS, take about O to the N, and cost to model these types of log sequences here. Now let's take a look at we have this implicit memory structure. So the SSMs they don't store all the past tokens explicitly. But instead what they do is they will store them within those equations that we showed before. Right. And then if we go off to the next one, this is where we have this continuous time element piece to it. Right. And this is where we want to have the SSMs. They use this continuous signals to model these said sequences. And then the final part would be what if we could combine many things together. Right. So we might combine SSMs with the transformers. And this removes a lot of the bottlenecks in these neural network architectures. Right. That we just said. But what we can do is we can think of it like this is that AI transformers, they remember everything, whereas a state space model remembers only what really matters. We've had several architectural breakthroughs. For example, the Structured State space sequence model, or S4 emerged. This is a new kind of neural network layer that's really designed to handle these long sequences of data. This helps us to get over that scaling wall that face traditional transformers. It's really a very efficient memory system that can remember what happened a long time ago without having to look at everything. First, I would like to talk a bit about the key idea, which is to model the dynamics of memory with a state vector that's continually updated. So we can think of it like this. So a transformer, it remembers different things or everything that happens. Now the state space model remembers only what matters. Now if we go to speed we can then look at this, that it's very slow and it takes a long time for the transformer to process all of what it remembers. Now, if you go to the S4, it's very fast and efficient with those two equations that I showed you before. So now we're at memory. I transform my memory. It grows very quickly with that exponential growth that it has to deal with. But then we would go to the S4. It's very compact modeled again by those two different equations. Now the fourth concept would be what it works like. So a transformer what it has to do, it literally has to reread almost an entire book before answering. Now the S4, it can read only notes and then begin to answer the same question that the transformer would have had. Now, if we begin to look at the Mamba piece and this family tree, we then look at a model that's designed for this real world and large scale generative AI. So it builds upon the S4 by adding what's called Selectivity. Now this enables the model to focus on what matters similar to attention within the transformer, but it also retains those properties of being fast, like the FSM. In fact, it is a type of an SSM. Now it is unstructured and it changes over time. Now what it also does is it introduces matrices that change dynamically, depend upon the input, so that the model can then selectively update its memory. This enables the model to ignore these unimportant tokens that really don't contribute to the answer. Now this is huge. This gives Mamba this attention like flexibility. Without the cost of that full attention that a transformer would potentially have to run. Now, the math is very much optimized for this hardware friendly piece. Instead of having these very large matrices of multiplications that it has to use. We now in turn use convolutions and these simple multiplications. One example would be Mamba models, right? So they include the Mamba one, which was released in 2024. Now this was the original model. And then we go down into these multimodal models like Mamba Bite, Vision, Mamba Audio, Mamba and those others that help us to understand the world around us and the way in which is presented. Now we look at Mamba two, which is more performant than the Mamba one model here. We could think of it this way that S4 taught AI how to remember efficiently, while the Mamba family of models taught it how to remember intelligently. Now, today, the mainstream LMS such as granite V4, it's built on a hybrid SFM, as well as transformers that have further improvements with Bamba. You'll even see some hybrid SSMs on top of benchmark leaderboards. In fact, these are very small models that could be 1 billion to 350 million parameter sizes that could even run on your phone, laptops, or maybe even a CPU and consumer grade GPUs. And check out this upper performance graph. I mean, it's really impressive when you see at the top we have some of these hybrid type models now. And there we have it. We've seen how state space models are quietly reshaping the future of AI faster thinking, smarter memory, and more efficient learning. They've evolved from elegant math into these engines behind next generation models like S4 and Mamba, helping AI really focus on what truly matters. But together with these new architectures and hardware advances, they're breaking past limits and redefining what intelligence systems can do. The future of AI isn't just about bigger models, but it's about models that can remember better, think faster, and evolve continuously.","**Unlocking the Power of State Space Models: Revolutionizing AI and Machine Learning**

Imagine an AI system that can think faster, remember more, and process vast amounts of data in real-time. This is made possible by **State Space Models (SSMs)**, a revolutionary technology that is redefining the future of **Artificial Intelligence (AI)** and **Machine Learning (ML)**. SSMs enable AI to track hidden patterns over time, turning them into actionable insights. By combining SSMs with cutting-edge architectures and optimized hardware, AI can become significantly faster and more efficient.

**What are State Space Models?**

SSMs are **neural building blocks** that act as memory layers, learning how information evolves over time. They are powerful tools for processing sequential data, such as speech, text, or time series data. SSMs represent systems using two key components: the **state equation** and the **observation equation**. The state equation models how a hidden state evolves over time, while the observation equation maps the hidden state to the observed output.

**Key Benefits of State Space Models**

1. **Efficient Memory**: SSMs store information in a compact and efficient manner, allowing them to process long sequences of data without requiring large amounts of memory.
2. **Fast Processing**: SSMs can process data quickly, making them ideal for real-time applications.
3. **Improved Accuracy**: By learning patterns over time, SSMs can make more accurate predictions and decisions.

**Overcoming Limitations of Traditional Transformers**

Traditional **Transformers** are limited by their **memory bandwidth** and **computational complexity**. SSMs can help overcome these limitations by:

1. **Reducing Memory Requirements**: SSMs can process long sequences of data without requiring large amounts of memory.
2. **Improving Computational Efficiency**: SSMs can reduce the computational complexity of traditional Transformers, making them more efficient.

**Next-Generation Models: S4 and Mamba**

The **S4** model is a new type of neural network layer designed to handle long sequences of data. It is a **state space model** that can remember what happened a long time ago without having to look at everything. The **Mamba** model is a family of models that build upon the S4 architecture, adding **selectivity** and **attention-like flexibility**. Mamba models are designed for large-scale generative AI and can run on consumer-grade hardware.

**The Future of AI**

The future of AI is not just about bigger models, but about models that can remember better, think faster, and evolve continuously. SSMs are quietly reshaping the future of AI, enabling **faster thinking**, **smarter memory**, and **more efficient learning**. With the advent of next-generation models like S4 and Mamba, we can expect to see significant advancements in AI and ML, leading to breakthroughs in areas like **natural language processing**, **computer vision**, and **robotics**.

**Social Media Post Ideas**

1. Discover the power of State Space Models and how they're revolutionizing AI and Machine Learning! #AI #ML #SSMs
2. Learn how SSMs are overcoming the limitations of traditional Transformers and enabling faster, more efficient processing of sequential data. #Transformers #SSMs
3. Explore the next-generation models like S4 and Mamba that are shaping the future of AI and ML. #S4 #Mamba #AI #ML
4. Join the conversation on how SSMs are enabling AI to think faster, remember more, and process vast amounts of data in real-time. #AI #ML #SSMs",2026-01-16T02:12:50.880233
DeepLearningAI,One of the best ways to improve the apps you build is to share them early: Andrew Ng,50uLstQ0TN8,"get AI to generate some code for you and download the HTML file and see what results you get. If you feel so moved, I hope you also show it to a friend or show it to someone else to get that feedback. One mindset I hope you have is that getting feedback is often a great step in building software applications. Whenever I write software, I'll often show it to friends, show it to family, or sometimes respectfully approach strangers and ask if they're willing to look at whatever I'm building and see if they can let me know what they think or email it to a colleague or post anal online forum to get feedback. Because I find that when people look at it, they'll often have suggestions for how to make it even better. or sometimes if you get a laugh out of the friend by showing them something funny. I find that really encouraging as well and gives me the energy to keep on going. Please come back to the next video where we'll keep on working on the app and we'll look at how you can add even more features to the birthday card app to make it even more fun and interesting.","**Sharing Your Work Early: A Key to Building Better Apps**

In a recent video, **Andrew Ng** emphasizes the importance of sharing your apps early to improve their development. By doing so, you can gather **feedback** from others, which is a crucial step in building successful software applications. **Getting feedback** from friends, family, or even strangers can provide valuable insights and suggestions on how to enhance your app.

**Andrew Ng** shares his personal approach to software development, where he actively seeks **feedback** from others by showing them his work and asking for their opinions. This approach not only helps identify areas for improvement but also boosts his motivation to continue working on the project. Even a simple **laugh or positive reaction** from someone can be a great encouragement to keep moving forward.

The key takeaways from this video are:

* **Sharing your work early** can help you gather valuable feedback and improve your app
* **Feedback** is a crucial step in building successful software applications
* **Don't be afraid to show your work** to others, even if it's not perfect, as it can help you identify areas for improvement
* **Use feedback to iterate and refine** your app, making it more fun and interesting for users

By adopting this mindset, you can create better apps that meet the needs of your users. As **Andrew Ng** suggests, sharing your work early and being open to feedback can be a game-changer in the app development process.

**Social Media Post Ideas:**

* Share a screenshot of your app in development and ask for feedback from your followers
* Use hashtags like #appdevelopment, #feedback, and #softwaredevelopment to connect with other developers and users
* Share a quote from **Andrew Ng** and ask your followers to share their own experiences with sharing their work early and gathering feedback
* Create a poll asking your followers how they gather feedback for their apps and what they've learned from the process",2026-01-16T02:13:55.199419
